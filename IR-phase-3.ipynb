{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز سوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='cyan' size=6>\n",
    "AmirHossein Naghi Razlighi </font>\n",
    "<font color='red' size=6> 99102423 </font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    در این فاز از پروژه، تمرکز ما بر \n",
    "    crawling \n",
    "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
    "    web crawling\n",
    "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
    "    <br>\n",
    "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
    "    PageRank\n",
    "    و \n",
    "    HITS\n",
    "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
    "    <br>\n",
    "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
    "    <br>\n",
    "در نهایت، ما یک \n",
    "    task \n",
    "     در مورد \n",
    "    recommendation system \n",
    "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler \n",
    "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
    "    ۱۰\n",
    "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
    "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
    "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
    "</li>\n",
    "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
    "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
    "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install requests\n",
    "%pip install python-dotenv\n",
    "%pip install selenium\n",
    "%pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>is_crawled</th>\n",
       "      <th>related_prof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/The-Eigh...</td>\n",
       "      <td>False</td>\n",
       "      <td>kasaei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Benign-a...</td>\n",
       "      <td>False</td>\n",
       "      <td>kasaei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Event-De...</td>\n",
       "      <td>False</td>\n",
       "      <td>kasaei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/An-effic...</td>\n",
       "      <td>False</td>\n",
       "      <td>kasaei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Deep-Lea...</td>\n",
       "      <td>False</td>\n",
       "      <td>kasaei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  is_crawled related_prof\n",
       "0  https://www.semanticscholar.org/paper/The-Eigh...       False       kasaei\n",
       "1  https://www.semanticscholar.org/paper/Benign-a...       False       kasaei\n",
       "2  https://www.semanticscholar.org/paper/Event-De...       False       kasaei\n",
       "3  https://www.semanticscholar.org/paper/An-effic...       False       kasaei\n",
       "4  https://www.semanticscholar.org/paper/Deep-Lea...       False       kasaei"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_related_links_to_papers(prof_name):\n",
    "    prof_name = prof_name.lower()\n",
    "    prof_name = prof_name[0].upper() + prof_name[1:]\n",
    "\n",
    "    if not os.path.exists(f\"{prof_name}.txt\"):\n",
    "        raise Exception(f\"File {prof_name}.txt does not exist\")\n",
    "    df = pd.DataFrame(columns=[\"link\", \"is_crawled\"])\n",
    "\n",
    "    with open(f\"{prof_name}.txt\", \"r\") as f:\n",
    "        links = f.readlines()\n",
    "        links = [link.strip() for link in links]\n",
    "        df[\"link\"] = links\n",
    "        df[\"is_crawled\"] = False\n",
    "        df[\"related_prof\"] = prof_name.lower()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "get_related_links_to_papers(\"kasaEI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>is_crawled</th>\n",
       "      <th>related_prof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Spatial-...</td>\n",
       "      <td>False</td>\n",
       "      <td>rabiee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multires...</td>\n",
       "      <td>False</td>\n",
       "      <td>rabiee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/A-Hybrid...</td>\n",
       "      <td>False</td>\n",
       "      <td>rabiee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Novel-da...</td>\n",
       "      <td>False</td>\n",
       "      <td>rabiee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Deep-Pri...</td>\n",
       "      <td>False</td>\n",
       "      <td>rabiee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  is_crawled related_prof\n",
       "0  https://www.semanticscholar.org/paper/Spatial-...       False       rabiee\n",
       "1  https://www.semanticscholar.org/paper/Multires...       False       rabiee\n",
       "2  https://www.semanticscholar.org/paper/A-Hybrid...       False       rabiee\n",
       "3  https://www.semanticscholar.org/paper/Novel-da...       False       rabiee\n",
       "4  https://www.semanticscholar.org/paper/Deep-Pri...       False       rabiee"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_links_to_papers(\"Rabiee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kasaei_df = get_related_links_to_papers(\"kasaEI\")\n",
    "rabiee_df = get_related_links_to_papers(\"Rabiee\")\n",
    "rohban_df = get_related_links_to_papers(\"Rohban\")\n",
    "sharifi_df = get_related_links_to_papers(\"Sharifi\")\n",
    "soleymani_df = get_related_links_to_papers(\"Soleymani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium.service'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m \u001b[39mimport\u001b[39;00m Chrome\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m \u001b[39mimport\u001b[39;00m webdriver\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m Service\n\u001b[1;32m     11\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPaper\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[39mid\u001b[39m: \u001b[39mstr\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium.service'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "@dataclass\n",
    "class Paper:\n",
    "    id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    publication_year: int\n",
    "    authors: List[str]\n",
    "    related_topics: List[str]\n",
    "    citation_count: int\n",
    "    reference_count: int\n",
    "    references: List[str]\n",
    "    link: str\n",
    "\n",
    "\n",
    "def get_id_from_link(url: str) -> str:\n",
    "    return url.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "def get_link_from_id(id_: str) -> str:\n",
    "    return f\"https://www.semanticscholar.org/paper/{id_}\"\n",
    "\n",
    "\n",
    "class Spider:\n",
    "    MAX_PAPER_FOR_EACH_COUNT = 2000 // 5\n",
    "\n",
    "    def __init__(self, start_urls):\n",
    "        self.driver = self.setup_driver()\n",
    "        self.scrap_queue = [get_id_from_link(url.strip()) for url in start_urls]\n",
    "        self.crawled_count = 0\n",
    "        self.stored_ids = set()\n",
    "        self.papers = []\n",
    "        self.reference_number_patt = re.compile(r\"(?P<count>[\\d,]+) References\")\n",
    "        self.citation_number_patt = re.compile(r\"(?P<count>[\\d,]+) Citations\")\n",
    "\n",
    "    def setup_driver(self):\n",
    "        chrome_opts = webdriver.ChromeOptions()\n",
    "        chrome_opts.add_argument(\"--headless\")\n",
    "        chrome_opts.add_argument(\"--no-sandbox\")\n",
    "        chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_opts)\n",
    "        driver.implicitly_wait(10)\n",
    "        return driver\n",
    "\n",
    "    @property\n",
    "    def has_next(self) -> bool:\n",
    "        return (\n",
    "            bool(self.scrap_queue) and len(self.papers) < self.MAX_PAPER_FOR_EACH_COUNT\n",
    "        )\n",
    "\n",
    "    def pop_new_id(self) -> str:\n",
    "        id_ = self.scrap_queue.pop(0)\n",
    "        while id_ in self.stored_ids:\n",
    "            id_ = self.scrap_queue.pop(0)\n",
    "        return id_\n",
    "\n",
    "    def crawl_paper_with_id(self, id_: str):\n",
    "        paper = self.crawl_paper(id_)\n",
    "        self.stored_ids.add(id_)\n",
    "        self.papers.append(paper)\n",
    "        self.scrap_queue.extend(paper.references)\n",
    "\n",
    "    def get_next(self):\n",
    "        while self.has_next:\n",
    "            id_ = self.pop_new_id()\n",
    "            try:\n",
    "                self.crawl_paper_with_id(id_)\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                print(f\"Page not found: {id_}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "    def error_on_not_found(self):\n",
    "        error_status = self.driver.find_elements(By.ID, \"error-status\")\n",
    "        if error_status and error_status[0].get_attribute(\"value\") == \"404\":\n",
    "            raise NoSuchElementException()\n",
    "\n",
    "    def crawl_paper(self, id_: str) -> Paper:\n",
    "        url = get_link_from_id(id_)\n",
    "        self.driver.get(url)\n",
    "        self.error_on_not_found()\n",
    "\n",
    "        paper = Paper(\n",
    "            id=id_,\n",
    "            title=self.current_title,\n",
    "            abstract=self.current_abstract,\n",
    "            authors=self.current_authors,\n",
    "            publication_year=self.current_publication_year,\n",
    "            references=self.current_references,\n",
    "            citation_count=self.current_citation_count,\n",
    "            reference_count=self.current_reference_count,\n",
    "            related_topics=self.current_related_topics,\n",
    "            link=get_link_from_id(id_),\n",
    "        )\n",
    "\n",
    "        self.crawled_count += 1\n",
    "        return paper\n",
    "\n",
    "    def get_meta_one(self, name: str) -> str:\n",
    "        return self.driver.find_element(\n",
    "            By.CSS_SELECTOR, f'meta[name=\"{name}\"]'\n",
    "        ).get_attribute(\"content\")\n",
    "\n",
    "    def get_meta_multi(self, name: str) -> List[str]:\n",
    "        return [\n",
    "            a.get_attribute(\"content\")\n",
    "            for a in self.driver.find_elements(By.CSS_SELECTOR, f'meta[name=\"{name}\"]')\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def current_title(self) -> str:\n",
    "        return self.get_meta_one(\"citation_title\")\n",
    "\n",
    "    @property\n",
    "    def current_abstract(self) -> str:\n",
    "        return self.get_meta_one(\"description\")\n",
    "\n",
    "    @property\n",
    "    def current_publication_year(self) -> int:\n",
    "        try:\n",
    "            return int(self.get_meta_one(\"citation_publication_date\"))\n",
    "        except NoSuchElementException:\n",
    "            return -1\n",
    "\n",
    "    @property\n",
    "    def current_authors(self) -> List[str]:\n",
    "        return self.get_meta_multi(\"citation_author\")\n",
    "\n",
    "    @property\n",
    "    def current_related_topics(self) -> List[str]:\n",
    "        return [\n",
    "            li.text.split(\", \")\n",
    "            for li in self.driver.find_elements(By.CSS_SELECTOR, \"li.paper-meta-item\")\n",
    "            if not li.find_elements(By.CSS_SELECTOR, \"span\")\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def current_citation_count(self) -> int:\n",
    "        citation_count = [\n",
    "            mo.group(\"count\").replace(\",\", \"\")\n",
    "            for x in self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \"span.paper-nav__nav-label\"\n",
    "            )\n",
    "            if (mo := self.citation_number_patt.match(x.text))\n",
    "        ]\n",
    "        return int(citation_count[0]) if citation_count else -1\n",
    "\n",
    "    @property\n",
    "    def current_reference_count(self) -> int:\n",
    "        reference_count = [\n",
    "            mo.group(\"count\").replace(\",\", \"\")\n",
    "            for x in self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \"span.paper-nav__nav-label\"\n",
    "            )\n",
    "            if (mo := self.reference_number_patt.match(x.text))\n",
    "        ]\n",
    "        return int(reference_count[0]) if reference_count else -1\n",
    "\n",
    "    @property\n",
    "    def current_references(self) -> List[str]:\n",
    "        references_section = self.driver.find_elements(\n",
    "            By.CSS_SELECTOR, 'div[data-test-id=\"reference\"]'\n",
    "        )\n",
    "        if references_section:\n",
    "            return [\n",
    "                div.get_attribute(\"data-paper-id\")\n",
    "                for div in references_section[0]\n",
    "                .find_element(By.CSS_SELECTOR, \"div.citation-list__citations\")\n",
    "                .find_elements(By.XPATH, \".//div[@data-paper-id]\")\n",
    "            ]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Service' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# a sample link to test the spider\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sample_spider \u001b[39m=\u001b[39m Spider(\n\u001b[1;32m      3\u001b[0m     [\n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mhttps://www.semanticscholar.org/paper/EEGNet\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m3A-a-compact-convolutional-neural-network-for-Lawhern-Solon/64a2261b178f09e19a4dbbe5296f9f15cee053e3\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m sample_spider\u001b[39m.\u001b[39mMAX_PAPER_FOR_EACH_COUNT \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      8\u001b[0m sample_spider\u001b[39m.\u001b[39mget_next()\n",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m, in \u001b[0;36mSpider.__init__\u001b[0;34m(self, start_urls)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, start_urls):\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdriver \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_driver()\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscrap_queue \u001b[39m=\u001b[39m [get_id_from_link(url\u001b[39m.\u001b[39mstrip()) \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m start_urls]\n\u001b[1;32m     38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrawled_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m, in \u001b[0;36mSpider.setup_driver\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m chrome_opts\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--no-sandbox\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m chrome_opts\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--disable-dev-shm-usage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m service \u001b[39m=\u001b[39m Service(ChromeDriverManager()\u001b[39m.\u001b[39minstall())\n\u001b[1;32m     52\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome(service\u001b[39m=\u001b[39mservice, options\u001b[39m=\u001b[39mchrome_opts)\n\u001b[1;32m     53\u001b[0m driver\u001b[39m.\u001b[39mimplicitly_wait(\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Service' is not defined"
     ]
    }
   ],
   "source": [
    "# a sample link to test the spider\n",
    "sample_spider = Spider(\n",
    "    [\n",
    "        \"https://www.semanticscholar.org/paper/EEGNet%3A-a-compact-convolutional-neural-network-for-Lawhern-Solon/64a2261b178f09e19a4dbbe5296f9f15cee053e3\"\n",
    "    ]\n",
    ")\n",
    "sample_spider.MAX_PAPER_FOR_EACH_COUNT = 2\n",
    "sample_spider.get_next()\n",
    "sample_spider.papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 114.0.5735\n",
      "Get LATEST driver version for 114.0.5735\n",
      "Driver [/Users/amirhossein/.wdm/drivers/chromedriver/mac64/114.0.5735.90/chromedriver] found in cache\n",
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 114.0.5735\n",
      "Get LATEST driver version for 114.0.5735\n",
      "Driver [/Users/amirhossein/.wdm/drivers/chromedriver/mac64/114.0.5735.90/chromedriver] found in cache\n",
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 114.0.5735\n",
      "Get LATEST driver version for 114.0.5735\n",
      "Driver [/Users/amirhossein/.wdm/drivers/chromedriver/mac64/114.0.5735.90/chromedriver] found in cache\n",
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 114.0.5735\n",
      "Get LATEST driver version for 114.0.5735\n",
      "Driver [/Users/amirhossein/.wdm/drivers/chromedriver/mac64/114.0.5735.90/chromedriver] found in cache\n",
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 114.0.5735\n",
      "Get LATEST driver version for 114.0.5735\n",
      "Driver [/Users/amirhossein/.wdm/drivers/chromedriver/mac64/114.0.5735.90/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "kasaei_spider = Spider(kasaei_df[\"link\"].tolist())\n",
    "rohban_spider = Spider(rohban_df[\"link\"].tolist())\n",
    "sharifi_spider = Spider(sharifi_df[\"link\"].tolist())\n",
    "soleymani_spider = Spider(soleymani_df[\"link\"].tolist())\n",
    "rabiee_spider = Spider(rabiee_df[\"link\"].tolist())\n",
    "\n",
    "spiders = [\n",
    "    kasaei_spider,\n",
    "    rohban_spider,\n",
    "    sharifi_spider,\n",
    "    soleymani_spider,\n",
    "    rabiee_spider,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 1769748.52it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 1862066.15it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 1959955.14it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2073821.51it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2173214.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for crawler in spiders:\n",
    "    with tqdm(total=2000 // 5) as pbar:\n",
    "        while crawler.has_next:\n",
    "            crawler.get_next()\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(prof_name: str, crawler: Spider):\n",
    "    with open(f\"{prof_name}.json\", \"w\") as f:\n",
    "        json.dump([asdict(paper) for paper in crawler.papers], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "profs = [\n",
    "    (\"Kasaei\", kasaei_spider),\n",
    "    (\"Rohban\", rohban_spider),\n",
    "    (\"Sharifi\", sharifi_spider),\n",
    "    (\"Soleymani\", soleymani_spider),\n",
    "    (\"Rabiee\", rabiee_spider),\n",
    "]\n",
    "\n",
    "for prof_name, crawler in profs:\n",
    "    save_to_json(prof_name, crawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"Kasaei.json\", \"r\") as f:\n",
    "    kasaei_data = json.load(f)\n",
    "\n",
    "with open(\"Rabiee.json\", \"r\") as f:\n",
    "    rabiee_data = json.load(f)\n",
    "\n",
    "with open(\"Rohban.json\", \"r\") as f:\n",
    "    rohban_data = json.load(f)\n",
    "\n",
    "with open(\"Sharifi.json\", \"r\") as f:\n",
    "    sharifi_data = json.load(f)\n",
    "\n",
    "with open(\"Soleymani.json\", \"r\") as f:\n",
    "    soleymani_data = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>PageRank \n",
    "        شخصی‌سازی‌شده\n",
    "        (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش، الگوریتم \n",
    "    PageRank \n",
    "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم \n",
    "    PageRank\n",
    "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def pagerank(\n",
    "    graph: Dict[str, List[str]], user_prefs: Dict[str, float]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns the personalized PageRank scores for the nodes in the graph, given the user's preferences.\n",
    "\n",
    "    Parameters:\n",
    "    graph (Dict[str, List[str]]): The graph represented as a dictionary of node IDs and their outgoing edges.\n",
    "    user_prefs (Dict[str, float]): The user's preferences represented as a dictionary of node IDs and their scores.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, float]: A dictionary of node IDs and their personalized PageRank scores.\n",
    "    \"\"\"\n",
    "    damping_factor = 0.85\n",
    "    max_iterations = 100\n",
    "    convergence_threshold = 0.0001\n",
    "\n",
    "    num_nodes = len(graph)\n",
    "    initial_score = 1 / num_nodes\n",
    "    page_ranks = {node: initial_score for node in graph}\n",
    "\n",
    "    jumping_probs = {}\n",
    "    sum_prefs = sum(user_prefs.values())\n",
    "\n",
    "    if sum_prefs != 0:\n",
    "        for node, score in user_prefs.items():\n",
    "            jumping_probs[node] = score / sum_prefs\n",
    "\n",
    "    in_graph = {}\n",
    "    for node, out_nodes in graph.items():\n",
    "        for out_node in out_nodes:\n",
    "            if out_node not in in_graph:\n",
    "                in_graph[out_node] = []\n",
    "            in_graph[out_node].append(node)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        new_page_ranks = {}\n",
    "        for node in graph:\n",
    "            new_page_rank = (1 - damping_factor) / num_nodes\n",
    "\n",
    "            if node in in_graph:\n",
    "                for in_node in in_graph[node]:\n",
    "                    new_page_rank += (\n",
    "                        damping_factor * page_ranks[in_node] / len(graph[in_node])\n",
    "                    )\n",
    "\n",
    "            if node in jumping_probs:\n",
    "                new_page_rank += damping_factor * jumping_probs[node]\n",
    "\n",
    "            new_page_ranks[node] = new_page_rank\n",
    "\n",
    "        diff = 0\n",
    "        for node in graph:\n",
    "            diff += abs(new_page_ranks[node] - page_ranks[node])\n",
    "\n",
    "        page_ranks = new_page_ranks\n",
    "\n",
    "        if diff < convergence_threshold:\n",
    "            break\n",
    "\n",
    "    return page_ranks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش از الگوریتم \n",
    "PageRank\n",
    "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
    "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد \n",
    "خاص استفاده می‌کنیم. این تابع، یک \n",
    "    field \n",
    "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
    "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_by_id = {}\n",
    "\n",
    "for data in [kasaei_data, rabiee_data, rohban_data, sharifi_data, soleymani_data]:\n",
    "    for paper in data:\n",
    "        papers_by_id[paper[\"ID\"]] = paper\n",
    "\n",
    "papers_by_prof = {}\n",
    "\n",
    "profs = [\"kasaei\", \"Rabiee\", \"Rohban\", \"Sharifi\", \"Soleymani\"]\n",
    "\n",
    "for prof in profs:\n",
    "    with open(f\"{prof}.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        papers_by_prof[prof] = data\n",
    "\n",
    "\n",
    "papers_dataset = list(papers_by_id.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Eighth Visual Object Tracking VOT2020 Challenge Results'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_dataset[0]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def important_articles(Professor: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
    "\n",
    "    Parameters:\n",
    "    Professor (str): Professor's name.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
    "    \"\"\"\n",
    "\n",
    "    graph = {}\n",
    "    for paper in papers_dataset:\n",
    "        graph[paper[\"ID\"]] = []\n",
    "        for ref in paper[\"References\"]:\n",
    "            if ref in papers_by_id:\n",
    "                graph[paper[\"ID\"]].append(ref)\n",
    "\n",
    "    user_prefs = {paper[\"ID\"]: 1.0 for paper in papers_by_prof[Professor]}\n",
    "\n",
    "    # Calculate the personalized PageRank scores for the nodes in the graph\n",
    "    node_scores = pagerank(graph, user_prefs)\n",
    "\n",
    "    # Sort the nodes based on their scores\n",
    "    sorted_nodes = sorted(node_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top 10 nodes\n",
    "    return [node[0] for node in sorted_nodes[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c316d5ec14e5768d7eda3d8916bddc1de142a1c2',\n",
       " 'eda3368a5198ca55768b07b6f5667aea28baf2cd',\n",
       " 'c63a34ac6a4e049118070e707ca7679fbb132d33',\n",
       " 'bfba194dfd9c7c27683082aa8331adc4c5963a0d',\n",
       " '7574b7e5a75fdd338c27af5aeb77ab79460c4437',\n",
       " '966aad492f75b17f698e981e008b73b51816c6aa',\n",
       " '4b1a47709d0546e5bc614bf9a521c550e6881d04',\n",
       " '320d05db95ab42ade69294abe46cd1aca6aca602',\n",
       " '61394599ed0aabe04b724c7ca3a778825c7e776f',\n",
       " '9926020dda21874dc7a5ef1511bae6c4cef5ecb9']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = important_articles(\"kasaei\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(paper_id: str):\n",
    "    \"\"\"\n",
    "    Prints the paper's title, authors, and abstract.\n",
    "\n",
    "    Parameters:\n",
    "    paper_id (str): Paper's ID.\n",
    "    \"\"\"\n",
    "    paper = papers_by_id[paper_id]\n",
    "    print(f\"Title: {paper['Title']}\")\n",
    "    print(f\"Authors: {paper['Authors']}\")\n",
    "    print(f\"Abstract: {paper['Abstract']}\")\n",
    "    print(f\"Year: {paper['Publication Year']}\")\n",
    "    print(f\"References: {paper['References']}\")\n",
    "    print(f\"ID: {paper['ID']}\")\n",
    "    print(f\"Topics: {paper['Related Topics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Siamese Instance Search for Tracking\n",
      "Authors: ['Ran Tao', 'Efstratios Gavves', 'Arnold W. M. Smeulders']\n",
      "Abstract: In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the… \n",
      "Year: 19 May 2016\n",
      "References: ['b2180fc4f5cb46b5b5394487842399c501381d67', 'b762ecb0624005831f2f3d8eb626d53e8eca4b6c', 'c559e4099a6351837753b0a413f9bafed90f5dcd', 'eda3368a5198ca55768b07b6f5667aea28baf2cd', '1b3a107739e7f7e05c50999a3d79b8225746f662', 'fe2aaad872a2cf08c09dd52ca972f323666306db', '61394599ed0aabe04b724c7ca3a778825c7e776f', '505f48d8236eb25f871da272c2ac2fe4b41ea289', 'b5e17a0ed14349d6c4066d2408409751f9595e04', 'c63a34ac6a4e049118070e707ca7679fbb132d33']\n",
      "ID: c316d5ec14e5768d7eda3d8916bddc1de142a1c2\n",
      "Topics: ['Computer Science']\n"
     ]
    }
   ],
   "source": [
    "pretty_print(sample[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text: str):\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization, and punctuations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The title or abstract of an article\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: tokenize, case_folding, stem, lemmatize, punctuations\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    case_folded = [word.lower() for word in tokenized]\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in case_folded]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "    punctuations = [word for word in lemmatized if word.isalpha()]\n",
    "    return punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import json\n",
    "\n",
    "\n",
    "def get_references(prof_name: str) -> List[str]:\n",
    "    \"\"\"Gets the references of the articles of a professor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prof_name : str\n",
    "        The name of the professor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of references\n",
    "    \"\"\"\n",
    "\n",
    "    global papers_by_prof\n",
    "\n",
    "    references = []\n",
    "    for paper in papers_by_prof[prof_name]:\n",
    "        for ref in paper[\"References\"]:\n",
    "            references.append(ref)\n",
    "\n",
    "    return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for paper in papers_dataset:\n",
    "    for word in clean_data(paper[\"Title\"]):\n",
    "        if word not in index:\n",
    "            index[word] = []\n",
    "        index[word].append(paper[\"ID\"])\n",
    "\n",
    "    for word in clean_data(paper[\"Abstract\"]):\n",
    "        if word not in index:\n",
    "            index[word] = []\n",
    "        index[word].append(paper[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12508951ba96b7d4c0906ed95542287d3ebdfd95',\n",
       " '12508951ba96b7d4c0906ed95542287d3ebdfd95',\n",
       " 'f1d53e9c301d78e0b148e2f91adfc4fde2621ee5',\n",
       " 'f1d53e9c301d78e0b148e2f91adfc4fde2621ee5',\n",
       " 'ef61778d85357bdab8c71cf79cf5e0024f5b39c5',\n",
       " '786577081e00d69eeac8e9612eaf2dad59765e73',\n",
       " '786577081e00d69eeac8e9612eaf2dad59765e73',\n",
       " '219e9a4527110baf1feb3df20db12064eeafdfb7',\n",
       " '219e9a4527110baf1feb3df20db12064eeafdfb7',\n",
       " '350d507f5d899e4d7293b1aa951aa0f81b9fd30a',\n",
       " '350d507f5d899e4d7293b1aa951aa0f81b9fd30a',\n",
       " '047ea298464b041a90c4ab4e716356c019d613ab',\n",
       " '047ea298464b041a90c4ab4e716356c019d613ab',\n",
       " '966aad492f75b17f698e981e008b73b51816c6aa',\n",
       " '966aad492f75b17f698e981e008b73b51816c6aa',\n",
       " '4b1a47709d0546e5bc614bf9a521c550e6881d04',\n",
       " '8e75f635dd7578926aa7ae19ff29a8fc5c3911ae',\n",
       " '483f0f12feb8ac1c396349e5526a7552b6b067cd',\n",
       " '0c2ced886708cc3aea4705f8765d152cd3f69cd2',\n",
       " '388d29f001411ff80650f80cf197afc440d98b51',\n",
       " '1855818c492d5f42dbe14814e4dd9b5733d54790',\n",
       " '15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d',\n",
       " '15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d',\n",
       " 'f6186788541d332af19a96183787e01ef9080fb0',\n",
       " 'd8d847b085e9af12eeafc0af8df95ff2a1a98fb5',\n",
       " '19fe26d0cfe16471f4d2a05053c9f51cf14f0fa8',\n",
       " '50c60583dc0ef09484358deab329f82ee22c2b66',\n",
       " 'fc2c4aaea508973c26fe05edcb3202005e04494a',\n",
       " '6179ac06f1a8fd1ac6b693b02824948dff438d54',\n",
       " '6179ac06f1a8fd1ac6b693b02824948dff438d54',\n",
       " '0f5a25b52bef4f5fb969887224ba84e89c32e47c',\n",
       " '4dff84213493bb177dc6bff266a9893538a1f879',\n",
       " '17f16b89edaed5d16867287ed8a85e917304b4ba',\n",
       " '17f16b89edaed5d16867287ed8a85e917304b4ba',\n",
       " '53329e5c79c1128c7b252a12b182c472a3413bfa',\n",
       " '53329e5c79c1128c7b252a12b182c472a3413bfa',\n",
       " 'ddf4b612c1c944dbf47999b9c319c8186df709e0',\n",
       " '169f59ac3987301bfdecd77301eeb16ecc0e9358',\n",
       " '37257b305a7a5962306505b60f3384fc0d03f184',\n",
       " 'dd7282d139c163df0243f7cb508c7be979aa9fda',\n",
       " 'd10fa706f2383510f8ecbf69a6c404fc4a5837f8',\n",
       " '18aed16d53bfd0570d40d0be0f3b35338d1c9ead',\n",
       " '2af1310dd611857d8bd1b1374695fc9c8913a4c9',\n",
       " 'abfb457caba5314466652825260c12da01a41e0f',\n",
       " 'ba6b8c2852be77702ff1765cefde34004da9d4b5',\n",
       " '3b3aefbbdb64e5812f133f220b3f129a36a30065',\n",
       " '4c1abd8969fc1c360f50373f6552bcfb3cc408b7',\n",
       " '1206ccdce4f721462b5e185c9b2414b5f8f13116',\n",
       " '6c20cd584e7258056840eb88437d69731000bb0f',\n",
       " '5a7a7dfea3674d4e0474f7fdd596951da44babe4',\n",
       " '50a58e66397b3f22955915d16bacf151fbb532f8',\n",
       " '4b6a31a92674a7d700c669336295ab8503b01689',\n",
       " '4b6a31a92674a7d700c669336295ab8503b01689',\n",
       " '0088d6433a9715b7e74a623920925f2bfb04c920',\n",
       " '5a391667242b4a631acdd5917681b16a86523987',\n",
       " '9f2f1a5d4f6e582d4afd857915802805a7f4185e',\n",
       " '83ad3a253c05f7010a39f0c52c23302546ff8ebc',\n",
       " '544d6cd24db5adad8453033e0cc1aa7d3d6224ab',\n",
       " 'c03d64bbdd0d1ea9d47e1fc1ec43a1774f2cc10d',\n",
       " 'e21fc1f7ea8ad737e3d201468a15b080079e277a',\n",
       " '330990c1bcd331c86b55ed32d573d67c6f4015bf',\n",
       " '3c96ed310d2ba5df9744a3448d696722fce6cf9f',\n",
       " 'deeeda3e9d352de048b3d23fedeeb32e9ad94124',\n",
       " 'bf131254f1c3e24092c448e71c12e96949f7a7ed',\n",
       " '24a862c1ce346e435d6e6225e36f6a32f185258a',\n",
       " '360bcd55143613770192013aecb1656a3e8129b0',\n",
       " 'c8430c5d13834e46631960469d86f7ed4577b0e2',\n",
       " 'ed21b3d44f50d9ef8d1235b4c19a142a5cdb21a5',\n",
       " 'dab7da74e7911c908125bba602240d7192e3c3bb',\n",
       " '78bd5b068dd4be59e40fc642abd3b190d8636a2f',\n",
       " 'c0a2fc86c98438b05ada6aed5b2bd80b30add35a',\n",
       " '2ee856d5b6acc20f1313943d77374b37826ff97f',\n",
       " '5e85f4cefa96286387a9717e3260111d64d7e2ae',\n",
       " '5e85f4cefa96286387a9717e3260111d64d7e2ae',\n",
       " 'b25d8a060bb774132f94ec32ba95549ec51b6ea9',\n",
       " 'b1a9517ebf81ef3f80734ba3d5e48a37693ae1c9',\n",
       " 'b1a9517ebf81ef3f80734ba3d5e48a37693ae1c9',\n",
       " '78661cecf81340be9bd5720ac5ae97dc0e037bb9',\n",
       " '30895c61bb836f2cae7ef5ba6516886f746a7153',\n",
       " '62d49fa60b54fed1e2a2cde3cb49d3639db76768',\n",
       " '30b99ae0682d42a2010be401dd1d8f7baca9bb5f',\n",
       " 'c8c04ed972d38e2326a53d322a6f2d7e0f8218c1',\n",
       " 'c8c04ed972d38e2326a53d322a6f2d7e0f8218c1',\n",
       " '1db6e3078597386ac4222ba6c3f4f61b61f53539',\n",
       " 'f93bdba4177051d3cb285e65dc911dc77d332d11',\n",
       " 'f93bdba4177051d3cb285e65dc911dc77d332d11',\n",
       " '87c0dd990287d92796c7dc83edba6f52a2f52e21',\n",
       " '9e7e1d962ab25ae00e6fb5713116967e9b64be50',\n",
       " '2b40eabd11fe435dfa56bb0f8a6434b0dcccb5bf',\n",
       " '4e6b703476316bf7be1fa962ae695715fb784232',\n",
       " 'ec5a566be5cb54a8729343e2bb54ea262a3ec650',\n",
       " '67711d66ff40194c4578b9135ff05d7e60033f35',\n",
       " '67711d66ff40194c4578b9135ff05d7e60033f35',\n",
       " '3bffa23a16c273ac2228a13e65dade6766ce7777',\n",
       " '7b5be0cdec2a1b36cd8b61d161cff716b3594846',\n",
       " '7b5be0cdec2a1b36cd8b61d161cff716b3594846',\n",
       " 'cdecac6e2f578cfc56140e00aaa74a78f864fea2',\n",
       " 'cdecac6e2f578cfc56140e00aaa74a78f864fea2',\n",
       " 'e10d7c818b3fea4f37c762c241965b2f665c9b70',\n",
       " '0e9789615e4f3a55d300b8ca46070efab8a93513',\n",
       " 'd79a1467759d6d87123d89d99b38453b61785949',\n",
       " '69bce314f35a83b83665b0bb03b3a165c58e0edc',\n",
       " '40071396490823a1f110911ee314c8bdb6aea813',\n",
       " '3af9f90488ef3333d55f8785fd95f63b3c3bf9e7',\n",
       " 'f75bf767d060785e553326627adfee77f8e19d86',\n",
       " '8041b1bbc07947531254517bf9413ae13c954a4c',\n",
       " '43d568d061ec32dc348a4a295edf5e178041ade1',\n",
       " 'b7871e1190a4b7bc2a68f4fbf866d6c41f5bdf0d',\n",
       " 'c9583402f10c6f1819a8037ac0ad9a5399e7478d',\n",
       " '03bcca8c99f3ada98ac2a771e58c5da2635b68a3',\n",
       " '9f3547c23a6a17b7eb8e18c95cd3c00bd2432f0c',\n",
       " '5095bdbf69aa1684fb4213e331b611a6463cb77c',\n",
       " 'f877645e9e6190c1b15c1089b4bd6dc454db5daf',\n",
       " 'd77a245f8ad4f6ab838cf1c77e85ecefed25d5f1',\n",
       " 'a947c55e48e4361cf60a02fb5d91b09704b09bd6',\n",
       " 'fa4a2c677559d494fc2db6dbacf6f50b35a5ff1b',\n",
       " '5b5cd02436dc433ed64b537b829da1184ca9d1cb',\n",
       " '709f250ce84f8ac4ef3af35918a0f94b74495a2b',\n",
       " '56d059bb4aaf2d27b7557ca29a9e02aa88d8dde4',\n",
       " '64a8dec57071511f2b4dc008a1dc89fd536e3083',\n",
       " 'e3af65738a113a712d19d83309354537584969a1',\n",
       " '2fe6988bfe1e212b72ef9a3f6cb1b6446b3c6cf6',\n",
       " '56a23186d2d580a3fc1038410ec5e433c0cf3930',\n",
       " '004fd631279ea11be8856f27e3946e955a08d065',\n",
       " '682d6050b2ac0146faedf0d5fe37ef17f1c15599',\n",
       " '4d2c1d0c8af539be7fb3f965862ba0d6aa2cd423',\n",
       " '4c4cb99efaf509bb3fbc807b954dfa2ad468657a',\n",
       " 'cee6d0a02240539b78173c7f3e761993b483c16a',\n",
       " 'c5f16365b091a46fa5a3e2fa2897cd0a0237618d',\n",
       " 'eb06efe9b239c68186b63742ebc1004544f250f4',\n",
       " '522d5d55c5ad16cf523d4e744ebd43bd025e254e',\n",
       " 'dfc1e6e6c2194ea83f27740d72e9e9eafbe50ed6',\n",
       " 'db464545e0f12ff36d031ba13152dbd15a7810c7',\n",
       " 'b7ffaba8ac847bff52d90b4b842c749b9395ee54',\n",
       " '0831e41b7a2d2b02f0d45686278c5bc4dcc85346',\n",
       " 'e348edf4aa255405ce22cbb45c4db54439d6230b',\n",
       " 'e348edf4aa255405ce22cbb45c4db54439d6230b',\n",
       " '1eae26fe1ca566f17468080c3aecab1c3f9efb66',\n",
       " '915a55642ba2930061f625b86fee7daa362769cf',\n",
       " '2cab7f5d64a427cb59fb21112fe8dc28fb753b56',\n",
       " '4ef11d0b2d5bd02eab3f8113601370fc7183cc30',\n",
       " 'b2e9ab6f182579d75fa0a61d266252b258e61746',\n",
       " '53a77e8f73f2ca422d6e38fa9ecc490231ac044c',\n",
       " '5671c7890b7bfa329b161144661126aa0bcc6480',\n",
       " '5b6d03ed66473599ee31872b3cd5ad2ce282371f',\n",
       " 'f56f5e516f71822b085591f59020f3cf373f654c',\n",
       " 'f90e37dc39cbdf49cd44b41071fb5a4cdd07ad14',\n",
       " '258107a9777e978476f61534e2963da50cc1e068',\n",
       " '3c0edbe173aa15ffa3eb203cfa463e7f0abb8240',\n",
       " 'fbbd63e1f06d03c2d38884758c587d2f42e26935',\n",
       " 'e6ee69f334b71dc0edc72eecc3f29d0ef846560b',\n",
       " '343a5ab97e47368da0aa7b50256736297cbb9ce6',\n",
       " 'd035391d54e9a8b2f0f3393886ae82349d4fffae',\n",
       " 'bb83cccd9309861aeff98bfcba3653d4dde1dc87',\n",
       " '93952887ad880486f0ac90c5e37e766c33692c0c',\n",
       " 'e729730d5f69c9af97c4e3748297e37e057fd55f',\n",
       " '9fe88d7659ccf326a5a9b92c2fba159eeb272669',\n",
       " '4fa75914a2f4cd24022fafe792d57c4bdb0403e0',\n",
       " '3720bc93ed64a447b25a2be1efafa3b6b0f8fcd5',\n",
       " '884bf3333f061b0ab9adc07c7e89d1428151bcfb',\n",
       " 'f402db195032286e8a715e531aca1357ac6f4b3a',\n",
       " '333bddf6fe58ca3994805fd475cd35b6faa41adb',\n",
       " '333bddf6fe58ca3994805fd475cd35b6faa41adb',\n",
       " '419d6ca6faf224c98a62ddbb5f75bd0d4ea31b6c',\n",
       " '5066c41ef26ac9876ba797a7c7f49548cf713f9b',\n",
       " '84c8d939c765dd30574e6c7e6d0a3eb82db1c8fb',\n",
       " '84c8d939c765dd30574e6c7e6d0a3eb82db1c8fb',\n",
       " '03d9b59c84d648007d05fe8d50cdb20e0349b333',\n",
       " '11f4c7e85699b8760f5e81061293986b9caf7117',\n",
       " 'c22845f4f429ccc1018ad2b4d7e5463b516c7127',\n",
       " '3809fc1545f9876efd3cf8737662e2f88c609788',\n",
       " '2bd5b4aed18400bf1a1cc866d9b8d931aa047290']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import log10, sqrt\n",
    "import numpy as np\n",
    "\n",
    "N = len(papers_dataset)\n",
    "\n",
    "\n",
    "def search(\n",
    "    title_query: str,\n",
    "    abstract_query: str,\n",
    "    max_result_count: int,\n",
    "    method: str = \"ltn-lnn\",\n",
    "    weight: float = 0.5,\n",
    "    should_print=False,\n",
    "    preferred_field: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds relevant documents to query\n",
    "\n",
    "    Parameters\n",
    "    ---------------------------------------------------------------------------------------------------\n",
    "    max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
    "                      notice that if max_result_count = -1, then you have to return all docs\n",
    "\n",
    "    mode: 'detailed' for searching in title and text separately.\n",
    "          'overall' for all words, and weighted by where the word appears on.\n",
    "\n",
    "    where: when mode ='detailed', when we want search query\n",
    "            in title or text not both of them at the same time.\n",
    "\n",
    "    method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "\n",
    "    preferred_field: A list containing preference rate to Dr. Rabiee, Dr. Soleymani, Dr. Rohban,\n",
    "                     Dr. Kasaei, and Dr. Sharifi's papers, respectively.\n",
    "\n",
    "    Returns\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    list\n",
    "    Retrieved documents with snippet\n",
    "    \"\"\"\n",
    "    # TODO: return top 'max_result_count' documents for your searched query\n",
    "\n",
    "    # Retrieve relevant documents based on the query terms\n",
    "    title_terms = title_query.lower().split()\n",
    "    abstract_terms = abstract_query.lower().split()\n",
    "\n",
    "    title_terms = clean_data(title_query)\n",
    "    abstract_terms = clean_data(abstract_query)\n",
    "    query_terms = title_terms + abstract_terms\n",
    "\n",
    "    # Calculate the IDF for each term in the query\n",
    "    doc_freqs = defaultdict(int)\n",
    "    for term in query_terms:\n",
    "        for doc_id in index[term]:\n",
    "            doc_freqs[doc_id[0]] += 1\n",
    "\n",
    "    idfs = defaultdict(float)\n",
    "    for term in query_terms:\n",
    "        # Consider the 0 division problem\n",
    "        if len(index[term]) == 0:\n",
    "            idfs[term] = 0\n",
    "        else:\n",
    "            idfs[term] = log10(N / len(index[term]))\n",
    "\n",
    "    # Retrieve the relevant documents for each term in the query\n",
    "    relevant_docs = defaultdict(list)\n",
    "    for term in query_terms:\n",
    "        for doc in index[term]:\n",
    "            relevant_docs[term].append(doc)\n",
    "\n",
    "    # Calculate the score for each document. the final score is the weighted sum of title and abstract scores\n",
    "    title_scores = defaultdict(float)\n",
    "    abstract_scores = defaultdict(float)\n",
    "\n",
    "    # Calculate tf for each term in the query in each document and store it\n",
    "    doc_tf_s = {}\n",
    "    query_tf_s = defaultdict(int)\n",
    "    for term in query_terms:\n",
    "        for doc in relevant_docs[term]:\n",
    "            if doc not in doc_tf_s:\n",
    "                doc_tf_s[doc] = defaultdict(int)\n",
    "            doc_tf_s[doc][term] += 1\n",
    "        query_tf_s[term] += 1\n",
    "\n",
    "    avgdl = 0\n",
    "    for paper in papers_dataset:\n",
    "        avgdl += len(paper[\"Title\"]) + len(paper[\"Abstract\"])\n",
    "    avgdl /= N\n",
    "\n",
    "    if method == \"ltc-lnc\":\n",
    "        query_vector = np.zeros(len(title_terms))\n",
    "        query_norm_factor = 0\n",
    "\n",
    "        for i in range(len(title_terms)):\n",
    "            tf = query_tf_s[title_terms[i]]\n",
    "            if tf == 0:\n",
    "                query_vector[i] = 0\n",
    "            else:\n",
    "                query_vector[i] = 1 + log10(tf)\n",
    "                query_norm_factor += query_vector[i] ** 2\n",
    "\n",
    "        query_norm_factor = np.sqrt(query_norm_factor)\n",
    "\n",
    "        for term in title_terms:\n",
    "            for doc in relevant_docs[term]:\n",
    "                document_vector = np.zeros(len(title_terms))\n",
    "                document_norm_factor = 0\n",
    "\n",
    "                for i in range(len(title_terms)):\n",
    "                    tf = doc_tf_s[doc][title_terms[i]]\n",
    "                    if tf == 0:\n",
    "                        document_vector[i] = 0\n",
    "                    else:\n",
    "                        document_vector[i] = (1 + log10(tf)) * idfs[title_terms[i]]\n",
    "                        document_norm_factor += document_vector[i] ** 2\n",
    "\n",
    "                document_norm_factor = np.sqrt(document_norm_factor)\n",
    "                title_scores[doc] = np.dot(\n",
    "                    query_vector / query_norm_factor,\n",
    "                    document_vector / document_norm_factor,\n",
    "                )\n",
    "\n",
    "        query_vector = np.zeros(len(abstract_terms))\n",
    "        query_norm_factor = 0\n",
    "        for i in range(len(abstract_terms)):\n",
    "            tf = query_tf_s[abstract_terms[i]]\n",
    "            if tf == 0:\n",
    "                query_vector[i] = 0\n",
    "            else:\n",
    "                query_vector[i] = 1 + log10(tf)\n",
    "                query_norm_factor += query_vector[i] ** 2\n",
    "\n",
    "        query_norm_factor = np.sqrt(query_norm_factor)\n",
    "\n",
    "        for term in abstract_terms:\n",
    "            for doc in relevant_docs[term]:\n",
    "                document_vector = np.zeros(len(abstract_terms))\n",
    "                document_norm_factor = 0\n",
    "                for i in range(len(abstract_terms)):\n",
    "                    tf = doc_tf_s[doc][abstract_terms[i]]\n",
    "                    if tf == 0:\n",
    "                        document_vector[i] = 0\n",
    "                    else:\n",
    "                        document_vector[i] = (1 + log10(tf)) * idfs[abstract_terms[i]]\n",
    "                        document_norm_factor += document_vector[i] ** 2\n",
    "\n",
    "                document_norm_factor = np.sqrt(document_norm_factor)\n",
    "                abstract_scores[doc] = np.dot(\n",
    "                    query_vector / query_norm_factor,\n",
    "                    document_vector / document_norm_factor,\n",
    "                )\n",
    "\n",
    "    elif method == \"ltn-lnn\":\n",
    "        query_vector = np.zeros(len(title_terms))\n",
    "        for i in range(len(title_terms)):\n",
    "            query_vector[i] = 1 + log10(query_tf_s[title_terms[i]] + 1)\n",
    "\n",
    "        for term in title_terms:\n",
    "            for doc in relevant_docs[term]:\n",
    "                document_vector = np.zeros(len(title_terms))\n",
    "                for i in range(len(title_terms)):\n",
    "                    document_vector[i] = (\n",
    "                        1 + log10(doc_tf_s[doc][title_terms[i]] + 1)\n",
    "                    ) * idfs[title_terms[i]]\n",
    "                title_scores[doc] = np.dot(query_vector, document_vector)\n",
    "\n",
    "        query_vector = np.zeros(len(abstract_terms))\n",
    "        for i in range(len(abstract_terms)):\n",
    "            query_vector[i] = 1 + log10(query_tf_s[abstract_terms[i]] + 1)\n",
    "\n",
    "        for term in abstract_terms:\n",
    "            for doc in relevant_docs[term]:\n",
    "                document_vector = np.zeros(len(abstract_terms))\n",
    "                for i in range(len(abstract_terms)):\n",
    "                    document_vector[i] = (\n",
    "                        1 + log10(doc_tf_s[doc][abstract_terms[i]] + 1)\n",
    "                    ) * idfs[abstract_terms[i]]\n",
    "                abstract_scores[doc] = np.dot(query_vector, document_vector)\n",
    "\n",
    "    elif method == \"okapi25\":\n",
    "        # Setting Hyper parameters (you can change this if you desire!)\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        for term in query_terms:\n",
    "            for doc in relevant_docs[term]:\n",
    "                for term in title_terms:\n",
    "                    title_scores[doc] += (\n",
    "                        log10(N / doc_freqs[doc])\n",
    "                        * (doc_tf_s[doc][term] * (k1 + 1))\n",
    "                        / (\n",
    "                            doc_tf_s[doc][term]\n",
    "                            + k1 * (1 - b + b * (len(title_terms) / avgdl))\n",
    "                        )\n",
    "                    )\n",
    "                for term in abstract_query:\n",
    "                    abstract_scores[doc] += (\n",
    "                        log10(N / doc_freqs[doc])\n",
    "                        * (doc_tf_s[doc][term] * (k1 + 1))\n",
    "                        / (\n",
    "                            doc_tf_s[doc][term]\n",
    "                            + k1 * (1 - b + b * (len(abstract_terms) / avgdl))\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    scores = defaultdict(float)\n",
    "    for doc in title_scores:\n",
    "        scores[doc] = weight * title_scores[doc] + (1 - weight) * abstract_scores[doc]\n",
    "\n",
    "    for doc in abstract_scores:\n",
    "        if doc not in scores:\n",
    "            scores[doc] = (\n",
    "                weight * title_scores[doc] + (1 - weight) * abstract_scores[doc]\n",
    "            )\n",
    "\n",
    "    # Personalize the search based on preferred_field\n",
    "    important_articles_lst = [\n",
    "        important_articles(\"Rabiee\"),\n",
    "        important_articles(\"Soleymani\"),\n",
    "        important_articles(\"Rohban\"),\n",
    "        important_articles(\"kasaei\"),\n",
    "        important_articles(\"Sharifi\"),\n",
    "    ]\n",
    "\n",
    "    references = [\n",
    "        get_references(\"Rabiee\"),\n",
    "        get_references(\"Soleymani\"),\n",
    "        get_references(\"Rohban\"),\n",
    "        get_references(\"kasaei\"),\n",
    "        get_references(\"Sharifi\"),\n",
    "    ]\n",
    "\n",
    "    if preferred_field is not None:\n",
    "        s = sum(preferred_field)\n",
    "        for i in range(len(preferred_field)):\n",
    "            preferred_field[i] /= s\n",
    "\n",
    "        for doc in scores:\n",
    "            for i in range(len(preferred_field)):\n",
    "                if doc in important_articles_lst[i]:\n",
    "                    scores[doc] *= preferred_field[i]\n",
    "                elif doc in references[i]:\n",
    "                    scores[doc] *= preferred_field[i] * 0.5\n",
    "\n",
    "    result = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if len(result) > max_result_count:\n",
    "        return result[:max_result_count]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'eighth', 'visual', 'object', 'track', 'challeng', 'result', 'is', 'annual', 'tracker', 'benchmark', 'activ', 'organ', 'by', 'vot', 'initi', 'of', 'are', 'present', 'mani', 'publish', 'at', 'major', 'comput', 'vision', 'confer', 'or', 'in', 'journal', 'recent', 'year', 'wa', 'compos', 'five', 'focus', 'on', 'differ', 'domain', 'i', 'rgb', 'ii', 'deep', 'learn', 'for', 'a', 'comprehens', 'survey', 'target', 'one', 'most', 'yet', 'research', 'topic', 'given', 'natur', 'problem', 'and', 'it', 'popular', 'broad', 'rang', 'scenario', 'number', 'dataset', 'have', 'been', 'establish', 'which', 'consider', 'method', 'develop', 'demonstr', 'with', 'signific', 'progress', 'predominantli', 'dl', 'thi', 'aim', 'to', 'systemat', 'investig', 'ninth', 'four', 'review', 'experiment', 'comparison', 'fundament', 'task', 'ha', 'receiv', 'much', 'attent', 'studi', 'focu', 'address', 'shorter', 'video', 'howev', 'closer', 'practic', 'applic', 'more', 'complic', 'there', 'exist', 'longer', 'durat', 'such', 'even', 'also', 'need', 'handl', 'frequent', 'switch', 'refin', 'segment', 'framework', 'modifi', 'from', 'contain', 'machin', 'modul', 'improv', 'their', 'perform', 'we', 'empir', 'find', 'that', 'do', 'not', 'necessarili', 'lead', 'better', 'paper', 'make', 'simpl', 'care', 'select', 'specif', 'propos', 'new', 'combin', 'benefit', 'two', 'drawn', 'extend', 'wider', 'introduc', 'inform', 'multipl', 'modal', 'scene', 'promis', 'prospect', 'emerg', 'provid', 'thorough', 'summar', 'algorithm', 'especi', 'cocolot', 'complementari', 'how', 'capabl', 'an', 'ensembl', 'central', 'interest', 'achiev', 'but', 'consid', 'instead', 'set', 'substanti', 'ignor', 'solut', 'explicitli', 'name', 'characterist', 'first', 'person', 'understand', 'interact', 'fpv', 'follow', 'manipul', 'camera', 'wearer', 'can', 'use', 'cue', 'effect', 'model', 'avail', 'literatur', 'significantli', 'last', 'larg', 'varieti', 'despit', 'few', 'previou', 'attempt', 'exploit', 'discrimin', 'filter', 'siames', 'network', 'outlook', 'accur', 'robust', 'entail', 'estim', 'trajectori', 'imag', 'sequenc', 'onli', 'locat', 'rough', 'approxim', 'form', 'bound', 'box', 'correl', 'dcf', 'sn', 'domin', 'paradigm', 'led', 'rapid', 'evolut', 'commun', 'global', 'via', 'local', 'crux', 'lie', 'difficulti', 'discontinu', 'move', 'caus', 'occlus', 'typic', 'strategi', 'employ', 'smooth', 'anoth', 'detect', 'when', 'lost', 'while', 'tempor', 'context', 'like', 'histor', 'appear', 'potenti', 'limit', 'seventh', 'evalu', 'includ', 'standard', 'other', 'methodolog', 'analysi', 'well', 'sixth', 'over', 'eighti', 'experi', 'simul', 'situat', 'where', 'process', 'if', 'fifth', 'continu', 'run', 'sensor', 'compar', 'appli', 'test', 'largest', 'date', 'each', 'particip', 'short', 'descript', 'appendix', 'featur', 'go', 'beyond', 'predecessor', 'be', 'attract', 'decad', 'surg', 'public', 'made', 'almost', 'imposs', 'field', 'reason', 'lack', 'commonli', 'accept', 'annot', 'protocol', 'would', 'allow', 'issu', 'workshop', 'wild', 'oxuva', 'enabl', 'great', 'stride', 'defin', 'diver', 'these', 'work', 'exclus', 'just', 'ten', 'second', 'length', 'alway', 'visibl', 'consequ', 'design', 'tailor', 'singl', 'shot', 'current', 'due', 'restrict', 'transform', 'reduc', 'accuraci', 'narrow', 'gap', 'between', 'geometr', 'properti', 'invari', 'enhanc', 'mammogram', 'breast', 'cancer', 'classif', 'artifici', 'neural', 'drive', 'death', 'woman', 'both', 'nation', 'plan', 'system', 'must', 'decreas', 'irregular', 'part', 'approach', 'crop', 'region', 'roi', 'manual', 'base', 'extract', 'novel', 'hybrid', 'optimum', 'hof', 'out', 'architectur', 'higher', 'mortal', 'rate', 'than', 'earli', 'critic', 'diseas', 'diagnosi', 'vital', 'concept', 'techniqu', 'wide', 'doctor', 'physician', 'patholog', 'identifi', 'abnorm', 'tissu', 'volum', 'analyz', 'anatom', 'structur', 'treatment', 'convolut', 'common', 'among', 'entir', 'world', 'after', 'lung', 'automat', 'might', 'possibl', 'surviv', 'patient', 'through', 'start', 'cnn', 'dimension', 'princip', 'compon', 'pca', 'aid', 'mass', 'fastest', 'mammographi', 'proven', 'best', 'prognosi', 'cad', 'reader', 'interpret', 'although', 'incid', 'increas', 'fold', 'unfortun', 'stagnat', 'therefor', 'hierarch', 'tumor', 'introduct', 'chanc', 'screen', 'perfectli', 'differenti', 'healthi', 'individu', 'help', 'materi', 'regard', 'import', 'separ', 'normal', 'case', 'mri', 'background', 'kind', 'epitheli', 'rise', 'younger', 'gener', 'magnet', 'reson', 'play', 'role', 'today', 'clinic', 'grow', 'observ', 'becom', 'diversifi', 'autom', 'appeal', 'adapt', 'mammograph', 'distinguish', 'malign', 'benign', 'ass', 'function', 'maximum', 'likelihood', 'gradient', 'order', 'accommod', 'edg', 'were', 'weight', 'entropi', 'build', 'diagnos', 'purposeclassif', 'suspici', 'may', 'shape', 'built', 'purpos', 'contour', 'variou', 'paramet', 'partit', 'into', 'subset', 'represent', 'digit', 'found', 'radiograph', 'classifi', 'either', 'mean', 'support', 'vector', 'coeffici', 'encod', 'pixel', 'wavelet', 'alreadi', 'discus', 'our', 'nonparametr', 'bayesian', 'copula', 'event', 'soccer', 'standout', 'amongst', 'key', 'distinct', 'sort', 'area', 'data', 'pick', 'up', 'extens', 'expert', 'scholast', 'zone', 'subject', 'effort', 'impress', 'le', 'multimodel', 'relat', 'effici', 'match', 'doubt', 'circumst', 'ca', 'identif', 'fuse', 'low', 'mid', 'level', 'boundari', 'color', 'then', 'view', 'type', 'replay', 'devi', 'spatial', 'posit', 'occurr', 'cricket', 'nowaday', 'sport', 'gain', 'lot', 'traction', 'excit', 'team', 'watch', 'game', 's', 'intricaci', 'computer', 'difficult', 'motiv', 'u', 'conduct', 'will', 'advanc', 'articl', 'give', 'overview', 'confin', 'ellipt', 'ternari', 'pattern', 'extrem', 'broadcast', 'enorm', 'amount', 'content', 'all', 'captur', 'user', 'rebroadcast', 'inspect', 'huge', 'repositori', 'long', 'labori', 'job', 'overcom', 'edit', 'technolog', 'fast', 'creat', 'retriev', 'multimedia', 'brow', 'deliveri', 'veri', 'slow', 'henc', 'way', 'faster', 'index', 'peopl', 'spend', 'free', 'time', 'decis', 'tree', 'heavili', 'explor', 'transit', 'indispens', 'requir', 'precis', 'parallel', 'fusion', 'transfer', 'markov', 'prior', 'call', 'produc', 'zoom', 'statu', 'resolut', 'big', 'seriou', 'mine', 'numer', 'known', 'multi', 'draw', 'conclus', 'medium', 'remain', 'aspect', 'abl', 'pas', 'stori', 'clip', 'complet', 'meet', 'audienc', 'fulli', 'cinemat', 'some', 'goal', 'refere', 'output', 'three', 'semant', 'dynam', 'formul', 'knowledg', 'abstract', 'nongeometr', 'bn', 'dbn', 'valid', 'particular', 'special', 'corner', 'kick', 'penalti', 'card', 'basebal', 'integr', 'midlevel', 'scoreboard', 'rule', 'recognit', 'scheme', 'obtain', 'sever', 'essenti', 'construct', 'summari', 'worldwid', 'seek', 'power', 'highlight', 'audiovisu', 'within', 'mainli', 'heurist', 'hunt', 'wildlif', 'vide', 'documentari', 'textur', 'motion', 'blob', 'determin', 'class', 'descriptor', 'infer', 'speech', 'put', 'they', 'innov', 'webcast', 'text', 'devot', 'reli', 'itself', 'face', 'ontolog', 'indic', 'detector', 'root', 'genr', 'evid', 'gather', 'dure', 'train', 'consist', 'tool', 'databas', 'videoview', 'browser', 'hidden', 'multiscal', 'decomposit', 'distribut', 'map', 'firstli', 'detail', 'layer', 'enforc', 'those', 'refer', 'piti', 'probabl', 'primit', 'oper', 'accordingli', 'mislead', 'who', 'look', 'still', 'belong', 'old', 'space', 'proper', 'should', 'act', 'recolor', 'deriv', 'serv', 'academ', 'industri', 'often', 'under', 'label', 'correct', 'balanc', 'same', 'dark', 'perceptu', 'lighten', 'classic', 'statist', 'them', 'input', 'hue', 'bright', 'infrar', 'yuv', 'ir', 'lawn', 'land', 'red', 'alert', 'danger', 'emphas', 'hot', 'intens', 'similar', 'v', 'repres', 'optim', 'measur', 'alter', 'sourc', 'borrow', 'take', 'influenc', 'contribut', 'gaussian', 'membership', 'degre', 'night', 'modern', 'intensifi', 'thermal', 'advers', 'weather', 'condit', 'unnatur', 'hard', 'imageri', 'principl', 'grayscal', 'prefer', 'larger', 'carri', 'straightforward', 'invers', 'no', 'search', 'reveal', 'histori', 'past', 'brief', 'dimensionreduct', 'fuzzi', 'cluster', 'prove', 'spheric', 'ellipsoid', 'distanc', 'spuriou', 'converg', 'fcm', 'fcv', 'anisotrop', 'diffus', 'chromat', 'achromat', 'channel', 'basi', 'colour', 'shown', 'split', 'signal', 'accomplish', 'independ', 'involv', 'conveni', 'embed', 'complex', 'affect', 'light', 'mat', 'foreground', 'alpha', 'vari', 'novelti', 'procedur', 'nonlinear', 'success', 'util', 'cascad', 'keypoint', 'urgent', 'sinc', 'simultan', 'thu', 'owe', 'deform', 'adopt', 'anchor', 'ad', 'nois', 'becaus', 'rel', 'good', 'cost', 'regular', 'comet', 'small', 'unknown', 'aerial', 'high', 'altitud', 'pronounc', 'unavoid', 'drastic', 'densiti', 'multitask', 'offlin', 'cract', 'qualiti', 'crucial', 'rpn', 'regress', 'popularli', 'boost', 'deal', 'crac', 'yield', 'pedestrian', 'vehicl', 'tradit', 'describ', 'aerialmptnet', 'graphic', 'memori', 'graph', 'toward', 'coars', 'revisit', 'de', 'facto', 'predict', 'mask', 'usual', 'fork', 'branch', 'backbon', 'directli', 'without', 'neg', 'clutter', 'tend', 'lag', 'onlin', 'updat', 'quit', 'riski', 'straightforwardli', 'solv', 'uncertain', 'noisi', 'illustr', 'suffer', 'speed', 'impract', 'smaller', 'nine', 'neither', 'intend', 'whole', 'nor', 'point', 'so', 'far', 'spectacular', 'famili', 'ultim', 'ani', 'chang', 'properli', 'sometim', 'ambigu', 'frame', 'tri', 'categori', 'term', 'variat', 'exclud', 'interfer', 'maintain', 'respons', 'realtim', 'toler', 'hardli', 'get', 'top', 'pair', 'subnetwork', 'den', 'connect', 'competit', 'convent', 'abil', 'shallow', 'tackl', 'easili', 'drift', 'heavi', 'incorpor', 'circular', 'sampl', 'state', 'rgbd', 'momentum', 'thank', 'depth', 'pose', 'ﬁrstli', 'perspect', 'usag', 'metric', 'access', 'facilit', 'file', 'cdtb', 'record', 'passiv', 'setup', 'indoor', 'outdoor', 'acquir', 'direct', 'sunlight', 'magnitud', 'period', 'reptil', 'got', 'main', 'recogn', 'throughout', 'moreov', 'down', 'instanc', 'uniqu', 'realli', 'definit', 'maxim', 'probe', 'strength', 'outperform', 'behavior', 'show', 'link', 'furthermor', 'now', 'you', 'see', 'me', 'disappear', 'six', 'suitabl', 'recal', 'prosper', 'tlp', 'hd', 'real', 'encompass', 'minut', 'averag', 'per', 'total', 'cover', 'pave', 'r', 'recoveri', 'reliabl', 'bridg', 'baselin', 'superdimp', 'regressor', 'prdimp', 'dimp', 'uncertainti', 'reduct', 'random', 'era', 'agreement', 'realist', 'done', 'skim', 'peru', 'seri', 'candid', 'verifi', 'globaltrack', 'strong', 'absenc', 'failur', 'pure', 'assumpt', 'scale', 'excel', 'poorli', 'equip', 'mechan', 'error', 'resolv', 'uav', 'guidanc', 'temperatur', 'bottleneck', 'unlock', 'collect', 'mfgnet', 'attain', 'messag', 'adjust', 'kernel', 'unsupervis', 'distil', 'tir', 'doe', 'account', 'unstructur', 'environ', 'exampl', 'factor', 'multimod', 'dual', 'unifi', 'changer', 'prevent', 'mostli', 'group', 'size', 'jointli', 'late', 't', 'linearli', 'unreli', 'human', 'mht', 'primarili', 'constrain', 'aris', 'illumin', 'arriv', 'cheap', 'devic', 'everi', 'stage', 'author', 'reinforc', 'advantag', 'compact', 'student', 'marriag', 'compress', 'stabl', 'reappear', 'blindli', 'onc', 'necessari', 'scope', 'assist', 'addit', 'pool', 'reevalu', 'templat', 'togeth', 'pursu', 'wise', 'unlik', 'dtnet', 'execut', 'behaviour', 'trend', 'agent', 'polici', 'inspir', 'robot', 'percept', 'onboard', 'stream', 'inevit', 'discrep', 'ofﬂin', 'latenc', 'egotrack', 'egocentr', 'full', 'spectrum', 'embodi', 'ai', 'underrepres', 'hand', 'exit', 'intellig', 'degrad', 'unman', 'fatal', 'meccano', 'wearabl', 'thoroughli', 'third', 'understudi', 'encourag', 'ﬁeld', 'action', 'anticip', 'workspac', 'collabor', 'enough', 'stimul', 'million', 'imu', 'around', 'hour', 'suit', 'offer', 'dailylif', 'span', 'hundr', 'household', 'workplac', 'leisur', 'etc', 'countri', 'uphold', 'rigor', 'privaci', 'ethic', 'consent', 'relev', 'core', 'meta', 'driven', 'quickli', 'robustli', 'futur', 'ideal', 'avoid', 'overfit', 'motchalleng', 'push', 'advent', 'leaderboard', 'guid', 'mot', 'launch', 'know', 'your', 'surround', 'prone', 'fail', 'presenc', 'distractor', 'alon', 'insuffici', 'about', 'highli', 'benefici', 'propag', 'share', 'code', 'librari', 'gaug', 'art', 'briefli', 'criterion', 'struck', 'arbitrari', 'treat', 'happen', 'convert', 'clear', 'intermedi', 'step', 'augment', 'could', 'veloc', 'easi', 'attach', 'overlook', 'fact', 'induc', 'inconsist', 'acceler', 'ground', 'truth', 'downstream', 'consum', 'labor', 'siamoa', 'divid', 'subtask', 'score', 'besid', 'aggrav', 'burden', 'allevi', 'usher', 'superior', 'subspac', 'multitud', 'handcraft', 'sum', 'leverag', 'ratio', 'tediou', 'configur', 'siamban', 'express', 'fcn', 'autonom', 'surveil', 'secur', 'latest', 'siamcar', 'decompos', 'subproblem', 'manner', 'guidelin', 'demand', 'former', 'took', 'player', 'attribut', 'auxiliari', 'entertain', 'crowd', 'rank', 'tabl', 'reconstruct', 'optic', 'flow', 'stereo', 'pitfal', 'respect', 'interestingli', 'rather', 'actor', 'actual', 'attend', 'explicit', 'recurr', 'realiz', 'reserv', 'miss', 'transpar', 'opaqu', 'littl', 'paid', 'totb', 'mix', 'contrast', 'specular', 'least', 'insid', 'everyday', 'wear', 'eyewatchm', 'taken', 'extent', 'tld', 'cf', 'exhibit', 'remark', 'settl', 'unwant', 'suboptim', 'adversari', 'monitor', 'inher', 'maneuver', 'agil', 'blur', 'impair', 'abdnet', 'deblurr', 'recov', 'mutual', 'viewpoint', 'report', 'classiﬁ', 'loss', 'cross', 'ﬁnd', 'overﬁt', 'minim', 'ce', 'underli', 'holist', 'arduou', 'equal', 'patch', 'deliv', 'redesign', 'upon', 'stmtrack', 'harder', 'fix', 'resist', 'hinder', 'philosophi', 'awar', 'pseudo', 'dmtrack', 'prelearn', 'open', 'occur', 'correspond', 'relianc', 'stabil', 'redetect', 'bidirect', 'plethora', 'navig', 'servic', 'abov', 'equival', 'otb', 'toolkit', 'wors', 'contradictori', 'further', 'hamper', 'mirror', 'amtset', 'abrupt', 'releas', 'life', 'sudden', 'quantit', 'overal', 'weak', 'meanwhil', 'convinc', 'reflect', 'undergo', 'rotat', 'unconstrain', 'particl', 'expens', 'extractor', 'decoupl', 'kalman', 'smart', 'control', 'ptz', 'speaker', 'static', 'lectur', 'newli', 'virtual', 'inhibit', 'bia', 'fairli', 'tune', 'bias', 'basic', 'constraint', 'warm', 'kemel', 'civil', 'militari', 'posse', 'certain', 'undesir', 'kcf', 'rescal', 'constitut', 'learnt', 'next', 'camel', 'urban', 'hope', 'depend', 'inde', 'twofold', 'band', 'conjug', 'sequenti', 'signatur', 'sen', 'dedic', 'stationari', 'articul', 'constant', 'accord', 'theori', 'shift', 'histogram', 'camouflag', 'relearn', 'tao', 'anim', 'vast', 'coco', 'foster', 'lasot', 'la', 'ingl', 'o', 'bject', 'rack', 'half', 'randomli', 'doubl', 'revers', 'version', 'repeat', 'repetit', 'loop', 'versu', 'latex', 'altern', 'mml', 'math', 'msub', 'mn', 'xlink', 'align', 'axi', 'pipelin', 'afod', 'binari', 'bank', 'therebi', 'across', 'siammask', 'dtt', 'rich', 'finetun', 'immedi', 'radic', 'youtub', 'simpli', 'return', 'trackingnet', 'satur', 'matter', 'scarciti', 'nonrigid', 'self', 'behind', 'freedom', 'simplif', 'constel', 'topolog', 'assum', 'tractabl', 'videomatch', 'net', 'memor', 'cadx', 'subsequ', 'appropri', 'societi', 'mia', 'whether', 'whosetreat', 'reach', 'radiologist', 'preprocess', 'remov', 'health', 'until', 'diagnost', 'delin', 'liver', 'ct', 'viz', 'hepatocellular', 'carcinoma', 'hcc', 'metastat', 'met', 'slice', 'tomographi', 'scan', 'reorgan', 'threshold', 'zernik', 'moment', 'fals', 'margin', 'translat', 'nrl', 'outcom', 'ftrd', 'fourier', 'radial', 'empow', 'biomed', 'majorli', 'deadliest', 'earlier', 'grey', 'wolf', 'threat', 'globe', 'greater', 'glcm', 'hazard', 'matrix', 'aglcm', 'cure', 'swarm', 'microcalcif', 'correctli', 'post', 'unsharp', 'median', 'discret', 'risk', 'save', 'live', 'categor', 'brain', 'stroke', 'damag', 'neuroimag', 'obviou', 'white', 'gray', 'medic', 'bone', 'soft', 'blood', 'vessel', 'becam', 'dengu', 'fever', 'headach', 'muscl', 'ach', 'rash', 'preval', 'symptom', 'endem', 'south', 'asian', 'southeast', 'df', 'hemorrhag', 'dhf', 'shock', 'syndrom', 'ds', 'deadli', 'rais', 'phase', 'b', 'c', 'd', 'e', 'befor', 'physic', 'bodi', 'day', 'prematur', 'right', 'ripley', 'k', 'femal', 'western', 'ecolog', 'specimen', 'plant', 'matric', 'electron', 'store', 'neuron', 'ann', 'dramat', 'f', 'nodul', 'ultrasound', 'ultrasonographi', 'usg', 'analys', 'speckl', 'bilater', 'srbf', 'elimin', 'tumour', 'lesion', 'particularli', 'distort', 'cca', 'watersh', 'healthcar', 'subsid', 'hilbert', 'raw', 'infinit', 'genet', 'ttcnn', 'specialist', 'implement', 'multilevel', 'plagu', 'examin', 'place', 'center', 'forest', 'sixteen', 'μm', 'manag', 'macro', 'medicin', 'agricultur', 'gambl', 'environment', 'protect', 'forecast', 'compani', 'streamlin', 'market', 'campaign', 'analyt', 'valuabl', 'insight', 'program', 'thing', 'smarter', 'opportun', 'strain', 'myriad', 'steadili', 'age', 'popul', 'pandem', 'apart', 'remot', 'internet', 'iot', 'billion', 'poli', 'butylen', 'succinct', 'pb', 'nano', 'composit', 'substanc', 'catalyst', 'microcrystallin', 'polybutylen', 'nan', 'biodegrad', 'morpholog', 'compound', 'ester', 'ether', 'alloy', 'metal', 'retrial', 'ara', 'busi', 'server', 'crash', 'custom', 'traffic', 'holiday', 'interrupt', 'orbit', 'empti', 'end', 'vacat', 'wv', 'client', 'visitor', 'inact', 'p', 'q', 'variabl', 'wind', 'farm', 'topsi', 'air', 'turbin', 'batteri', 'energi', 'windmil', 'price', 'expect', 'fit', 'mill', 'calcul', 'charg', 'discharg', 'forc', 'multidisciplinari', 'exchang', 'privat', 'seamless', 'partner', 'chat', 'bot', 'mark', 'rcnn', 'deceas', 'true', 'ml', 'grown', 'mitos', 'histopatholog', 'mitot', 'nucleus', 'bc', 'undevelop', 'character', 'mutat', 'gene', 'pain', 'skin', 'pathologist', 'prognost', 'ga', 'primari', 'wisconsin', 'errat', 'growth', 'prolifer', 'cell', 'origin', 'pector', 'tag', 'brightest', 'makm', 'bethezata', 'hospit', 'bgh', 'gabor', 'claim', 'overtook', 'though', 'longev', 'slide', 'fear', 'obstacl', 'biggest', 'had', 'trial', 'choic', 'emot', 'cervic', 'colon', 'mandatori', 'engin', 'jump', 'januari', 'stack', 'confess', 'relaunch', 'ddsm', 'chain', 'highest', 'calcif', 'asymmetri', 'calc', 'circ', 'spic', 'misc', 'arch', 'asym', 'svm', 'window', 'probabilist', 'gold', 'previous', 'nonmalign', 'forward', 'float', 'sff', 'purposeimprov', 'sensit', 'ultrason', 'law', 'hepatoma', 'cirrhosi', 'bay', 'hotel', 'trace', 'acut', 'sharp', 'oppos', 'concentr', 'circumscrib', 'spicul', 'radiolog', 'turn', 'tremend', 'myocard', 'cardiac', 'explain', 'cardiovascular', 'cvd', 'endang', 'myocardium', 'microb', 'virus', 'hiv', 'mcd', 'associ', 'irrevers', 'cmri', 'cardiologist', 'epilept', 'seizur', 'electroencephalographi', 'eeg', 'autism', 'disord', 'asd', 'sign', 'childhood', 'deficit', 'psycholog', 'paramount', 'clinician', 'pcpcet', 'adewnn', 'biopsi', 'anxieti', 'ffdm', 'retrospect', 'aggnet', 'mitosi', 'histolog', 'publicli', 'crowdsourc', 'deeper', 'outsourc', 'microwav', 'snr', 'attenu', 'rf', 'smear', 'peak', 'jitter', 'dictionari', 'hyperspectr', 'spectral', 'contextu', 'idea', 'neighborhood', 'member', 'linear', 'element', 'multiresolut', 'anomali', 'generaliz', 'secondli', 'anomal', 'here', 'mobil', 'deploy', 'home', 'feed', 'lighter', 'fair', 'bed', 'spar', 'landcov', 'sophist', 'shapelet', 'herebi', 'sr', 'hsi', 'atom', 'ineffici', 'incomplet', 'whose', 'nonzero', 'entri', 'acquisit', 'sparsiti', 'subpixel', 'block', 'mixtur', 'flexibl', 'demix', 'lmm', 'simplex', 'endmemb', 'fraction', 'abund', 'squar', 'sd', 'unmixingwith', 'lasso', 'unmix', 'spectroradiomet', 'instrument', 'semisupervis', 'fashion', 'priori', 'dimens', 'ingredi', 'combat', 'cur', 'profil', 'mathemat', 'close', 'isol', 'spectromet', 'unavail', 'theoret', 'hyperdimension', 'hypersubspac', 'uniad', 'fall', 'ident', 'shortcut', 'spot', 'outlier', 'kd', 'teacher', 'pull', 'subtl', 'defect', 'neglect', 'deviat', 'overwhelmingli', 'unlabel', 'notori', 'prototyp', 'residu', 'manufactur', 'rare', 'supervis', 'seen', 'unsatisfactori', 'discern', 'let', 'project', 'pretrain', 'bring', 'thorni', 'collaps', 'uninform', 'latent', 'circumv', 'iter', 'manifold', 'autoencod', 'deterior', 'financ', 'transport', 'platform', 'amalgam', 'sdn', 'nfv', 'interplay', 'massiv', 'hardwar', 'interconnect', 'citi', 'ehealth', 'ubiquit', 'foreseen', 'cloud', 'joint', 'mediat', 'none', 'grace', 'preserv', 'datamix', 'send', 'resourc', 'afford', 'trustworthi', 'expand', 'coverag', 'incur', 'concern', 'breach', 'dnn', 'breakthrough', 'revolution', 'smartphon', 'app', 'nearli', 'unpreced', 'brought', 'her', 'own', 'onto', 'violat', 'expo', 'zoe', 'heterogen', 'revolut', 'phenomenon', 'final', 'question', 'evolv', 'societ', 'commerci', 'what', 'preach', 'enthral', 'bear', 'daili', 'wireless', 'anytim', 'anywher', 'institut', 'nist', 'cc', 'pore', 'heatmap', 'wit', 'breakout', 'count', 'postur', 'tracklet', 'orient', 'alarm', 'social', 'scienc', 'technic', 'safeti', 'sound', 'tantamount', 'web', 'wot', 'compris', 'simplifi', 'adher', 'commot', 'cuboid', 'grid', 'advect', 'plane', 'chapter', 'violent', 'outbreak', 'violenc', 'surveyor', 'constantli', 'sea', 'break', 'mind', 'someth', 'familiar', 'disciplin', 'sociolog', 'trendi', 'poor', 'salient', 'disco', 'obfusc', 'hold', 'facial', 'deepobfusc', 'request', 'impos', 'rescu', 'testimoni', 'whi', 'vulner', 'cooper', 'halv', 'partial', 'attack', 'intercept', 'disentangl', 'increasingli', 'oﬄoad', 'creep', 'malici', 'ﬁltere', 'grand', 'deeprotect', 'sensori', 'provabl', 'drop', 'pervas', 'host', 'fmri', 'sbdsm', 'superpixel', 'oversegment', 'salt', 'pepper', 'homogen', 'neurosci', 'admit', 'restor', 'audio', 'rm', 'overcomplet', 'pursuit', 'walk', 'favor', 'format', 'matroid', 'combinatori', 'denois', 'corrupt', 'noiseless', 'stochast', 'descent', 'minimum', 'fisher', 'kfd', 'adaboost', 'nevertheless', 'intrins', 'rnn', 'norm', 'sfl', 'obscur', 'csd', 'earth', 'surfac', 'lrr', 'neighbour', 'assign', 'frontier', 'airborn', 'spaceborn', 'fine', 'echo', 'spectroscopi', 'commod', 'product', 'resnet', 'cube', 'unfold', 'inclus', 'neighbor', 'confus', 'elabor', 'rsjsrc', 'pixelwis', 'predefin', 'src', 'plausibl', 'greedi', 'disadvantag', 'spatiospectr', 'notion', 'wherein', 'rbf', 'redund', 'covari', 'spectralandspati', 'cmr', 'jsr', 'valu', 'suffici', 'widespread', 'imbal', 'arisen', 'sanet', 'nonloc', 'cr', 'nearest', 'nrjsr', 'letter', 'sajsrc', 'pc', 'correntropi', 'spjsr', 'replac', 'l', 'spl', 'orthogon', 'somp', 'occ', 'broadli', 'supplementari', 'mode', 'multiattribut', 'emap', 'shortcom', 'odl', 'fault', 'occas', 'lbp', 'along', 'transmiss', 'storag', 'ssc', 'prohibit', 'scalabl', 'hierarchi', 'remodel', 'capsul', 'wasserstein', 'fe', 'quantiz', 'ineffect', 'ssasr', 'multifeatur', 'downsampl', 'gradual', 'enlarg', 'tikhonov', 'simplic', 'dkcrt', 'mfasr', 'sa', 'nn', 'stepwis', 'mrf', 'marin', 'coher', 'near', 'portion', 'electromagnet', 'speci', 'arous', 'restrain', 'dilemma', 'dnmf', 'prune', 'frequenc', 'subsampl', 'wsw', 'satellit', 'dcnn', 'tensor', 'tdsl', 'dwdnn', 'bee', 'coloni', 'hypergraph', 'evolutionari', 'diagon', 'hotspot', 'export', 'degener', 'remedi', 'defici', 'dlrl', 'classwis', 'nnlrr', 'harmon', 'affin', 'synthet', 'cppssc', 'multiobject', 'greatli', 'clean', 'tv', 'emd', 'mwt', 'rac', 'regist', 'entiti', 'multinomi', 'logist', 'posterior', 'wherebi', 'proport', 'lda', 'ensur', 'mild', 'dispar', 'site', 'angl', 'euclidean', 'wavelength', 'quantiti', 'contigu', 'ica', 'shrinkag', 'cast', 'superresolut', 'reweight', 'meaning', 'percentag', 'geometri', 'against', 'coordin', 'geode', 'isometr', 'isomap', 'guarante', 'multidimension', 'finit', 'nonneg', 'constitu', 'nmf', 'nonconvex', 'nonuniqu', 'convex', 'cl', 'cbp', 'cbpdn', 'bp', 'umbrella', 'algebra', 'scc', 'hic', 'blind', 'bs', 'lsmm', 'shortag', 'parameter', 'deca', 'radianc', 'quantif', 'lsma', 'dispers', 'uniti', 'zero', 'omp', 'ion', 'substitut', 'geolog', 'disjoint', 'spectroscop', 'adjac', 'come', 'lower', 'forthcom', 'german', 'eea', 'creation', 'multiband', 'coregist', 'panchromat', 'multispectr', 'resembl', 'keep', 'bps', 'sensibl', 'vertex', 'vca', 'vertic', 'superun', 'bregman', 'enclos', 'serious', 'intuit', 'craig', 'minor', 'modif', 'wrong', 'bundl', 'sslrr', 'heart', 'planetari', 'tetracord', 'chemic', 'bond', 'broadband', 'expertis', 'lagrangian', 'iplip', 'nonsmooth', 'preclud', 'unstabl', 'equat', 'm', 'n', 'underdetermin', 'ax', 'suppos', 'sparsest', 'fewest', 'ever', 'answer', 'x', 'ge', 'intersect', 'orthant', 'strengthen', 'note', 'orthonorm', 'blend', 'ecien', 'inaccur', 'wish', 'ℝ𝓂', 'contamin', 'y', '𝓃', '𝓂', 'fewer', 'row', 'column', 'decod', 'exactli', 'instantan', 'scatter', 'dirichlet', 'conceiv', 'resort', 'contract', 'reliev', 'lend', 'themselv', 'rapidli', 'pauciti', 'dsr', 'ssl', 'cut', 'gc', 'strike', 'satisfactori', 'smoother', 'laplacian', 'transduct', 'tsvm', 'hugh', 'multiclass', 'hyperplan', 'discoveri', 'mercer', 'multitempor', 'interferometr', 'sar', 'sweden', 'farmland', 'rosi', 'mp', 'emp', 'fr', 'mivi', 'lara', 'laboratori', 'divis', 'atmospher', 'pollut', 'italian', 'council', 'town', 'rome', 'napl', 'milan', 'verona', 'green', 'municip', 'biotop', 'hymap', 'seal', 'themat', 'outstand', 'singular', 'excess', 'analyst', 'formal', 'hu', 'ea', 'implicitli', 'multilay', 'mlnmf', 'mvsa', 'ifa', 'immers', 'unfavour', 'mc', 'rosvm', 'ongo', 'aviri', 'seldom', 'avala', 'feedforward', 'elm', 'somewhat', 'desir', 'strongli', 'necess', 'boser', 'guyon', 'vapnik', 'nasa', 'aisa', 'specim', 'finland', 'string', 'disregard', 'multisourc', 'consensu', 'laid', 'mileston', 'iad', 'vad', 'miad', 'mainten', 'panda', 'surprisingli', 'perhap', 'unreport', 'cutpast', 'glanc', 'concret', 'pyramid', 'unexpected', 'padim', 'concurr', 'multivari', 'proxi', 'quantifi', 'drawback', 'scadn', 'perfect', 'granular', 'destseg', 'mixedteach', 'stone', 'gan', 'cumbersom', 'extern', 'imagenet', 'exposur', 'nomin', 'peform', 'efﬁcienc', 'classiﬁc', 'impractic', 'sufﬁcient', 'sgsf', 'forg', 'lou', 'speciﬁc', 'conduc', 'metaform', 'pivot', 'latter', 'clue', 'univers', 'anoseg', 'lean', 'stpm', 'epoch', 'dozen', 'catch', 'black', 'swan', 'confirm', 'deviant', 'unseen', 'proceed', 'unbound', 'obvious', 'infrequ', 'line', 'think', 'stem', 'inappropri', 'unusu', 'diffusionad', 'queri', 'flawless', 'essenc', 'fastflow', 'infeas', 'nsa', 'poisson', 'seamlessli', 'inpaint', 'suggest', 'svdd', 'variant', 'knn', 'spade', 'render', 'al', 'spark', 'renew', 'rotnet', 'intrus', 'ℛd', 'ood', 'never', 'safe', 'puzzl', 'pretext', 'connet', 'encount', 'intern', 'impact', 'unsolv', 'harm', 'outsid', 'enclosur', 'distant', 'proxim', 'vae', 'ssad', 'tight', 'envelop', 'departur', 'cbigan', 'bigan', 'mvtec', 'invert', 'svhn', 'explan', 'astronom', 'geotranform', 'inlier', 'artifact', 'poison', 'crack', 'stain', 'substructur', 'scratch', 'inaccess', 'dsebm', 'determinist', 'ebm', 'ae', 'vanilla', 'unbalanc', 'instabl', 'vanish', 'supervisor', 'synthes', 'chosen', 'back', 'aae', 'aggreg', 'arbitrarili', 'compel', 'highdimension', 'utmost', 'impend', 'regardless', 'reformul', 'unreferenc', 'easier', 'tini', 'mit', 'nyu', 'dicult', 'lter', 'cortex', 'concur', 'usabl', 'argu', 'simplist', 'useless', 'unrel', 'too', 'confid', 'occupi', 'cadgmm', 'acoust', 'breakdown', 'factori', 'machineri', 'costli', 'da', 'gmm', 'dasvdd', 'interpol', 'nucleu', 'bowl', 'microscopi', 'biolog', 'bioimag', 'neuroplast', 'unabl', 'capac', 'seem', 'fanet', 'feedback', 'polygon', 'blurri', 'touch', 'overlap', 'centroid', 'topcod', 'macaqu', 'section', 'unbias', 'oncolog', 'rebat', 'feder', 'regal', 'faw', 'reform', 'advertis', 'redempt', 'disclosur', 'payment', 'trade', 'style', 'cellular', 'truli', 'assay', 'submit', 'morphometri', 'imagej', 'plugin', 'fluoresc', 'clump', 'imperfect', 'inhomogen', 'wherea', 'border', 'cellprofil', 'scientif', 'modular', 'softwar', 'infrastructur', 'satisfactorili', 'biomark', 'colorect', 'mr', 'radiotherapi', 'unit', 'gpu', 'footprint', 'trainabl', 'shrink', 'recept', 'pancrea', 'denseaspp', 'abdomin', 'prostat', 'kidney', 'gallbladd', 'adren', 'gland', 'dcan', 'routin', 'adenocarcinoma', 'monuseg', 'offici', 'miccai', 'geograph', 'gate', 'adequ', 'induct', 'skip', 'apriori', 'unnecessarili', 'matt', 'hair', 'cerebr', 'microscop', 'physiolog', 'pathophysiolog', 'ieee', 'transact', 'believ', 'rst', 'argument', 'prognos', 'intract', 'entireti', 'haematoxylin', 'eosin', 'h', 'xy', 'nuclear', 'prerequisit', 'marker', 'thousand', 'radhic', 'magnif', 'naïv', 'par', 'interrog', 'synthesi', 'hematoxylin', 'hardnet', 'mainstream', 'transunet', 'tripl', 'perceptron', 'contradict', 'med', 'althought', 'languag', 'inquir', 'quo', 'ask', 'inquiri', 'add', 'cpfnet', 'imbalanc', 'polyp', 'gastrointestin', 'experienc', 'gastroenterologist', 'anatomi', 'workflow', 'malf', 'registr', 'mi', 'transattunet', 'retin', 'diabet', 'hypertens', 'lightweight', 'hovernet', 'manuscript', 'conic', 'convnext', 'hed', 'afterward', 'pannuk', 'semi', 'exhaust', 'patchperpix', 'crossov', 'assembl', 'conceptu', 'overhead', 'explos', 'har', 'wordnet', 'synset', 'panopt', 'inferior', 'segmentor', 'sheet', 'mous', 'cognit', 'pack', 'profession', 'annotatorj', 'compart', 'unimagin', 'cellseg', 'multiplex', 'curat', 'lab', 'cytoplasm', 'mammalian', 'unresolv', 'basin', 'sink', 'phenotyp', 'lose', 'contact', 'pile', 'cytometri', 'swin', 'unet', 'promot', 'transfus', 'aggress', 'deepen', 'await', 'hounsfield', 'relationship', 'climat', 'equiti', 'justic', 'absent', 'strateg', 'want', 'financi', 'regulatori', 'european', 'union', 'stringent', 'regul', 'understood', 'organis', 'tie', 'vocat', 'anticipatori', 'young', 'adult', 'manageri', 'chasm', 'hire', 'item', 'colleg', 'leader', 'len', 'toolbox', 'econom', 'wealth', 'steroid', 'hormon', 'transcript', 'metabol', 'eukaryot', 'begin', 'unoccupi', 'oestrogen', 'receptor', 'thought', 'bind', 'transloc', 'prepar', 'cytosol', 'unfil', 'resid', 'estrogen', 'endocrin', 'therapi', 'estradiol', 'rat', 'uterin', 'uterotrop', 'administr', 'matur', 'ovariectom', 'dissoci', 'nm', 'display', 'curv', 'sigmoid', 'drug', 'respond', 'stimulus', 'intent', 'perturb', 'discov', 'rarer', 'fortun', 'lifetim', 'brightfield', 'regim', 'deepbac', 'bacteri', 'showcas', 'cultur', 'unwrap', 'biophys', 'fate', 'mechanist', 'corpu', 'underus', 'microbiolog', 'deeploc', 'hc', 'genom', 'molecular', 'breed', 'deepcel', 'scientist', 'protein', 'subcellular', 'yeast', 'auditori', 'ossicl', 'feasibl', 'hrct', 'malleu', 'incu', 'stape', 'omic', 'characteris', 'pet', 'complement', 'sim', 'diffract', 'versatil', 'wsi', 'reproduc', 'write', 'purchas', 'qupath', 'deepmib', 'fastpatholog', 'actin', 'microridg', 'evolutionarili', 'conserv', 'protrus', 'apic', 'squamou', 'zebrafish', 'epiderm', 'actomyosin', 'tta', 'cellpos', 'generalist', 'membran', 'retrain', 'na', 'throughput', 'barrier', 'alloc', 'cba', 'upsampl', 'segnet', 'rural', 'topmost', 'bottommost', 'rightmost', 'leftmost', 'postprocess', 'hyperparamet', 'arguabl', 'copiou', 'nuset', 'tightli', 'weka', 'tw', 'auspic', 'symposium', 'lay', 'tissuenet', 'mesmer', 'bit', 'utilis', 'atla', 'caenorhabd', 'elegan', 'embryo', 'lineag', 'migrat', 'nematod', 'spatiotempor', 'metazoan', 'development', 'attentionboost', 'pay', 'monolay', 'overlay', 'cycl', 'unreason', 'sole', 'merg', 'situ', 'hypothesi', 'eye', 'expans', 'packag', 'interfac', 'stringer', 'et', 'cytokit', 'backgroundmultiplex', 'intracellular', 'avenu', 'compat', 'friendli', 'http', 'biologist', 'rewritten', 'python', 'plate', 'viewer', 'rna', 'mitogen', 'verif', 'paint', 'dye', 'eight', 'organel', 'multiwel', 'primer', 'trigger', 'aleator', 'epistem', 'calibr', 'mitig', 'grader', 'exact', 'extra', 'clariti', 'rater', 'disagr', 'volumetr', 'steadi', 'dropout', 'voxelwis', 'dermatolog', 'beat', 'trust', 'otherwis', 'bad', 'persist', 'sought', 'longitudin', 'volumetri', 'bratumia', 'backpropag', 'stop', 'decay', 'penal', 'backprop', 'regularis', 'minimis', 'mnist', 'book', 'dna', 'transgen', 'localis', 'exponenti', 'immunofluoresc', 'implicit', 'confoc', 'morphogenesi', 'chromatin', 'informat', 'irreproduc', 'fish', 'nonrandom', 'arrang', 'carcinogenesi', 'glioma', 'onset', 'glean', 'bud', 'tb', 'front', 'exceedingli', 'luad', 'subtyp', 'upstream', 'enrich', 'recognis', 'medal', 'legal', 'interobserv', 'centuri', 'microanatom', 'seed', 'finest', 'richest', 'scanner', 'amen', 'backgroundsupervis', 'problemat', 'exemplar', 'outnumb', 'flood', 'facil', 'carta', 'endotheli', 'angiogenesi', 'antiangiogen', 'ec', 'pathway', 'increment', 'missform', 'utnet', 'unexplor', 'innat', 'nnformer', 'interleav', 'scant', 'atyp', 'cotr', 'born', 'pictur', 'unclear', 'artefact', 'hole', 'inaccuraci', 'voxel', 'convey', 'regularli', 'cerebrum', 'cerebellum', 'brainstem', 'cartilag', 'meniscu', 'knee', 'relaxometri', 'degen', 'osteoarthr', 'oa', 'cohort', 'spoil', 'focal', 'simpler', 'trail', 'tensorflow', 'dataflow', 'node', 'multicor', 'cpu', 'asic', 'tpu', 'choroid', 'neovascular', 'oct', 'cnv', 'wet', 'macular', 'amd', 'thorac', 'oar', 'irradi', 'relax', 'upper', 'bladder', 'rectum', 'male', 'pelvic', 'inprost', 'radiat', 'nearbi', 'rt', 'abdomen', 'pelvi', 'asymmetr', 'invis', 'rectal', 'dilat', 'ctv', 'ddcnn', 'implant', 'efficaci', 'head', 'neck', 'inner', 'outer', 'wall', 'urographi', 'thin', 'side', 'dice', 'weigh', 'j', 'weakli', 'youden', 'adequaci', 'signiﬁc', 'irrespect', 'offset', 'parallelli', 'disc', 'glaucoma', 'favour', 'worth', 'bell', 'whistl', 'succeed', 'substantia', 'nigra', 'parkinson', 'pretti', 'lge', 'gadolinium', 'infarct', 'indistinct', 'srscn', 'cta', 'chest', 'bottl', 'skull', 'cbct', 'craniomaxillofaci', 'cmf', 'mimic', 'pulmonari', 'learningbas', 'isointens', 'infant', 'month', 'myelin', 'semiautom', 'dunet', 'pancreat', 'compromis', 'atrou', 'nih', 'segreg', 'morbid', 'asymptomat', 'retroperitoneum', 'surgic', 'msff', 'surgeri', 'tgfβ', 'pluripot', 'embryon', 'shield', 'inhibitor', 'blastocyst', 'cultiv', 'prime', 'endoderm', 'hesc', 'compet', 'molecul', 'sm', 'stauprimid', 'rapamycin', 'rapa', 'chir', 'travers', 'beauti', 'aforement', 'gted', 'methyl', 'seesaw', 'backgrounddna', 'crosstalk', 'hallmark', 'downregul', 'extracellular', 'adhes', 'cumulu', 'infertil', 'polycyst', 'ovari', 'insulin', 'ecm', 'oocyt', 'coc', 'follicl', 'pco', 'ovarian', 'co', 'enrol', 'naiv', 'rhesu', 'monkey', 'epiblast', 'transient', 'nonexist', 'primat', 'blastomer', 'hatch', 'roadmap', 'trophectoderm', 'importantli', 'leukemia', 'inhibitori', 'esc', 'circuitri', 'ncrna', 'microrna', 'mirna', 'lif', 'fetal', 'bovin', 'serum', 'fb', 'primordi', 'germ', 'erk', 'pgc', 'reprogram', 'egc', 'vitro', 'somat', 'exogen', 'commit', 'elucid', 'kina', 'mapk', 'mek', 'β', 'ipsc', 'road', 'genotyp', 'pharmacolog', 'golden', 'mouse', 'fibroblast', 'fgf', 'wnt', 'glycogen', 'mesc', 'nonpermiss', 'uncommit', 'denot', 'predetermin', 'programm', 'blank', 'slate', 'foundat', 'suppress', 'vertebr', 'mysteri', 'nodal', 'lefti', 'symmetri', 'teratocarcinoma', 'preimplant', 'inject', 'subclon', 'refractori', 'adenomat', 'polyposi', 'coli', 'apc', 'synthas', 'canon', 'disrupt', 'therapeut', 'hpsc', 'pharmaceut', 'psc', 'matrigel', 'hepat', 'suspens', 'stir', 'bioreactor', 'agonist', 'hydrocortison', 'alveolar', 'atii', 'regen', 'precursor', 'supplement', 'hlc', 'invalu', 'toxicolog', 'coax', 'rout', 'streak', 'dissect', 'cocktail', 'reviewth', 'findingsrec', 'hepatocyt', 'laminin', 'activin', 'synergist', 'herald', 'injuri', 'dose', 'recombin', 'ligand', 'he', 'specifi', 'phosphatidylinositol', 'antagon', 'mesendoderm', 'embryoid', 'eb', 'insert', 'collagen', 'scaffold', 'dish', 'activina', 'pluripotenti', 'vivo', 'beta', 'transplant', 'schema', 'progenitor', 'hipsc', 'implic', 'surmount', 'realiti', 'dermal', 'subtre', 'gartner', 'borgwardt', 'kashima', 'mahet', 'sylvest', 'isomorph', 'cleft', 'amino', 'acid', 'motif', 'phylogenet', 'enzym', 'again', 'bruijn', 'read', 'suffix', 'basket', 'itemset', 'runtim', 'shortest', 'path', 'log', 'δ', 'diamet', 'charact', 'nest', 'subgraph', 'mlg', 'sperm', 'epigenom', 'father', 'child', 'inherit', 'egg', 'diet', 'toxic', 'stress', 'epigenet', 'reproduct', 'locus', 'mac', 'histon', 'fdr', 'instal', 'manorm', 'gateway', 'cage', 'disclos', 'unspecif', 'methylom', 'cpg', 'ncbi', 'geo', 'archiv', 'omnibu', 'microarray', 'metadata', 'searchabl', 'freeli', 'download', 'deposit', 'trimethyl', 'ly', 'poi', 'subunit', 'cxxc', 'finger', 'unmethyl', 'island', 'cgi', 'unanticip', 'declin', 'treatabl', 'anovulatori', 'ivf', 'transcriptom', 'porcin', 'granulosa', 'folliculogenesi', 'ovul', 'luteum', 'intercellular', 'lutein', 'follicular', 'fsh', 'lh', 'preovulatori', 'gonadotropin', 'twelv', 'pregnanc', 'microphysiolog', 'womb', 'matern', 'placent', 'microenviron', 'invas', 'tmt', 'proteom', 'fluid', 'manifest', 'etiolog', 'dysfunct', 'glycat', 'reactiv', 'endogen', 'hyperglycemia', 'imper', 'biomechan', 'dysregul', 'fertil', 'cyclic', 'recruit', 'displac', 'atresia', 'stand', 'mii', 'metaphas', 'suscept', 'ovulatori', 'gv', 'subfertil', 'predictor', 'backgroundpolycyst', 'centr', 'fatti', 'obes', 'ffa', 'lipid', 'aetiolog', 'pathogenesi', 'partli', 'cdna', 'normoovulatori', 'oligonucleotid', 'subdivid', 'bmi', 'overweight', 'subgroup', 'feeder', 'cytokin', 'calf', 'multifactori', 'ref', 'hypomethyl', 'demethyl', 'progeni', 'substrat', 'chromosom', 'depict', 'purifi', 'polypeptid', 'murin', 'continuum', 'rabbit', 'document', 'morula', 'bulk', 'oxid', 'phosphoryl', 'glycolysi', 'repress', 'landscap', 'gastrul', 'ontogeni', 'recapitul', 'anlag', 'landmark', 'sri', 'sexual', 'dimorph', 'convers', 'amnion', 'mesoderm', 'mammal', 'diverg', 'uncov', 'cardiogenesi', 'viabl', 'offspr', 'biotechnolog', 'bepsc', 'scnt', 'epsc', 'broader', 'extraembryon', 'pipsc', 'germlin', 'chimer', 'tf', 'elus', 'icm', 'interferon', 'te', 'varianc', 'scrna', 'govern', 'herein', 'serial', 'epi', 'cynomolgu', 'macaca', 'fasciculari', 'cyepi', 'thereaft', 'repair', 'embryogenesi', 'zygot', 'zga', 'wastag', 'nontransgen', 'earliest', 'later', 'toggl', 'deacetylas', 'chicken', 'apoptosi', 'spermatogonia', 'suppressor', 'tcga', 'deregul', 'antisens', 'oncomir', 'potent', 'lowli', 'abundantli', 'hypothes', 'immort', 'unlimit', 'procur', 'passag', 'noncod', 'drosophila', 'melanogast', 'biogenesi', 'rnai', 'posttranscript', 'dsrna', 'interf', 'sirna', 'pirna', 'indefinit', 'gamet', 'ectop', 'hypertrophi', 'pparδ', 'peroxisom', 'atrial', 'natriuret', 'anf', 'secretom', 'perinat', 'amniot', 'leftov', 'trimest', 'prenat', 'haf', 'endow', 'paracrin', 'prolif', 'iii', 'wast', 'schedul', 'nonetheless', 'retrovir', 'caveat', 'transfect', 'overexpress', 'apoptot', 'silenc', 'emt', 'knockout', 'microprocessor', 'slowli', 'accumul', 'reiter', 'hhfc', 'ip', 'complementar', 'nucleotid', 'sb', 'cleavag', 'physoxia', 'oxygen', 'vascularis', 'hyperox', 'rodent', 'unspeci', 'replic', 'gamma', 'pe', 'burst', 'microdroplet', 'bl', 'unravel', 'totipot', 'ex', 'uno', 'plure', 'apex', 'expend', 'explant', 'permiss', 'recalcitr', 'diapaus', 'tgfß', 'eg', 'chick', 'avian', 'vaccin', 'blastoderm', 'gonad', 'stood', 'regener', 'succe', 'preliminari', 'flavonoid', 'calycopterin', 'preferenti', 'antiprolif', 'pediatr', 'gct', 'adolesc', 'epidemiolog', 'older', 'multiag', 'regimen', 'gallic', 'curcumin', 'cytotox', 'phenol', 'mtt', 'nitrit', 'ro', 'mitochondri', 'gsh', 'annexin', 'unipot', 'tgfb', 'tgfbeta', 'eventu', 'motil', 'sustain', 'counterpart', 'alkalin', 'postcoitum', 'meiotic', 'steel', 'leukaemia', 'coitum', 'dpc', 'karyotyp', 'phosphatas', 'ap', 'antigen', 'potenc', 'leukocytopenia', 'xinjiang', 'uygur', 'june', 'leukocytosi', 'volunt', 'suspend', 'locu', 'basal', 'mesenchym', 'plastic', 'imprint', 'lncrna', 'testi', 'puberti', 'thyroid', 'spermatogenesi', 'juvenil', 'pulsatil', 'concert', 'amplif', 'sertoli', 'undifferenti', 'quick', 'clearli', 'nanog', 'messeng', 'mrna', 'untransl', 'fuell', 'recipi', 'shotgun', 'rnase', 'drosha', 'subclass', 'pacemak', 'bradycardia', 'enteroviru', 'coxsackieviru', 'viral', 'vmc', 'cardiomyopathi', 'dcm', 'arrest', 'fibrillin', 'marfan', 'mf', 'pathogen', 'myocardi', 'cardiomyocyt', 'cm', 'biomatric', 'hardest', 'pump', 'g', 'heartbeat', 'circul', 'elderli', 'concis', 'ischem', 'ihd', 'vascular', 'bmp', 'eomesodermin', 'eom', 'utero', 'congenit', 'malform', 'puriti', 'surpris', 'percent', 'newborn', 'coupl', 'recess', 'jervel', 'jln', 'rhythm', 'homozyg', 'delay', 'rectifi', 'potassium', 'ik', 'termin', 'compens', 'consequenti', 'peg', 'prompt', 'viabil', 'picmi', 'upregul', 'parenthood', 'steril', 'greatest', 'polyethylen', 'glycol', 'polym', 'thiol', 'acryl', 'michael', 'reaction', 'cryopreserv', 'mef', 'stemflex', 'thaw', 'untarget', 'dedifferenti', 'glucos', 'check', 'grain', 'ascit', 'zajdela', 'parent', 'daughter', 'clonal', 'sublin', 'holoclon', 'meroclon', 'distal', 'retain', 'intact', 'blockad', 'janu', 'transduc', 'episc', 'rewir', 'immatur', 'epitom', 'conform', 'lifr', 'anecdot', 'ago', 'domest', 'unsuccess', 'notabl', 'except', 'shortli', 'appar', 'pig', 'pepisc', 'bfgf', 'superfamili', 'preclin', 'genuin', 'ppsc', 'mxv', 'kofl', 'flat', 'ectoderm', 'metast', 'bona', 'fide', 'faith', 'livestock', 'dazl', 'tet', 'pristin', 'gsk', 'underpin', 'heterochromatin', 'pericentromer', 'multicellular', 'haematopoiet', 'hsc', 'silent', 'prolong', 'symmetr', 'morphogenet', 'extrins', 'teratoma', 'nurd', 'said', 'undefin', 'liaison', 'readili', 'embo', 'contrari', 'phosphoinositid', 'welham', 'allel', 'reset', 'germin', 'notch', 'asynchron', 'nonneur', 'mosaic', 'biochem', 'tyrosin', 'chimaer', 'did', 'engag', 'id', 'homeodomain', 'effector', 'myc', 'elev', 'withdraw', 'kinet', 'analogu', 'similarli', 'helper', 'lifrβ', 'eklf', 'erythroid', 'multipotenti', 'hematopoiet', 'nich', 'indrosophila', 'hub', 'unpair', 'cdm', 'myeloid', 'outgrowth', 'solubl', 'dia', 'eman', 'terra', 'synchron', 'clock', 'left', 'presomit', 'axial', 'skeleton', 'skelet', 'exterior', 'interior', 'disposit', 'placement', 'exteriorli', 'impart', 'primordium', 'eleg', 'embryolog', 'appreci', 'shell', 'coil', 'snail', 'cilium', 'leftward', 'invertebr', 'lr', 'unansw', 'caviti', 'calcium', 'thread', 'viscer', 'dictat', 'sided', 'fascin', 'voltag', 'dorsoanterior', 'midlin', 'xenopu', 'homologu', 'handed', 'gut', 'morphogen', 'chiral', 'macroscop', 'inversu', 'transposit', 'iib', 'sil', 'accompani', 'antagonist', 'diploid', 'graft', 'scid', 'ﬁbroblast', 'unexpectedli', 'dispens', 'men', 'founder', 'reintroduc', 'multilineag', 'chimera', 'chaotic', 'su', 'scrofa', 'stabli', 'spontan', 'cystic', 'confluenc', 'round', 'melanot', 'mutant', 'larva', 'hemocyt', 'flatten', 'lamellocyt', 'caudal', 'fat', 'phagocytosi', 'particul', 'escap', 'adipos', 'deplet', 'abrog', 'lymphoid', 'tcf', 'lef', 'jak', 'esrrb', 'master', 'trophoblast', 'intrauterin', 'lethal', 'occup', 'null', 'orchestr', 'autoregulatori', 'knockdown', 'orphan', 'controversi', 'transactiv', 'plasma', 'neonat', 'testicular', 'gpsc', 'sox', 'glial', 'nervou', 'neurogenesi', 'clone', 'gelatin', 'electrospun', 'seminifer', 'tubul', 'fabric', 'electrospin', 'sem', 'cytobiocompat', 'purposeto', 'ksr', 'aicar', 'backgroundpluripot', 'methyltransferas', 'hdac', 'valproic', 'vpa', 'oncogen', 'backgroundth', 'fvb', 'tamoxifen', 'inbr', 'isogen', 'prevail', 'absolut', 'mpsc', 'max', 'dusp', 'inactiv', 'dephosphoryl', 'klf', 'perk', 'evok', 'cochlin', 'autocrin', 'stimulu', 'ablat', 'imped', 'unidentifi', 'sto', 'cylind', 'smad', 'circuit', 'chose', 'soxb', 'multifacet', 'hematopoiesi', 'leukemogenesi', 'intervent', 'array', 'serin', 'intestin', 'homeostasi', 'splice', 'nutrient', 'circadian', 'phosphoproteom', 'xxx', 'truncat', 'predomin', 'tumorigenesi', 'oncotherapi', 'multifunct', 'threonin', 'aberr', 'inflamm', 'neurodegen', 'polyubiquitin', 'destruct', 'ubiquitin', 'disassembl', 'turnov', 'posttransl', 'axin', 'proteasom', 'slightli', 'thefus', 'gid', 'rg', 'seven', 'pppspx', 'coreceptor', 'ldl', 'casein', 'talk', 'heteromer', 'oligomer', 'elicit', 'secret', 'shed', 'deactiv', 'nucleocytoplasm', 'repertoir', 'exert', 'transmembran', 'partnership', 'frog', 'plasminogen', 'digest', 'megakaryocyt', 'platelet', 'cope', 'cord', 'option', 'chronic', 'cholestat', 'neoplasm', 'orthotrop', 'stromal', 'protract', 'noggin', 'micrograv', 'respiratori', 'tract', 'smg', 'rotari', 'br', 'spheroid', 'organoid', 'haec', 'qpcr', 'repsox', 'neuroectoderm', 'blockag', 'tube', 'sodium', 'butyr', 'islet', 'pp', 'dimethyl', 'epithelium', 'thymu', 'epcam', 'microparticl', 'lo', 'approv', 'dissolv', 'tank', 'dextran', 'sulfat', 'orthotop', 'donor', 'conjunct', 'bioprocess', 'inocul', 'nanofibrillar', 'cardiomyogenesi', 'intrapulmonari', 'autolog', 'distress', 'backgroundlung', 'ard', 'zealand', 'marrow', 'aspir', 'bronchopulmonari', 'dysplasia', 'bpd', 'preterm', 'stunt', 'clarif', 'pathomechan', 'novo', 'alveolus', 'aeii', 'endotrach', 'pneumocyt', 'keratinocyt', 'dexamethason', 'plu', 'camp', 'kgf', 'organogenesi', 'isobutylmethylxanthin', 'dci', 'foregut', 'hlo', 'inexhaust', 'suppli', 'requisit', 'biomanufactur', 'ssb', 'planar', 'stomach', 'hepatogen', 'wharton', 'nabu', 'postmortem', 'msc', 'purif', 'fac', 'harbor', 'cardiomyogen', 'rhythmic', 'week', 'ep', 'hepatoblast', 'hb', 'infecti', 'organotyp', 'mimick', 'off', 'shelf', 'proof', 'sex', 'ethnic', 'forth', 'myogen', 'muscular', 'dystrophi', 'fiber', 'duchenn', 'somit', 'psm', 'secondari', 'myogenesi', 'striat', 'engraft', 'pdgfra', 'myoblast', 'multipot', 'dystroph', 'endless', 'dmd', 'neuromuscular', 'paraxi', 'reconstitut', 'myofib', 'injur', 'alzheim', 'leap', 'nlp', 'bert', 'mlm', 'word', 'emb', 'token', 'proarrhythmia', 'cardiotox', 'mea', 'arrhythm', 'neoplast', 'nonneoplast', 'ill', 'interview', 'neuropsycholog', 'linguist', 'deliber', 'electroencephalogram', 'dementia', 'spoken', 'ovitad', 'wankerl', 'lm', 'rwthlm', 'mci', 'dnnlm', 'verbal', 'utter', 'multilingu', 'french', 'english', 'dementiabank', 'nd', 'mmse', 'montreal', 'moca', 'narr', 'autopsi', 'cours', 'whom', 'mortem', 'icu', 'reshap', 'kepler', 'plm', 'factual', 'ke', 'kg', 'textual', 'languagerepresent', 'invest', 'yago', 'dbpedia', 'wikidata', 'blurb', 'huggingfac', 'contractil', 'proarrhythm', 'silico', 'torsad', 'biomimicri', 'bioelectron', 'topograph', 'electr', 'nativ', 'nanotechnolog', 'biopolym', 'mexico', 'ehr', 'medica', 'nort', 'pressur', 'electrophysiolog', 'arrhythmia', 'microelectrod', 'cardiomda', 'cardioact', 'qt', 'lqt', 'repolar', 'panel', 'tdp', 'confluent', 'ryanodin', 'catecholaminerg', 'polymorph', 'ventricular', 'tachycardia', 'agx', 'xavier', 'nvidia', 'santa', 'clara', 'usa', 'binocular', 'stereolab', 'san', 'francisco', 'zed', 'gleason', 'grade', 'metastasi', 'struggl', 'gigapixel', 'scannet', 'lymph', 'metastas', 'postul', 'scarc', 'corpus', 'textbook', 'cooki', 'theft', 'fifti', 'questionnair', 'skill', 'destroy', 'andthink', 'adress', 'ctp', 'voic', 'gender', 'carolina', 'transcrib', 'unimpair', 'multiethn', 'literaci', 'distilbert', 'cheaper', 'budget', 'unlikelihood', 'dull', 'insidi', 'written', 'visuospati', 'judgment', 'stumbl', 'eleph', 'room', 'salienc', 'impli', 'pubm', 'ovid', 'sentenc', 'impetu', 'inexpens', 'escal', 'asr', 'shortterm', 'bilstm', 'debilit', 'motor', 'afflict', 'stutter', 'uncontrol', 'interject', 'sensorimotor', 'therapist', 'pw', 'pertin', 'statement', 'interspeech', 'competitor', 'sbert', 'aphasia', 'apraxia', 'agnosia', 'inabl', 'fasttext', 'spectrogram', 'cepstral', 'mfcc', 'mental', 'ser', 'phrase', 'sentiment', 'polar', 'opinion', 'cdt', 'lexic', 'dialogu', 'syntact', 'mistakenli', 'irrelev', 'judg', 'promin', 'mcadnnet', 'scholarli', 'halt', 'prodrom', 'disharmoni', 'batch', 'amyloid', 'plaqu', 'neurofibrillari', 'tangl', 'smri', 'malnutrit', 'dehydr', 'caregiv', 'meal', 'hydrat', 'regain', 'comfort', 'deepad', 'uptak', 'cortic', 'educ', 'everyon', 'notic', 'perman', 'figur', 'threefold', 'dollar', 'isn', 'tau', 'draft', 'vocal', 'ci', 'phone', 'administ', 'paralinguist', 'prosod', 'exploratori', 'polit', 'exam', 'swedish', 'neurolog', 'nineteen', 'march', 'bag', 'adresso', 'sota', 'hippocampu', 'grammat', 'ﬂuenci', 'prosodi', 'perplex', 'rwth', 'aachen', 'lstm', 'treebank', 'bla', 'bare', 'preced', 'spinal', 'disabl', 'episod', 'backgroundmild', 'underw', 'fuel', 'unobtrus', 'retel', 'fluenci', 'svf', 'teste', 'mandarin', 'chine', 'lexicosyntact', 'movi', 'rethink', 'taxonomi', 'shareabl', 'monolingu', 'portugues', 'discours', 'entitl', 'cohes', 'speak', 'phonet', 'japanes', 'vocabulari', 'unreach', 'examine', 'dysphasia', 'coral', 'irrecover', 'ampl', 'beg', 'inextric', 'lessen', 'dyad', 'troubl', 'tib', 'vd', 'depart', 'summaryintroduct', 'hypothesis', 'huntington', 'dat', 'dement', 'nondement', 'pd', 'consciou', 'recollect', 'resourceintens', 'lumbar', 'punctur', 'incipi', 'subtest', 'efa', 'bivari', 'mae', 'interrat', 'fight', 'iri', 'murdoch', 'operation', 'sporad', 'lobar', 'ftld', 'microlinguist', 'oral', 'phonolog', 'discurs', 'macrolinguist', 'narrat', 'efﬁcient', 'movement', 'practition', 'avatar', 'gcnn', 'pitt', 'glove', 'syntax', 'interrel', 'ensu', 'logopen', 'lvppa', 'amnest', 'verb', 'adject', 'profound', 'spectrograph', 'praat', 'noun', 'pronounr', 'ttr', 'brunet', 'w', 'honor', 'forti', 'thirti', 'greek', 'moder', 'nc', 'preciou', 'pragmat', 'kathleen', 'fraser', 'graduat', 'toronto', 'thesi', 'boston', 'psychiatr', 'scd', 'depress', 'uniformli', 'disproportion', 'dialog', 'pope', 'davi', 'paraphras', 'clarifi', 'dissolut', 'nonstructur', 'evidenc', 'anomia', 'spare', 'deﬁcit', 'trillion', 'maria', 'yancheva', 'british', 'novelist', 'die', 'agatha', 'christi', 'suspect', 'jame', 'healthili', 'garrard', 'pdt', 'edt', 'adt', 'rubric', 'outlin', 'proposit', 'john', 'hopkin', 'nun', 'reexamin', 'eighteen', 'aphas', 'computeris', 'cpidr', 'spider', 'roughli', 'adverb', 'preposit', 'montylingua', 'liu', 'mismatch', 'mtl', 'structbert', 'roberta', 'nlu', 'elman', 'simcs', 'luke', 'declutr', 'don', 'news', 'ckg', 'opposit', 'coreferenti', 'plain', 'corefer', 'glue', 'nli', 'erni', 'zhang', 'ngram', 'stanford', 'squad', 'blstm', 'sspt', 'devlin', 'pairwis', 'permeat', 'absorb', 'pce', 'elmo', 'nmt', 'subclaus', 'spotlight', 'hit', 'genkgc', 'xlnet', 'ppke', 'krl', 'kglm', 'recommend', 'necessit', 'symbol', 'encycloped', 'everyth', 'colak', 'adept', 'grasp', 'commonsens', 'stale', 'encyclopedia', 'marcu', 'winograd', 'levesqu', 'learner', 'taskspecif', 'webpag', 'webtext', 'coqa', 'softmax', 'kb', 'mention', 'kganet', 'actknow', 'infus', 'fewshot', 'tucker', 'kgner', 'lexicon', 'ner', 'terminolog', 'thoth', 'port', 'transg', 'jaket', 'begun', 'el', 'bart', 'implaus', 'catastroph', 'forget', 'yamada', 'wordpiec', 'spirit', 'knowbert', 'peter', 'rememb', 'linker', 'polyad', 'cp'])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e9be22d7ec8756c432729b2c21c1bec911fbdc4c', 5.199029056889167),\n",
       " ('ae0d3f3f13f10d72ba151405b751b273ed3e82d5', 4.9760064484609146),\n",
       " ('1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97', 4.955442995939102),\n",
       " ('fc2c4aaea508973c26fe05edcb3202005e04494a', 4.641878853991672),\n",
       " ('2d4713ce1df60f771b65e900fd02352989df82ef', 4.619671394099153),\n",
       " ('cf4ade985aa66109909e56c9df93ff0b3ef2ca98', 4.619671394099153),\n",
       " ('f1d53e9c301d78e0b148e2f91adfc4fde2621ee5', 4.593405543928522),\n",
       " ('dc9a66e0f329de8054f4ab845331fb7183987418', 4.561258741386764),\n",
       " ('0f5a25b52bef4f5fb969887224ba84e89c32e47c', 4.561258741386764),\n",
       " ('4a2d0374dac0b4b43de606a610dde145054f01ba', 4.561258741386764)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search(\n",
    "    \"vision tracking\",\n",
    "    \"computer vision\",\n",
    "    10,\n",
    "    \"ltn-lnn\",\n",
    "    0.5,\n",
    "    True,\n",
    "    [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An optimized region-based color transfer method for night vision application\n",
      "Authors: ['Tanish Hemalbhai Zaveri', 'Mukesh A. Zaveri', 'Ishit Makwana', 'Harshit Mehta']\n",
      "Abstract: Modern night-vision systems like image intensifiers and thermal cameras enable operations at night and in adverse weather conditions. Modern night vision camera provides false-colored fused image as an output which is unnatural in appearance and it is therefore hard to interpret. In this paper, a region-based natural color mapping method for night vision imagery is presented. The proposed method colorizes the night vision imagery by using a combined framework consisting of hill-climbing… \n",
      "Year: 1 December 2010\n",
      "References: ['2ad3927e656867ecb47c694b3197806b1143faa7', 'ff85aa39c86b57e045e045537ac32813522375fd', 'd5986548cac9950f6837e71272da94e9178b9618', '6327caf546ef6e47b9b05735cc5ec572733cb756', '45816243ca4f26be099e14d09744a7d906e8b5c9', '5deb3a34a773e9a620eee09136a163fd4c253ee3', '05f62d07ad886cb9b7beea3da28ea807cb67160f', '9b80d7498e1197870e72da035c8079a76d3e58a6', '53fc0415e0d00f9691994a49b8232a1cc2dfad5f', 'd5c6edb53dc41f298f145041cd2c53e40e3acf2b']\n",
      "ID: e9be22d7ec8756c432729b2c21c1bec911fbdc4c\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "5.199029056889167\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: Egocentric Prediction of Action Target in 3D\n",
      "Authors: ['Yiming Li', 'Ziang Cao', 'Andrew Liang', 'Benjamin Liang', 'Luoyao Chen', 'Hang Zhao', 'Chen Feng']\n",
      "Abstract: We are interested in anticipating as early as possible the target location of a person's object manipulation action in a 3D workspace from egocentric vision. It is important in fields like human-robot collaboration, but has not yet received enough attention from vision and learning communities. To stimulate more research on this challenging egocentric vision task, we propose a large multimodality dataset of more than 1 million frames of RGB-D and IMU streams, and provide evaluation metrics… \n",
      "Year: 24 March 2022\n",
      "References: ['73bc8e056d4318b41bb76bf270442f52e8080f82', '792829f263a523eedf1a8748ec23d25cf664c2b4', '07cf6c4c1714a0cd88e5c1566aac9df40e111db7', 'ed78a2671ef61c031759c01434678c282f23faec', '994481d46df92709b61614f5e756e40df4117622', 'a58a0732664b97b471b795df5812f98f24840490', '8bfa0c14c2ae48c1ee6b145008137e4d69688416', '6021af236342c11c44f681d2aa21b0b46756236a', '7ef40f47e6f20c87391dd77b6e8c081709e1b8bd', '98656db7128d5349822c7a59e705baf618ae2a2d']\n",
      "ID: ae0d3f3f13f10d72ba151405b751b273ed3e82d5\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.9760064484609146\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives\n",
      "Authors: ['Jun Li', 'Junyu Chen', 'Yucheng Tang', 'Bennett A. Landman', 'S. Kevin Zhou']\n",
      "Abstract: Transformer, one of the latest technological advances of deep learning, has gained prevalence in natural language processing or computer vision. Since medical imaging bear some resemblance to computer vision, it is natural to inquire about the status quo of Transformers in medical imaging and ask the question: can the Transformer models transform medical imaging? In this paper, we attempt to make a response to the inquiry. After a brief introduction of the fundamentals of Transformers… \n",
      "Year: 2 June 2022\n",
      "References: ['d0bc2b31b4e6afce9caf0f315e9a7d2c94ccd9fb', '0d5fbaa26646ba95ea5f55b0d6292435cb44bc91', '113c7656628e3a0dec0940b35ccfb30f02e761a6', '9f1b0e4c42a5a85d4c023030557ade4419f82ecf', '9b7ec6c22b8aa0ea825000fdc40f32286c5f7389', 'f2275f86909675d8e3a561534cca575cf94e18da', '92a574d34837b970e6c0610226362e801ca83442', '076a8e778f2e9efb3c2fd45fed534ae9e6035f1b', 'c69003edc5e25e203f138b8de5b2fb9315393ad5', '7b4125cced6db9b14af663de1c593416f314e0da']\n",
      "ID: 1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97\n",
      "Topics: ['Physics']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.955442995939102\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: Transformers in Single Object Tracking: An Experimental Survey\n",
      "Authors: ['Janani Thangavel', 'Thanikasalam Kokul', 'Amirthalingam Ramanan', 'Subha Fernando']\n",
      "Abstract: Single-object tracking is a well-known and challenging research topic in computer vision. Over the last two decades, numerous researchers have proposed various algorithms to solve this problem and achieved promising results. Recently, Transformer-based tracking approaches have ushered in a new era in single-object tracking by introducing new perspectives and achieving superior tracking robustness. In this paper, we conduct an in-depth literature analysis of Transformer tracking approaches by… \n",
      "Year: 23 February 2023\n",
      "References: ['24de23963bec39fe0e39612e2cacb76c83d66f93', '1af342faa6a90612651788917e4bbbd3f06f8410', 'b6eaec7917439d79ce840fa97bc371552e9b6685', '9916ed982600be133ed2d185b70fe721809a3096', '8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b', 'c8b25fab5608c3e033d34b4483ec47e68ba109b7', '522c2da0e51f6d3d50493e7a9a2dfedb7f72e649', '39b27ee48caa5bb68a8c50ef5f02121729847334', 'c1329f91cfa11011712227c8765fbbe38b9f2b7e', 'dc47b17250b639d3a89a716c7216ef69b33f9e33']\n",
      "ID: fc2c4aaea508973c26fe05edcb3202005e04494a\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.641878853991672\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: Long-term Visual Tracking: Review and Experimental Comparison\n",
      "Authors: ['Chang Liu', 'Xiao-Fan Chen', 'Chunjuan Bo', 'Dong Wang']\n",
      "Abstract: As a fundamental task in computer vision, visual object tracking has received much attention in recent years. Most studies focus on short-term visual tracking which addresses shorter videos and always-visible targets. However, long-term visual tracking is much closer to practical applications with more complicated challenges. There exists a longer duration such as minute-level or even hour-level in the long-term tracking task, and the task also needs to handle more frequent target disappearance… \n",
      "Year: 7 November 2022\n",
      "References: ['23f8927f996d56f3b5076d8993a70bcfc70182a1', '3275944117b43cc44beebe7c82bffc13ec8cb0fa', '19d6b9725a59f4b624205829d5f03ac893ca1367', '219e9a4527110baf1feb3df20db12064eeafdfb7', '12508951ba96b7d4c0906ed95542287d3ebdfd95', 'ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7', '1ae15ff20d54d9ffd2a45a9c124c77ad2b419ae3', '786577081e00d69eeac8e9612eaf2dad59765e73', '894e4376750b83b63649cc518b121f345ca0df83', '913cebc279c363fb9476496f096519e27212b3d5']\n",
      "ID: 2d4713ce1df60f771b65e900fd02352989df82ef\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.619671394099153\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: Single Object Tracking Research: A Survey\n",
      "Authors: ['Ruize Han', 'Wei Feng', 'Qing Guo', 'Qinghua Hu']\n",
      "Abstract: Visual object tracking is an important and fundamental task in computer vision, which has many real-world applications, e.g., video surveillance, visual navigation and robotic service. Visual object tracking also has many challenges, such as object loss, object deformation, background clutters, and object fast motion. To solve the above problems and track the target accurately and efficiently, many visual object tracking algorithms have been emerged in recent years. In this paper, we first… \n",
      "Year: 25 April 2022\n",
      "References: ['b7d540cd0de72e984cdec44afa4a4d039cfd5eea', 'eda3368a5198ca55768b07b6f5667aea28baf2cd', 'eb35dff22c4e947dcd753dc38bb8557d8def47cc', '2e7e3b4eb8bc0a7f29ca560b1cceb986a1dcd977', '388d29f001411ff80650f80cf197afc440d98b51', '1098c07a5d80a07c281d3af340ae74b2a6c82317', '505f48d8236eb25f871da272c2ac2fe4b41ea289', 'f5dbe4550d24d5374d9e10fce44a35b105c7ee07', '60863b09592df8ef41bd616ac9df947eb212246a', '91f2b2aeb7e65d0b673ed7e782488b3365027979']\n",
      "ID: cf4ade985aa66109909e56c9df93ff0b3ef2ca98\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.619671394099153\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: The Ninth Visual Object Tracking VOT2021 Challenge Results\n",
      "Authors: ['Matej Kristan', 'Jiri Matas', 'Ale{\\\\vs} Leonardis', 'Michael Felsberg', 'Roman P. Pflugfelder', 'J. K{\\\\&quot;a}m{\\\\&quot;a}r{\\\\&quot;a}inen', 'Hyung Jin Chang', 'Martin Danelljan', 'Luka Cehovin Zajc', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Ondrej Drbohlav', 'Jani K{\\\\&quot;a}pyl{\\\\&quot;a}', 'Gustav H{\\\\&quot;a}ger', 'Song Yan', 'Jinyu Yang', 'Zhongqun Zhang', 'Gustavo Javier Fernandez', 'Mohamed H. Abdelpakey', 'Goutam Bhat', 'Llukman Cerkezi', 'Hakan Çevikalp', 'Shengyong Chen', 'Xin Chen', 'Miao Cheng', 'Ziyi Cheng', 'Yu-Chen Chiu', 'Ozgun Cirakman', 'Yutao Cui', 'Kenan Dai', 'Mohana Murali Dasari', 'Qili Deng', 'Xingping Dong', 'Daniel K. Du', 'Matteo Dunnhofer', 'Zhenhua Feng', 'Zhiyong Feng', 'Z. Fu', 'Shiming Ge', 'Rama Krishna Sai Subrahmanyam Gorthi', 'Yuzhang Gu', 'Bilge Gunsel', 'Qing Guo', 'Filiz Gurkan', 'Wencheng Han', 'Yanyan Huang', 'Felix J{\\\\&quot;a}remo Lawin', 'Shang-Jhih Jhang', 'Rongrong Ji', 'Cheng Jiang', 'Yingjie Jiang', 'Felix Juefei-Xu', 'Yin Jun', 'Xiaolong Ke', 'Fahad Shahbaz Khan', 'Byeong Hak Kim', 'Josef Kittler', 'Xiangyuan Lan', 'Jun Ha Lee', 'Bastian Leibe', 'Hui Li', 'Jianhua Li', 'Xianxian Li', 'Yuezhou Li', 'Bo Liu', 'Chang Liu', 'Jingen Liu', 'Li Liu', 'Qingjie Liu', 'Huchuan Lu', 'Wei Lu', 'Jonathon Luiten', 'Jie Ma', 'Ziang Ma', 'Niki Martinel', 'Christoph Mayer', 'Alireza Memarmoghadam', 'Christian Micheloni', 'Yuzhen Niu', 'Danda Pani Paudel', 'Houwen Peng', 'Shoumeng Qiu', 'Aravindh Rajiv', 'Muhammad Abid Rana', 'Andreas Robinson', 'Hasan Saribas', 'Ling Shao', 'Mohamed S. Shehata', 'Furao Shen', 'Jianbing Shen', 'Kristian Simonato', 'Xiaoning Song', 'Zhangyong Tang', 'Radu Timofte', 'Philip H. S. Torr', 'Chi-Yi Tsai', 'Bedirhan Uzun', 'Luc Van Gool', 'Paul Voigtlaender', 'Dong Wang', 'Guangting Wang', 'Liangliang Wang', 'Lijun Wang', 'Limin Wang', 'Linyuan Wang', 'Yong Wang', 'Yunhong Wang', 'Chenyang Wu', 'Gangshan Wu', 'Xiaojun Wu', 'Fei Xie', 'Tianyang Xu', 'Xiang Xu', 'Wanli Xue', 'Bin Yan', 'Wankou Yang', 'Xiaoyun Yang', 'Yu Ye', 'J. Yin', 'Chengwei Zhang', 'Chunhui Zhang', 'Haitao Zhang', 'Kaihua Zhang', 'Kangkai Zhang', 'Xiaohan Zhang', 'Xiaolin Zhang', 'Xinyu Zhang', 'Zhibing Zhang', 'Shao-Chuan Zhao', 'Mingmin Zhen', 'Bineng Zhong', 'Jiawen Zhu', 'Xuefeng Zhu']\n",
      "Abstract: The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative. Results of 71 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The VOT2021 challenge was composed of four sub-challenges focusing on different tracking domains: (i) VOT-ST2021 challenge focused on short-term tracking in RGB, (ii) VOT-RT2021 challenge focused on \"real-time\" short… \n",
      "Year: 1 October 2021\n",
      "References: ['12508951ba96b7d4c0906ed95542287d3ebdfd95', '786577081e00d69eeac8e9612eaf2dad59765e73', '350d507f5d899e4d7293b1aa951aa0f81b9fd30a', '966aad492f75b17f698e981e008b73b51816c6aa', '15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d', '4b1a47709d0546e5bc614bf9a521c550e6881d04', 'c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508', '45512d44f1205bc92775f2e880858b3f23c9f5fd', 'f202feae9ca7b3766e072b6af657beed2236a93c', '0f50914e86b6010586f1772308858de9a418fb9f']\n",
      "ID: f1d53e9c301d78e0b148e2f91adfc4fde2621ee5\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.593405543928522\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: RGBD Object Tracking: An In-depth Review\n",
      "Authors: ['Jinyu Yang', 'Zhe Li', 'Song Yan', 'Feng Zheng', 'Alevs Leonardis', 'Joni-Kristian Kamarainen', 'Ling Shao']\n",
      "Abstract: —RGBD object tracking is gaining momentum in computer vision research thanks to the development of depth sensors. Although numerous RGBD trackers have been pro- posed with promising performance, an in-depth review for comprehensive understanding of this area is lacking. In this paper, we ﬁrstly review RGBD object trackers from different perspectives, including RGBD fusion, depth usage, and tracking framework. Then, we summarize the existing datasets and the evaluation metrics. We benchmark a… \n",
      "Year: 26 March 2022\n",
      "References: ['625aec94369715717158843c3ee288869cbe098f', 'f33b4ba5efdef921383bde48ed1ed4edff86edb9', '487eb86379e979a72ebfef67db6eb8f048d1d258', 'd884af3933148cef3b50fd38c810f5a7763d0fc9', '6290d7a7e353fbfe77e21e4d1086143f5e66312b', 'f202feae9ca7b3766e072b6af657beed2236a93c', 'c06ecdf5b149c322db0381adb6b3fd5ccb31a720', '7681f4c80774c6661980c5a76ffab357cca5f5cf', '3f02406b9b59d6f966c735953930fede1d751d0d', '761a9b5d8750eb63a9717650c4aaca53ce36a364']\n",
      "ID: dc9a66e0f329de8054f4ab845331fb7183987418\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.561258741386764\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: Deep Bidirectional Correlation Filters for Visual Object Tracking\n",
      "Authors: ['Sajid Javed', 'Xiaoxiong Zhang', 'Lakmal D. Seneviratne', 'J. Dias', 'Naoufel Werghi']\n",
      "Abstract: Visual Object Tracking (VOT) is an essential task for many computer vision applications. VOT becomes challenging when a target object faces severe occlusion, drastic illumination changes, and scale variation problems. In the literature, Discriminative Correlation Filters (DCFs)-based tracking methods have achieved promising results in terms of accuracy and efficiency in many complex VOT scenarios. A plethora of DCFs trackers have been proposed which exploit information observed in past frames… \n",
      "Year: 1 July 2020\n",
      "References: ['9e6d1625cb8ae06eaa741bd79c2a89cd98ec8f9a', '5c8a6874011640981e4103d120957802fa28f004', '3eedcf9302f299a1eebbc0f543a366454d6dcefb', '01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c', '09769e80cdf027db32a1fcb695a1aa0937214763', '9ce6dd3ad74236b09a12062b7e86942cff76034d', 'c2046fc4744a9d358ea7a8e9c21c92fd58df7a64', 'ce8c76bfedc5d86faabf0d49dc42a4924f75876d', '9f45b55af027503fab557f55f70e81e43c6c1db7', 'f46318bf67ab6b30284f125ac8bb6f9a7503595e']\n",
      "ID: 0f5a25b52bef4f5fb969887224ba84e89c32e47c\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.561258741386764\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Title: AFOD: Adaptive Focused Discriminative Segmentation Tracker\n",
      "Authors: ['Yiwei Chen', 'Jingtao Xu', 'Jiaqian Yu', 'Qiang Wang', 'ByungIn Yoo', 'Jae-Joon Han']\n",
      "Abstract: Visual object tracking is a fundamental task in computer vision which could be integrated into numerous real-world applications. Traditional object tracking methods focus on providing the bounding box as object position, while some recent trackers start to consider the combination of segmentation module to generate the binary segmentation mask, pursuing more accurate localization. However, how to effectively integrate different information for accurate and robust tracking is an open question… \n",
      "Year: 23 August 2020\n",
      "References: ['12fae9a2c1ed867997e1ca70eba271b3c741c42f', '45512d44f1205bc92775f2e880858b3f23c9f5fd', 'f4f34b56ef957981cebc3d901f49ddd638007d8d', '1190e0210430e8b743af24cdc43efdeef407b669', 'd74169a8fd2f90a06480d1d583d0ae5e980ea951', 'd58e13f7e5e06440c9470a9101ccbb1bfd91b5a1', '48e52aef87084fa17e6ccb20ad9b3a8ec45934f0', '2c8315ae713b3e27c6e9f291a158134d9c516166', '8b74008565b575f9ab7a0962ca5f6955d64db045', '320d05db95ab42ade69294abe46cd1aca6aca602']\n",
      "ID: 4a2d0374dac0b4b43de606a610dde145054f01ba\n",
      "Topics: ['Computer Science']\n",
      "--------------------------------------------------------\n",
      "-------------------Score is: ---------------------------\n",
      "4.561258741386764\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in results:\n",
    "    pretty_print(entry[0])\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"-------------------Score is: ---------------------------\")\n",
    "    print(entry[1])\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>  \n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_by_id(paper_id: str, papers_ds):\n",
    "    for paper in papers_ds:\n",
    "        if paper[\"ID\"] == paper_id:\n",
    "            return paper\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def hit_algorithm(papers, n):\n",
    "    \"\"\"\n",
    "    Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
    "\n",
    "    Parameters\n",
    "    ---------------------------------------------------------------------------------------------------\n",
    "    papers: A list of paper dictionaries with the following keys:\n",
    "            \"id\": A unique ID for the paper\n",
    "            \"title\": The title of the paper\n",
    "            \"abstract\": The abstract of the paper\n",
    "            \"date\": The year in which the paper was published\n",
    "            \"authors\": A list of the names of the authors of the paper\n",
    "            \"related_topics\": A list of IDs for related topics (optional)\n",
    "            \"citation_count\": The number of times the paper has been cited (optional)\n",
    "            \"reference_count\": The number of references in the paper (optional)\n",
    "            \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
    "    n: An integer representing the number of top authors to return.\n",
    "\n",
    "    Returns\n",
    "    ---------------------------------------------------------------------------------------------------\n",
    "    List\n",
    "    list of the top n authors based on their hub scores.\n",
    "    \"\"\"\n",
    "    # Create a graph of authors and papers (all of the authors and papers represented as nodes, and all of the authors who wrote each paper connected to the corresponding paper node by an edge)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for paper in papers:\n",
    "        G.add_node(paper[\"ID\"])\n",
    "        for author in paper[\"Authors\"]:\n",
    "            G.add_node(author)\n",
    "\n",
    "    for paper in papers:\n",
    "        for author in paper[\"Authors\"]:\n",
    "            G.add_edge(author, paper[\"ID\"])\n",
    "\n",
    "    for paper in papers:\n",
    "        for reference in paper[\"References\"]:\n",
    "            if get_paper_by_id(reference, papers) is not None:\n",
    "                for author in paper[\"Authors\"]:\n",
    "                    for second_author in get_paper_by_id(reference, papers)[\"Authors\"]:\n",
    "                        if author != second_author:\n",
    "                            G.add_edge(author, second_author)\n",
    "\n",
    "    # Run the HITS algorithm\n",
    "    hubs, authorities = nx.hits(G)\n",
    "\n",
    "    # Create a list of top n authors based on their hub scores\n",
    "    top_authors = sorted(hubs, key=hubs.get, reverse=True)[:n]\n",
    "    return top_authors\n",
    "\n",
    "\n",
    "# call the hit_algorithm function\n",
    "top_authors = hit_algorithm(papers_dataset, 10)\n",
    "\n",
    "# print the top authors\n",
    "print(top_authors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
    "\n",
    "در فایل recommended_papers.json\n",
    "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
    "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
    "\n",
    "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"recommended_papers.json\", \"r\") as fp:\n",
    "    recommended_papers = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_papers': [{'paperId': 'd9404b4a794c07b5e2cdf3203aabf06d70c6be9b',\n",
       "   'title': 'CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform',\n",
       "   'abstract': 'Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '6568423cfaca7e24c88ea208cb0e67129e43aa9b',\n",
       "   'title': 'Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels',\n",
       "   'abstract': \"We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL.\",\n",
       "   'fieldsOfStudy': ['Computer Science', 'Engineering', 'Mathematics']},\n",
       "  {'paperId': '22cc0ac7fdf06de9a8edb58fd1a1518c0c04d376',\n",
       "   'title': 'LSH forest: self-tuning indexes for similarity search',\n",
       "   'abstract': \"We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search engines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images; peer-to-peer systems desire distributed similarity indexes with low communication cost. We propose an indexing scheme called LSH Forest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH), but improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH's performance guarantees for skewed data distributions while retaining the same storage and query overhead. We show how to construct this index in main memory, on disk, in parallel systems, and in peer-to-peer systems. We evaluate the design with experiments on multiple text corpora and demonstrate both the self-tuning nature and the superior performance of LSH Forest.\",\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '3ac546bd7f554ee4344359b5739ff795a94aa399',\n",
       "   'title': 'SoC, NoC and Hierarchical Bus Implementations of Applications on FPGAs Using the FCUDA Flow',\n",
       "   'abstract': \"The FCUDA project aims to improve programmability of FPGAs and expression of application parallelism in High Level Synthesis (HLS) through the use of the CUDA language. The CUDA language is a popular single-instruction multiple data (SIMD) style programming language with wide adoption, thus offering significant opportunity to bring experienced programmers to FPGA computing. The FCUDA project now has open-sourced the core CUDA to RTL transformation as well as the infrastructure for design space exploration, bus-based andNoC-based on-chip communications, and platform integration with Xilinx's SoC systems. In this paper, we present FCUDA's design space exploration, interconnect and platform integration to present guidelines for selecting system-level infrastructure for an application for the best implementation.\",\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '1e7991f7b297089eca79db1329cfc5c594167a22',\n",
       "   'title': \"The Visual Appearance of User's Avatar Can Influence the Manipulation of Both Real Devices and Virtual Objects\",\n",
       "   'abstract': \"This paper describes two experiments conducted to study the influence of visual appearance of user's avatar (or 3D cursor) on the manipulation of both interaction devices and virtual objects in 3D virtual environments (VE). In both experiments, participants were asked to pick up a virtual cube and place it at a random location in a VE. The first experiment showed that the visual appearance of a 3D cursor could influence the participants in the way they manipulated the real interaction device. The participants changed the orientation of their hand as function of the orientation suggested visually by the shape of the 3D cursor. The second experiment showed that one visual properly of the avatar (i.e., the presence or absence of a directional cue) could influence the way participants picked up the cube in the VE. When using avatars or 3D cursors with a strong directional cue (e.g., arrows pointing to the left or right), participants generally picked up the cube by a specific side (e.g., right or left side). When using 3D cursors with no main directional cue, participants picked up the virtual cube more frequently by its front or top side. Taken together our results suggest that some visual aspects (such as directional cues) of avatars or 3D cursors chosen to display the user in the VE could partially determine his/her behaviour during manipulation tasks. Such an influence could be used to prevent wrong uses or to favour optimal uses of manipulation interfaces such as haptic devices in virtual environments\",\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'fb319da30d7036e393e7c97fa0235f20d81f143e',\n",
       "   'title': 'Customer engagement on social media, brand equity and financial performance: a comparison of the US and Korea',\n",
       "   'abstract': 'PurposeThe purpose of this paper is to analyze the relationship between customer engagement in social media (CESM), brand equity and corporate performance and investigated whether these relationships differed according to cultural factors in the United States and South Korea.Design/methodology/approachWe collected customer engagement on social media data on Facebook and brand equity data from Interbrand for listed companies in the United States and Korea. A total of 405 data sets were analyzed by partial least squares structural equation modeling (PLS-SEM).FindingsResults revealed that CESM did not affect financial performance through a direct path but was found to have a positive indirect path via the mediation of brand equity. In addition, this relationship was found to differ between the United States and South Korea.Originality/valueThis study contributed to the literature on social media and international management by verifying the relationship between CESM, brand equity and financial performance, and by presenting exploratory research results to ascertain if these relationships differ according to the cultural dimension of the country.',\n",
       "   'fieldsOfStudy': ['Business']},\n",
       "  {'paperId': '997b2638992595b23b923f65f3cf98f6316cf9fd',\n",
       "   'title': 'A Kalman Filter-Based Framework for Enhanced Sensor Fusion',\n",
       "   'abstract': \"Sensor fusion has found a lot of applications in today's industrial and scientific world with Kalman filtering being one of the most practiced methods. Despite their simplicity and effectiveness, Kalman filters are usually prone to uncertainties in system parameters and particularly system noise covariance. This paper proposes a Kalman filtering framework for sensor fusion, which provides robustness to the uncertainties in the system parameters such as noise covariance and state initialization. Two methods are developed based on the proposed approach. The effectiveness of the proposed methods is verified through numerous simulations and experiments.\",\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '9d36472d5ae93ce8bd5d5ba8ca602d65e9435f9c',\n",
       "   'title': 'MultiSpeech: Multi-Speaker Text to Speech with Transformer',\n",
       "   'abstract': 'Transformer-based text to speech (TTS) model (e.g., Transformer TTS~\\\\cite{li2019neural}, FastSpeech~\\\\cite{ren2019fastspeech}) has shown the advantages of training and inference efficiency over RNN-based model (e.g., Tacotron~\\\\cite{shen2018natural}) due to its parallel computation in training and/or inference. However, the parallel computation increases the difficulty while learning the alignment between text and speech in Transformer, which is further magnified in the multi-speaker scenario with noisy data and diverse speakers, and hinders the applicability of Transformer for multi-speaker TTS. In this paper, we develop a robust and high-quality multi-speaker Transformer TTS system called MultiSpeech, with several specially designed components/techniques to improve text-to-speech alignment: 1) a diagonal constraint on the weight matrix of encoder-decoder attention in both training and inference; 2) layer normalization on phoneme embedding in encoder to better preserve position information; 3) a bottleneck in decoder pre-net to prevent copy between consecutive speech frames. Experiments on VCTK and LibriTTS multi-speaker datasets demonstrate the effectiveness of MultiSpeech: 1) it synthesizes more robust and better quality multi-speaker voice than naive Transformer based TTS; 2) with a MutiSpeech model as the teacher, we obtain a strong multi-speaker FastSpeech model with almost zero quality degradation while enjoying extremely fast inference speed.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Engineering']},\n",
       "  {'paperId': '7f03aa91b5bfdfc2b5c1a177262ca5da21dfca04',\n",
       "   'title': 'A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis',\n",
       "   'abstract': 'The advancement of generative radiance fields has pushed the boundary of 3Daware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shapecolor ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code will be released at https://github.com/XingangPan/ShadeGAN.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '04379f6850e4b92de02c43dac3c4a95971ca7fe4',\n",
       "   'title': 'Reducing maternal mortality in sub–Saharan Africa: the role of ethical consumerism',\n",
       "   'abstract': 'Between 1990 and 2015, the maternal mortality ratio for sub–Saharan Africa as a whole fell by roughly 45 percent [1]. Unfortunately, this decline has not been uniformly distributed across the region. A number of countries have shown little or no progress and continue to experience mortality rates that rank among the highest in the world. In countries such as Angola, Liberia, Sierra Leone, Chad, Somalia and the Democratic Republic of Congo (DRC) a significant impediment to progress has been the decimation of health infrastructure by protracted regional and civil armed conflicts. During times of conflict, there is also increasing evidence of violence being directed specifically against pregnant women [2]. The 1990s saw the emergence of warfare in Africa as a means of accumulating wealth and power [3]. In–depth analyses by the United Nations and others have identified competition among various groups for the illegal expropriation and sale of the region’s vast natural resources, as both an underlying cause of conflict and also as a catalyst for ongoing conflict [4]. It has been estimated that Maternal Mortality rates are at least 30% higher in sub–Saharan African countries that have experienced recent conflict than in those which are conflict free [5]. Addressing the underlying issues relating to the protracted cycles of conflict is therefore an essential precursor to developing an environment in which interventions to address maternal mortality, or indeed any other health issue, can be implemented.',\n",
       "   'fieldsOfStudy': ['Geography', 'Medicine']},\n",
       "  {'paperId': '74ff6d48f9c62e937023106629d27ef2d2ddf8bc',\n",
       "   'title': 'Least Squares Generative Adversarial Networks',\n",
       "   'abstract': 'Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics']},\n",
       "  {'paperId': '90b47dfe4313548de19d3af3adb837ebcb07e443',\n",
       "   'title': 'Hybrid Social Force-Fuzzy Logic Evacuation Simulation Model for Multiple Exits',\n",
       "   'abstract': \"One of the most important aspect of evacuation management system, when it comes to organizing a safer large-scale gathering is crowd dynamics. Utilizing evacuation simulation of crowd dynamics during egress, for planning efficient crowd control can minimize crowd disaster to a great extent. Most of the previous studies on evacuation models have been done over a discrete space which have neglected the uncertainty aspect of an agent's decision making, especially when it comes to panic situations. This study proposes a model for evacuation simulation under uncertainty conditions in a continuous space via computer simulations. It will focus on developing an intelligent simulation model utilizing one of the artificial intelligence techniques which is fuzzy logic. Social Force Model will be taken as the base for basic agent motion. Membership functions such as distance from the exit, familiarity and visibility of the exit, density of crowd around the exit are incorporated in the fuzzy logic system to model the system. From our findings, it can be deduced that factors such as density, distance, and familiarity all considerably affect the time of evacuation of agents from the threat place. Indeed, uncertainty aspect influences agents' decision making, thus affecting the result of evacuation time.\",\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '2a88cc8cc9562b4addec03ba16b35cb4d3baaa43',\n",
       "   'title': 'EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks',\n",
       "   'abstract': 'Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the “best of both worlds”, i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of 3 different DNN tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 43.6%, 15.3% and 11.9% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics']},\n",
       "  {'paperId': '002f450563b22a0f7918e1a1ae4d5ccb31daf720',\n",
       "   'title': 'Selecting the Best Data Filtering Method for NMT Training',\n",
       "   'abstract': 'Performance of NMT systems has been proven to depend on the quality of the training data. In this paper we explore different open-source tools that can be used to score the quality of translation pairs, with the goal of obtaining clean corpora for training NMT models. We measure the performance of these tools by correlating their scores with human scores, as well as rank models trained on the resulting filtered datasets in terms of their performance on different test sets and MT performance metrics.',\n",
       "   'fieldsOfStudy': None},\n",
       "  {'paperId': 'b6065f5ce32e21a3328c28becc2f654d8f9bb0f9',\n",
       "   'title': 'Fast Rotation Search with Stereographic Projections for 3D Registration',\n",
       "   'abstract': 'Recently there has been a surge of interest to use branch-and-bound (bnb) optimisation for 3D point cloud registration. While bnb guarantees globally optimal solutions, it is usually too slow to be practical. A fundamental source of difficulty is the search for the rotation parameters in the 3D rigid transform. In this work, assuming that the translation parameters are known, we focus on constructing a fast rotation search algorithm. With respect to an inherently robust geometric matching criterion, we propose a novel bounding function for bnb that allows rapid evaluation. Underpinning our bounding function is the usage of stereographic projections to precompute and spatially index all possible point matches. This yields a robust and global algorithm that is significantly faster than previous methods. To conduct full 3D registration, the translation can be supplied by 3D feature matching, or by another optimisation framework that provides the translation. On various challenging point clouds, including those taken out of lab settings, our approach demonstrates superior efficiency.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics', 'Medicine']},\n",
       "  {'paperId': '1bb9b26b926ac3dc3f95d1e8fb590d2a75b26bfe',\n",
       "   'title': 'Point Cloud Segmentation via Constrained Nonlinear Least Squares Surface Normal Estimates',\n",
       "   'abstract': 'We present a point cloud segmentation scheme based on estimated surface normals and local point connectivity, that operates on unstructured point cloud data. We can segment a point cloud into disconnected components as well as piecewise smooth components as needed. Given that the performance of the segmentation routine depends on the quality of the surface normal approximation, we also propose an improved surface normal approximation method based on recasting the popular principal component analysis formulation as a constrained least squares problem. The new approach is robust to singularities in the data, such as corners and edges, and also incorporates data denoising in a manner similar to planar moving least squares.',\n",
       "   'fieldsOfStudy': None},\n",
       "  {'paperId': '5b6fc54fc67a0857638e1da5e7ba6bfb6990fbe1',\n",
       "   'title': '3-D object recognition based on SVM and stereo-vision: Application in endoscopic imaging',\n",
       "   'abstract': 'In this paper we focus on the recognition of threedimensional objects captured by an active stereo vision sensor. The study is related to our research project Cyclope, this embedded sensor based on active stereo-vision approach allows real time 3D objects reconstruction. Our medical application requires differentiation between hyperplastic and adenomatous polyps during 3D endoscopic imaging. The detection algorithm consists of SVM classifier trained on robust feature descriptors of a surfacic 3D point cloud extracted from the surface of studied object. We compared our feature extraction method with others. Experimental results were encouraging and show correct classification rate of approximately 97%. The work contains many techniques concerning image processing and system calibration and provides detailed statistics about the detection rate and the computing complexity.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'b54c477885d53a27039c81f028e710ca54c83f11',\n",
       "   'title': 'Semi-Supervised Kernel Mean Shift Clustering',\n",
       "   'abstract': 'Mean shift clustering is a powerful nonparametric technique that does not require prior knowledge of the number of clusters and does not constrain the shape of the clusters. However, being completely unsupervised, its performance suffers when the original distance metric fails to capture the underlying cluster structure. Despite recent advances in semi-supervised clustering methods, there has been little effort towards incorporating supervision into mean shift. We propose a semi-supervised framework for kernel mean shift clustering (SKMS) that uses only pairwise constraints to guide the clustering procedure. The points are first mapped to a high-dimensional kernel space where the constraints are imposed by a linear transformation of the mapped points. This is achieved by modifying the initial kernel matrix by minimizing a log det divergence-based objective function. We show the advantages of SKMS by evaluating its performance on various synthetic and real datasets while comparing with state-of-the-art semi-supervised clustering algorithms.',\n",
       "   'fieldsOfStudy': ['Mathematics', 'Computer Science', 'Medicine']},\n",
       "  {'paperId': '7cf8a7ad1fffb3b4ce3f5f0715f73e0f22e4758a',\n",
       "   'title': 'Educational Data Mining and Analysis of Students’ Academic Performance Using WEKA',\n",
       "   'abstract': 'In this competitive scenario of the educational system, the higher education institutes use data mining tools and techniques for academic improvement of the student performance and to prevent drop out. The authors collected data from three colleges of Assam, India. The data consists of socio-economic, demographic as well as academic information of three hundred students with twenty-four attributes. Four classification methods, the J48, PART, Random Forest and Bayes Network Classifiers were used. The data mining tool used was WEKA. The high influential attributes were selected using the tool. The internal assessment attribute in the continuous evaluation process makes the highest impact in the final semester results of the students in our dataset.\\xa0 The results showed that random forest outperforms the other classifiers based on accuracy and classifier errors. Apriori algorithm was also used to find the association rule mining among all the attributes and the best rules were also displayed.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '63c022ae3b385d1d49c119142bfabb5cdb5ec90b',\n",
       "   'title': 'Adversarially Robust Generalization Just Requires More Unlabeled Data',\n",
       "   'abstract': 'Neural network robustness has recently been highlighted by the existence of adversarial examples. Many previous works show that the learned networks do not perform well on perturbed test data, and significantly more labeled data is required to achieve adversarially robust generalization. In this paper, we theoretically and empirically show that with just more unlabeled data, we can learn a model with better adversarially robust generalization. The key insight of our results is based on a risk decomposition theorem, in which the expected robust risk is separated into two parts: the stability part which measures the prediction stability in the presence of perturbations, and the accuracy part which evaluates the standard classification accuracy. As the stability part does not depend on any label information, we can optimize this part using unlabeled data. We further prove that for a specific Gaussian mixture problem illustrated by [35], adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabeled data is provided. Inspired by the theoretical findings, we propose a new algorithm called PASS by leveraging unlabeled data during adversarial training. We show that in the transductive and semi-supervised settings, PASS achieves higher robust accuracy and defense success rate on the Cifar-10 task.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics']},\n",
       "  {'paperId': '1fa360ed74cb2d5f8295c35544a42d4c7a2bf9ed',\n",
       "   'title': 'Gaussian-Mixture-Model-Based Spatial Neighborhood Relationships for Pixel Labeling Problem',\n",
       "   'abstract': 'In this paper, we present a new algorithm for pixel labeling and image segmentation based on the standard Gaussian mixture model (GMM). Unlike the standard GMM where pixels themselves are considered independent of each other and the spatial relationship between neighboring pixels is not taken into account, the proposed method incorporates this spatial relationship into the standard GMM. Moreover, the proposed model requires fewer parameters compared with the models based on Markov random fields. In order to estimate model parameters from observations, instead of utilizing an expectation-maximization algorithm, we employ gradient method to minimize a higher bound on the data negative log-likelihood. The performance of the proposed model is compared with methods based on both standard GMM and Markov random fields, demonstrating the robustness, accuracy, and effectiveness of our method.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Medicine', 'Mathematics']},\n",
       "  {'paperId': '887dca3660b7c6543faac513d804e621f0134470',\n",
       "   'title': 'Text-to-Image Generation Grounded by Fine-Grained User Attention',\n",
       "   'abstract': 'Localized Narratives [28] is a dataset with detailed natural language descriptions of images paired with mouse traces that provide a sparse, fine-grained visual grounding for phrases. We propose TRECS, a sequential model that exploits this grounding to generate images. TRECS uses descriptions to retrieve segmentation masks and predict object labels aligned with mouse traces. These alignments are used to select and position masks to generate a fully covered segmentation canvas; the final image is produced by a segmentation-to-image generator using this canvas. This multi-step, retrieval-based approach outperforms existing direct text-to-image generation models on both automatic metrics and human evaluations: overall, its generated images are more photo-realistic and better match descriptions.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'a89520b7448f56ce7d2ec76990108ea505cd0d55',\n",
       "   'title': 'Deep Learning for Underwater Visual Odometry Estimation',\n",
       "   'abstract': 'This paper addresses Visual Odometry (VO) estimation in challenging underwater scenarios. Robot visual-based navigation faces several additional difficulties in the underwater context, which severely hinder both its robustness and the possibility for persistent autonomy in underwater mobile robots using visual perception capabilities. In this work, some of the most renown VO and Visual Simultaneous Localization and Mapping (v-SLAM) frameworks are tested on underwater complex environments, assessing the extent to which they are able to perform accurately and reliably on robotic operational mission scenarios. The fundamental issue of precision, reliability and robustness to multiple different operational scenarios, coupled with the rise in predominance of Deep Learning architectures in several Computer Vision application domains, has prompted a great a volume of recent research concerning Deep Learning architectures tailored for visual odometry estimation. In this work, the performance and accuracy of Deep Learning methods on the underwater context is also benchmarked and compared to classical methods. Additionally, an extension of current work is proposed, in the form of a visual-inertial sensor fusion network aimed at correcting visual odometry estimate drift. Anchored on a inertial supervision learning scheme, our network managed to improve upon trajectory estimates, producing both metrically better estimates as well as more visually consistent trajectory shape mimicking.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'c691018d2706e16649a6dd55f555135bbc52679d',\n",
       "   'title': 'Particle Smoothing for Hidden Diffusion Processes: Adaptive Path Integral Smoother',\n",
       "   'abstract': 'Smoothing methods are used for inference of stochastic processes given noisy observations. The estimation of the marginal posterior distribution given all observations is typically a computationally intensive task. We propose a novel algorithm based on path integral control theory to efficiently estimate the smoothing distribution of continuous-time diffusion processes from partial observations. In particular, we use an adaptive importance sampling method to improve the effective sampling size of the posterior and the reliability of the estimation of the marginals. This is achieved by estimating a feedback controller, together with an adaptive initialization and an annealing scheme to sample efficiently from the joint smoothing distribution. We compare the results with estimations obtained from the standard Forward Filter/Backward Simulator (FFBSi) for two diffusion processes of different complexity. We show that the proposed method gives more accurate estimates than the standard FFBSi.',\n",
       "   'fieldsOfStudy': ['Mathematics', 'Computer Science']},\n",
       "  {'paperId': '9afe5db4edb7846cd70aefc9d15549d50169249e',\n",
       "   'title': 'Variable Selection and Updating In Model-Based Discriminant Analysis for High Dimensional Data with Food Authenticity Applications.',\n",
       "   'abstract': 'Food authenticity studies are concerned with determining if food samples have been correctly labelled or not. Discriminant analysis methods are an integral part of the methodology for food authentication. Motivated by food authenticity applications, a model-based discriminant analysis method that includes variable selection is presented. The discriminant analysis model is fitted in a semi-supervised manner using both labeled and unlabeled data. The method is shown to give excellent classification performance on several high-dimensional multiclass food authenticity datasets with more variables than observations. The variables selected by the proposed method provide information about which variables are meaningful for classification purposes. A headlong search strategy for variable selection is shown to be efficient in terms of computation and achieves excellent classification performance. In applications to several food authenticity datasets, our proposed method outperformed default implementations of Random Forests, AdaBoost, transductive SVMs and Bayesian Multinomial Regression by substantial margins.',\n",
       "   'fieldsOfStudy': ['Medicine', 'Mathematics', 'Computer Science']},\n",
       "  {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "   'title': 'Maximum a Posteriori Policy Optimisation',\n",
       "   'abstract': 'We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics']},\n",
       "  {'paperId': '7cb3c22f12a6786c6dc9652d8db8cf79f7107cbd',\n",
       "   'title': 'An Incentive Mechanism for Federated Learning in Wireless Cellular Networks: An Auction Approach',\n",
       "   'abstract': 'Federated Learning (FL) is a distributed learning framework that can deal with the distributed issue in machine learning and still guarantee high learning performance. However, it is impractical that all users will sacrifice their resources to join the FL algorithm. This motivates us to study the incentive mechanism design for FL. In this paper, we consider a FL system that involves one base station (BS) and multiple mobile users. The mobile users use their own data to train the local machine learning model, and then send the trained models to the BS, which generates the initial model, collects local models and constructs the global model. Then, we formulate the incentive mechanism between the BS and mobile users as an auction game where the BS is an auctioneer and the mobile users are the sellers. In the proposed game, each mobile user submits its bids according to the minimal energy cost that the mobile users experiences in participating in FL. To decide winners in the auction and maximize social welfare, we propose the primal-dual greedy auction mechanism. The proposed mechanism can guarantee three economic properties, namely, truthfulness, individual rationality and efficiency. Finally, numerical results are shown to demonstrate the performance effectiveness of our proposed mechanism.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'f93490e4009fe1f3595506f804d4447e7ca9e415',\n",
       "   'title': 'Novel Convergence Results of Adaptive Stochastic Gradient Descents',\n",
       "   'abstract': 'Adaptive stochastic gradient descent, which uses unbiased samples of the gradient with stepsizes chosen from the historical information, has been widely used to train neural networks for computer vision and pattern recognition tasks. This paper revisits the theoretical aspects of two classes of adaptive stochastic gradient descent methods, which contain several existing state-of-the-art schemes. We focus on the presentation of novel findings: In the general smooth case, the nonergodic convergence results are given, that is, the expectation of the gradients’ norm rather than the minimum of past iterates is proved to converge; We also studied their performances under Polyak-Łojasiewicz property on the objective function. In this case, the nonergodic convergence rates are given for the expectation of the function values. Our findings show that more substantial restrictions on the steps are needed to guarantee the nonergodic function values’ convergence (rates).',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Medicine']},\n",
       "  {'paperId': '1692506a12ade107daf66bea32db9261d1392ed9',\n",
       "   'title': 'Attention-based Graph Neural Network for Semi-supervised Learning',\n",
       "   'abstract': 'Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics']},\n",
       "  {'paperId': '1b573ad8a2e0ec2971b77749b67c2759273438aa',\n",
       "   'title': 'High-dimensional Ising model selection using ℓ1-regularized logistic regression',\n",
       "   'abstract': 'We consider the problem of estimating the graph associated with a binary Ising Markov random field. We describe a method based on $\\\\ell_1$-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an $\\\\ell_1$-constraint. The method is analyzed under high-dimensional scaling in which both the number of nodes $p$ and maximum neighborhood size $d$ are allowed to grow as a function of the number of observations $n$. Our main results provide sufficient conditions on the triple $(n,p,d)$ and the model parameters for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. With coherence conditions imposed on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes $n=\\\\Omega(d^3\\\\log p)$ with exponentially decaying error. When these same conditions are imposed directly on the sample matrices, we show that a reduced sample size of $n=\\\\Omega(d^2\\\\log p)$ suffices for the method to estimate neighborhoods consistently. Although this paper focuses on the binary graphical models, we indicate how a generalization of the method of the paper would apply to general discrete Markov random fields.',\n",
       "   'fieldsOfStudy': ['Mathematics']},\n",
       "  {'paperId': 'cc1b8c7d0c5b3203023999b8b5cfa2fdc26fefbc',\n",
       "   'title': 'Sparse superresolution phase retrieval from phase-coded noisy intensity patterns',\n",
       "   'abstract': 'Abstract. We consider a computational superresolution inverse diffraction problem for phase retrieval from phase-coded intensity observations. The optical setup includes a thin lens and a spatial light modulator for phase coding. The designed algorithm is targeted on an optimal solution for Poissonian noisy observations. One of the essential instruments of this design is a complex-domain sparsity applied for complex-valued object (phase and amplitude) to be reconstructed. Simulation experiments demonstrate that good quality imaging can be achieved for high-level of the superresolution with a factor of 32, which means that the pixel of the reconstructed object is 32 times smaller than the sensor’s pixel. This superresolution corresponds to the object pixel as small as a quarter of the wavelength.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Engineering']},\n",
       "  {'paperId': 'd01f3aebb1aee8f1564558fbab6bfdba703c1e42',\n",
       "   'title': 'A Robust Variable Selection Method for Sparse Online Regression via the Elastic Net Penalty',\n",
       "   'abstract': 'Variable selection has been a hot topic, with various popular methods including lasso, SCAD, and elastic net. These penalized regression algorithms remain sensitive to noisy data. Furthermore, “concept drift” fundamentally distinguishes streaming data learning from batch learning. This article presents a method for noise-resistant regularization and variable selection in noisy data streams with multicollinearity, dubbed canal-adaptive elastic net, which is similar to elastic net and encourages grouping effects. In comparison to lasso, the canal adaptive elastic net is especially advantageous when the number of predictions (p) is significantly larger than the number of observations (n), and the data are multi-collinear. Numerous simulation experiments have confirmed that canal-adaptive elastic net has higher prediction accuracy than lasso, ridge regression, and elastic net in data with multicollinearity and noise.',\n",
       "   'fieldsOfStudy': None},\n",
       "  {'paperId': '32a40b045e665db39e120c12338f9f1238b0690b',\n",
       "   'title': 'Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition',\n",
       "   'abstract': 'Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for fine-grained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional filters that capture class-specific discriminative patches without extra part or bounding box annotations. Such a filter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are provided to understand our approach.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'f902a64f7d08aaa6bfca7463e8729952ddc6134e',\n",
       "   'title': 'LVIS: A Dataset for Large Vocabulary Instance Segmentation',\n",
       "   'abstract': 'Progress on object detection is enabled by datasets that focus the research community’s attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced ‘el-vis’): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect 2.2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at http://www.lvisdataset.org.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '4c01e39c0b47e313cace1187e80b2db5fffe2747',\n",
       "   'title': 'Generalized Learning Vector Quantization for Classification in Randomized Neural Networks and Hyperdimensional Computing',\n",
       "   'abstract': \"Machine learning algorithms deployed on edge devices must meet certain resource constraints and efficiency requirements. Random Vector Functional Link (RVFL) networks are favored for such applications due to their simple design and training efficiency. We propose a modified RVFL network that avoids computationally expensive matrix operations during training, thus expanding the network's range of potential applications. Our modification replaces the least-squares classifier with the Generalized Learning Vector Quantization (GLVQ) classifier, which only employs simple vector and distance calculations. The GLVQ classifier can also be considered an improvement upon certain classification algorithms popularly used in the area of Hyperdimensional Computing. The proposed approach achieved state-of-the-art accuracy on a collection of datasets from the UCI Machine Learning Repository-higher than previously proposed RVFL networks. We further demonstrate that our approach still achieves high accuracy while severely limited in training iterations (using on average only 21% of the least-squares classifier computational costs).\",\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'e2a9b42832cf318a7857517ad88ffdcf4c45fff9',\n",
       "   'title': 'A Simulation Based Dynamic Evaluation Framework for System-wide Algorithmic Fairness',\n",
       "   'abstract': 'We propose the use of Agent Based Models (ABMs) inside a reinforcement learning framework in order to better understand the relationship between automated decision making tools, fairness-inspired statistical constraints, and the social phenomena giving rise to discrimination towards sensitive groups. There have been many instances of discrimination occurring due to the applications of algorithmic tools by public and private institutions. Until recently, these practices have mostly gone unchecked. Given the large-scale transformation these new technologies elicit, a joint effort of social sciences and machine learning researchers is necessary. Much of the research has been done on determining statistical properties of such algorithms and the data they are trained on. We aim to complement that approach by studying the social dynamics in which these algorithms are implemented. We show how bias can be accumulated and reinforced through automated decision making, and the possibility of finding a fairness inducing policy. We focus on the case of recidivism risk assessment by considering simplified models of arrest. We find that if we limit our attention to what is observed and manipulated by these algorithmic tools, we may determine some blatantly unfair practices as fair, illustrating the advantage of analyzing the otherwise elusive property with a system-wide model. We expect the introduction of agent based simulation techniques will strengthen collaboration with social scientists, arriving at a better understanding of the social systems affected by technology and to hopefully lead to concrete policy proposals that can be presented to policymakers for a true systemic transformation.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'ffa46c6c0956204fbb59c576155ba7199cc49c96',\n",
       "   'title': 'Hyperspectral Anomaly Detection by Graph Pixel Selection',\n",
       "   'abstract': 'Hyperspectral anomaly detection (AD) is an important problem in remote sensing field. It can make full use of the spectral differences to discover certain potential interesting regions without any target priors. Traditional Mahalanobis-distance-based anomaly detectors assume the background spectrum distribution conforms to a Gaussian distribution. However, this and other similar distributions may not be satisfied for the real hyperspectral images. Moreover, the background statistics are susceptible to contamination of anomaly targets which will lead to a high false-positive rate. To address these intrinsic problems, this paper proposes a novel AD method based on the graph theory. We first construct a vertex- and edge-weighted graph and then utilize a pixel selection process to locate the anomaly targets. Two contributions are claimed in this paper: 1) no background distributions are required which makes the method more adaptive and 2) both the vertex and edge weights are considered which enables a more accurate detection performance and better robustness to noise. Intensive experiments on the simulated and real hyperspectral images demonstrate that the proposed method outperforms other benchmark competitors. In addition, the robustness of the proposed method has been validated by using various window sizes. This experimental result also demonstrates the valuable characteristic of less computational complexity and less parameter tuning for real applications.',\n",
       "   'fieldsOfStudy': ['Mathematics', 'Computer Science', 'Medicine']},\n",
       "  {'paperId': '3a5d433b56500ef46c2ca1e38079dc23f5c8c50e',\n",
       "   'title': 'Audio Source Separation Using Variational Autoencoders and Weak Class Supervision',\n",
       "   'abstract': 'In this letter, we propose a source separation method that is trained by observing the mixtures and the class labels of the sources present in the mixture without any access to isolated sources. Since our method does not require source class labels for every time-frequency bin but only a single label for each source constituting the mixture signal, we call this scenario as weak class supervision. We associate a variational autoencoder (VAE) with each source class within a nonnegative (compositional) model. Each VAE provides a prior model to identify the signal from its associated class in a sound mixture. After training the model on mixtures, we obtain a generative model for each source class and demonstrate our method on one-second mixtures of utterances of digits from 0\\xa0to\\xa09. We show that the separation performance obtained by source class supervision is as good as the performance obtained by source signal supervision.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '63edef27a14121b6fab44b2f7ebffdc4cb6cf649',\n",
       "   'title': 'Self-Consistency of the Fokker-Planck Equation',\n",
       "   'abstract': 'The Fokker-Planck equation (FPE) is the partial differential equation that governs the density evolution of the It ˆ o process and is of great importance to the literature of statistical physics and machine learning. The FPE can be regarded as a continuity equation where the change of the density is com-pletely determined by a time varying velocity ﬁeld. Importantly, this velocity ﬁeld also depends on the current density function. As a result, the ground-truth velocity ﬁeld can be shown to be the solution of a ﬁxed-point equation, a property that we call self-consistency . In this paper, we exploit this concept to design a potential function of the hypothesis velocity ﬁelds, and prove that, if such a function diminishes to zero during the training procedure, the trajectory of the densities generated by the hypothesis velocity ﬁelds converges to the solution of the FPE in the Wasserstein-2 sense. The proposed potential function is amenable to neural-network based parameterization as the stochastic gradient with respect to the parameter can be efﬁciently computed. Once a parameterized model, such as Neural Ordinary Differential Equation is trained, we can generate the entire trajectory to the FPE.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '9759290674ac0d1703ae39cf3db72598cc5b9296',\n",
       "   'title': 'Towards Miniaturization of a MEMS-Based Wearable Motion Capture System',\n",
       "   'abstract': 'This paper presents a modular architecture to develop a wearable system for real-time human motion capture. The system is based on a network of smart inertial measurement units (IMUs) distributed on the human body. Each of these modules is provided with a 32-bit RISC microcontroller (MCU) and miniaturized MEMS sensors: three-axis accelerometer, three-axis gyroscopes, and three-axis magnetometer. The MCU collects measurements from the sensors and implement the sensor fusion algorithm, a quaternion-based extended Kalman filter to estimate the attitude and the gyroscope biases. The design of the proposed IMU, in order to overcome the problems of the commercial solution, aims to improve performance and to reduce size and weight. In this way, it can be easily embedded in a tracksuit for total body motion reconstruction with considerable enhancement of the wearability and comfort. Furthermore, the main achievements will be presented with a performance comparison between the proposed IMU and some commercial platforms.',\n",
       "   'fieldsOfStudy': ['Engineering', 'Computer Science']},\n",
       "  {'paperId': 'a0b6e0121ff07c9df070f2114cdbdb0d0b146052',\n",
       "   'title': 'On Sparse Variational Methods and the Kullback-Leibler Divergence between Stochastic Processes',\n",
       "   'abstract': 'The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes.',\n",
       "   'fieldsOfStudy': ['Mathematics', 'Computer Science']},\n",
       "  {'paperId': 'fbda91cfacd2b792794fb726e9417aef58480c72',\n",
       "   'title': 'Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study',\n",
       "   'abstract': 'Background There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task. Methods and findings A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of 43.5%, 44.8%, and 57.3%, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong’s test. The prevalence of pneumonia was high enough at MSH (34.2%) relative to NIH and IU (1.2% and 1.0%) that merely sorting by hospital system achieved an AUC of 0.861 (95% CI 0.855–0.866) on the joint MSH–NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU (P values 0.580 and 0.273, respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; P values both <0.001). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95% CI 0.927–0.936), but this model demonstrated significantly lower external performance at IU (AUC 0.815, 95% CI 0.745–0.885, P = 0.001). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH–NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data (P = 0.88). When a 10-fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model (10× MSH risk P < 0.001; 10× NIH P = 0.002), but this outperformance failed to generalize to IU (MSH 10× P < 0.001; NIH 10× P = 0.027). CNNs were able to directly detect hospital system of a radiograph for 99.95% NIH (22,050/22,062) and 99.98% MSH (8,386/8,388) radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system–specific biases. Conclusion Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.',\n",
       "   'fieldsOfStudy': ['Medicine']},\n",
       "  {'paperId': '1dc2f442a9172c5c734cfd8f3d4682a9acb1c7e0',\n",
       "   'title': 'Echolocation versus echo suppression in humans',\n",
       "   'abstract': 'Several studies have shown that blind humans can gather spatial information through echolocation. However, when localizing sound sources, the precedence effect suppresses spatial information of echoes, and thereby conflicts with effective echolocation. This study investigates the interaction of echolocation and echo suppression in terms of discrimination suppression in virtual acoustic space. In the ‘Listening’ experiment, sighted subjects discriminated between positions of a single sound source, the leading or the lagging of two sources, respectively. In the ‘Echolocation’ experiment, the sources were replaced by reflectors. Here, the same subjects evaluated echoes generated in real time from self-produced vocalizations and thereby discriminated between positions of a single reflector, the leading or the lagging of two reflectors, respectively. Two key results were observed. First, sighted subjects can learn to discriminate positions of reflective surfaces echo-acoustically with accuracy comparable to sound source discrimination. Second, in the Listening experiment, the presence of the leading source affected discrimination of lagging sources much more than vice versa. In the Echolocation experiment, however, the presence of both the lead and the lag strongly affected discrimination. These data show that the classically described asymmetry in the perception of leading and lagging sounds is strongly diminished in an echolocation task. Additional control experiments showed that the effect is owing to both the direct sound of the vocalization that precedes the echoes and owing to the fact that the subjects actively vocalize in the echolocation task.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Medicine']},\n",
       "  {'paperId': 'bbef6e157a5f082a6f2f1d230ee4fc428ff0f2c7',\n",
       "   'title': 'Tuning PAK Activity to Rescue Abnormal Myelin Permeability in HNPP',\n",
       "   'abstract': 'Schwann cells in the peripheral nervous systems extend their membranes to wrap axons concentrically and form the insulating sheath, called myelin. The spaces between layers of myelin are sealed by myelin junctions. This tight insulation enables rapid conduction of electric impulses (action potentials) through axons. Demyelination (stripping off the insulating sheath) has been widely regarded as one of the most important mechanisms altering the action potential propagation in many neurological diseases. However, the effective nerve conduction is also thought to require a proper myelin seal through myelin junctions such as tight junctions and adherens junctions. In the present study, we have demonstrated the disruption of myelin junctions in a mouse model (Pmp22+/-) of hereditary neuropathy with liability to pressure palsies (HNPP) with heterozygous deletion of Pmp22 gene. We observed a robust increase of F-actin in Pmp22+/- nerve regions where myelin junctions were disrupted, leading to increased myelin permeability. These abnormalities were present long before segmental demyelination at the late phase of Pmp22+/- mice. Moreover, the increase of F-actin levels correlated with an enhanced activity of p21-activated kinase (PAK1), a molecule known to regulate actin polymerization. Pharmacological inhibition of PAK normalized levels of F-actin, and completely prevented the progression of the myelin junction disruption and nerve conduction failure in Pmp22+/- mice. Our findings explain how abnormal myelin permeability is caused in HNPP, leading to impaired action potential propagation in the absence of demyelination. We call it “functional demyelination”, a novel mechanism upstream to the actual stripping of myelin that is relevant to many demyelinating diseases. This observation also provides a potential therapeutic approach for HNPP.',\n",
       "   'fieldsOfStudy': ['Biology', 'Medicine']},\n",
       "  {'paperId': '2eaba874fab3720a118eb6b5d21662f6d8917b08',\n",
       "   'title': 'Assessing the Effect of Pedestrians’ Use of Cell Phones on Their Walking Behavior: A Study Based on Automated Video Analysis',\n",
       "   'abstract': 'The objective of the study is to assess the effect of the use of cell phones while walking at urban crosswalks. The methodology uses recent findings in health science concerning the relationship between tempo-spatial characteristics of gait and the cognitive abilities of pedestrians. Gait measures are shown to be affected by the complexity of the task (e.g., talking and texting) performed during walking. This study focuses on the effect of distraction states, distraction types (visual such as texting/reading and auditory such as talking/listening), and pedestrian-vehicle interactions on the gait parameters of pedestrians at crosswalks. Experiments are performed on a video data set near a college campus in the city of Kamloops, British Columbia. The analysis relies on automated video-based data collection using a computer vision technique. The benefits of such an automated system include the ability to capture the natural movement of pedestrians and minimizing the risk of disturbing their behavior. Results show that pedestrians distracted by texting/reading (visually) or talking/listening (auditory) while walking tend to reduce and control their walking speed by adjusting their step length or step frequency, respectively. Pedestrians distracted by texting/reading (visually) have significantly lower step length and are less stable in walking. Distracted pedestrians involved in interactions with approaching vehicles tend to reduce and control their walking speeds by adjusting their step frequencies. This research can find applications in pedestrian facility design, modeling and calibrating pedestrian simulations, and pedestrian safety intervention programs and legislative actions.',\n",
       "   'fieldsOfStudy': ['Psychology']},\n",
       "  {'paperId': '8b0ee3c9ff9a810bc4fff11ae7b38ad9da3bb0f1',\n",
       "   'title': 'Do You Hear What I Hear: The Balancing Act of Designing an Electronic Hockey Puck for Playing Hockey Non-Visually',\n",
       "   'abstract': 'Blind hockey is a sport that is gaining popularity in the United States after having an international presence for years. In blind hockey, a modified puck is used that emits sounds via ball bearings that rattle inside the puck when it is moving. The modified puck’s lifetime is minimal due to its lack of durability, and it does not provide feedback when the puck stops moving. This article presents an evaluation of multiple prototypes that investigate the appropriate acoustic profiles for an electronic version of a puck that has the ability to overcome some of these challenges. Our approach leverages the use of alternative 3D printable materials and the implementation of four distinct sound profiles: the league-standard puck in blind hockey, a 3.5kHz piezo buzzer, an 800Hz sine tone, and simulated white noise. We present the design and prototype of the pucks, along with benchtop and user validation tests of the prototypes, comparing them to the league standard puck with a focus on acoustic performance. Participants rated the white noise sound profile highest in pleasantness and loudness and the LSP highest in localization. The white noise sound profile was associated with lower angle and distance errors. Of the prototypes produced, the white noise prototype puck appeared to demonstrate the most promise for playing hockey non-visually. We close with a discussion of recommendations for future electronic hockey puck designs to support blind hockey moving forward.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'cda320a7a0c41d3618207b3133ef62ca9af38c26',\n",
       "   'title': 'Multimodal Analysis of SCN1A Missense Variants Improves Interpretation of Clinically Relevant Variants in Dravet Syndrome',\n",
       "   'abstract': 'Objective: We aimed to improve the classification of SCN1A missense variants in patients with Dravet syndrome (DS) by combining and modifying the current variants classification criteria to minimize inconclusive test results. Methods: We established a score classification workflow based on evidence of pathogenicity to adapt the classification of DS-related SCN1A missense variants. In addition, we compiled the variants reported in the literature and our cohort and assessed the proposed pathogenic classification criteria. We combined information regarding previously established pathogenic amino acid changes, mode of inheritance, population-specific allele frequencies, localization within protein domains, and deleterious effect prediction analysis. Results: Our meta-analysis showed that 46% (506/1,101) of DS-associated SCN1A variants are missense. We applied the score classification workflow and 56.5% (286/506) of the variants had their classification changed from VUS: 17.8% (90/506) into “pathogenic” and 38.7% (196/506) as “likely pathogenic.” Conclusion: Our results indicate that using multimodal analysis seems to be the best approach to interpret the pathogenic impact of SCN1A missense changes for the molecular diagnosis of patients with DS. By applying the proposed workflow, most DS related SCN1A variants had their classification improved.',\n",
       "   'fieldsOfStudy': ['Biology', 'Medicine']},\n",
       "  {'paperId': '672ee40b2d802d0f474b80a6069cffe7303d6470',\n",
       "   'title': 'COVID-19-CT-CXR: A Freely Accessible and Weakly Labeled Chest X-Ray and CT Image Collection on COVID-19 From Biomedical Literature',\n",
       "   'abstract': 'The latest threat to global health is the COVID-19 outbreak. Although there exist large datasets of chest X-rays (CXR) and computed tomography (CT) scans, few COVID-19 image collections are currently available due to patient privacy. At the same time, there is a rapid growth of COVID-19-relevant articles in the biomedical literature, including those that report findings on radiographs. Here, we present COVID-19-CT-CXR, a public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. We extracted figures, associated captions, and relevant figure descriptions in the article and separated compound figures into subfigures. Because a large portion of figures in COVID-19 articles are not CXR or CT, we designed a deep-learning model to distinguish them from other figure types and to classify them accordingly. The final database includes 1,327 CT and 263 CXR images (as of May 9, 2020) with their relevant text. To demonstrate the utility of COVID-19-CT-CXR, we conducted four case studies. (1) We show that COVID-19-CT-CXR, when used as additional training data, is able to contribute to improved deep-learning (DL) performance for the classification of COVID-19 and non-COVID-19 CT. (2) We collected CT images of influenza, another common infectious respiratory illness that may present similarly to COVID-19, and fine-tuned a baseline deep neural network to distinguish a diagnosis of COVID-19, influenza, or normal or other types of diseases on CT. (3) We fine-tuned an unsupervised one-class classifier from non-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4) From text-mined captions and figure descriptions, we compared 15 clinical symptoms and 20 clinical findings of COVID-19 versus those of influenza to demonstrate the disease differences in the scientific publications. Our database is unique, as the figures are retrieved along with relevant text with fine-grained descriptions, and it can be extended easily in the future. We believe that our work is complementary to existing resources and hope that it will contribute to medical image analysis of the COVID-19 pandemic. The dataset, code, and DL models are publicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': 'ecef28ddfcacf4f2ba9c22a3c8296d4e19322d3d',\n",
       "   'title': 'Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks',\n",
       "   'abstract': 'It is a long standing question how biological systems transform visual inputs to robustly infer high-level visual information. Research in the last decades has established that much of the underlying computations take place in a hierarchical fashion along the ventral visual pathway. However, the exact processing stages along this hierarchy are difficult to characterise. Here we present a method to generate stimuli that will allow a principled description of the processing stages along the ventral stream. We introduce a new parametric texture model based on the powerful feature spaces of convolutional neural networks optimised for object recognition. We show that constraining a spatial summary statistic over feature maps suffices to synthesise high-quality natural textures. Moreover we establish that our texture representations continuously disentangle high-level visual information and demonstrate that the hierarchical parameterisation of the texture model naturally enables us to generate novel types of stimuli for systematically probing mid-level vision.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Biology']},\n",
       "  {'paperId': '1113e5267315865d9f8661b097f3ae0d86031592',\n",
       "   'title': 'Review of FPGA-Based Accelerators of Deep Convolutional Neural Networks',\n",
       "   'abstract': 'Recent research has shown that Deep Convolutional Neural Networks (CNNs) are extremely accurate in a big range of cognitive tasks. This has piqued the interest of researchers. FPGAs are regarded as an attractive platform for hardware acceleration of CNNs due to their performance, flexibility, and energy efficiency. To improve the performance of CNNs, numerous accelerators have been employed, including ASICs, FPGAs, and graphics processors (GPUs). The use of FPGAs to accelerate deep learning networks is primarily driven by their high power efficiency and their ability to maximize parallelism. To provide comparisons and differences regarding the various techniques used to implement and optimize CNN algorithms on FPGA, the works were categorized into categories. Recent advances in deep learning networks using FPGA-based accelerators are described in this paper. This review should guide future advances in hardware acceleration and make deep learning research more efficient.',\n",
       "   'fieldsOfStudy': None}],\n",
       " 'recommendedPapers': [{'paperId': '94eebbefe8a37cf394be899b85af295c2e3a1f01',\n",
       "   'title': 'Efficient Parametric Approximations of Neural Network Function Space Distance',\n",
       "   'abstract': 'It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics']},\n",
       "  {'paperId': 'ee46421f518c17a51145d33a0fe7ccc6da31c51e',\n",
       "   'title': 'A Gradient Boosting Approach for Training Convolutional and Deep Neural Networks',\n",
       "   'abstract': 'Deep learning has revolutionized the computer vision and image classification domains. In this context Convolutional Neural Networks (CNNs) based architectures are the most widely applied models. In this article, we introduced two procedures for training Convolutional Neural Networks (CNNs) and Deep Neural Network based on Gradient Boosting (GB), namely GB-CNN and GB-DNN. These models are trained to fit the gradient of the loss function or pseudo-residuals of previous models. At each iteration, the proposed method adds one dense layer to an exact copy of the previous deep NN model. The weights of the dense layers trained on previous iterations are frozen to prevent over-fitting, permitting the model to fit the new dense as well as to fine-tune the convolutional layers (for GB-CNN) while still utilizing the information already learned. Through extensive experimentation on different 2D-image classification and tabular datasets, the presented models show superior performance in terms of classification accuracy with respect to standard CNN and Deep-NN with the same architectures.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '16176c4916b1a21323870f551d7b1f8d960659c6',\n",
       "   'title': 'Guided Deep Kernel Learning',\n",
       "   'abstract': 'Combining Gaussian processes with the expressive power of deep neural networks is commonly done nowadays through deep kernel learning (DKL). Unfortunately, due to the kernel optimization process, this often results in losing their Bayesian benefits. In this study, we present a novel approach for learning deep kernels by utilizing infinite-width neural networks. We propose to use the Neural Network Gaussian Process (NNGP) model as a guide to the DKL model in the optimization process. Our approach harnesses the reliable uncertainty estimation of the NNGPs to adapt the DKL target confidence when it encounters novel data points. As a result, we get the best of both worlds, we leverage the Bayesian behavior of the NNGP, namely its robustness to overfitting, and accurate uncertainty estimation, while maintaining the generalization abilities, scalability, and flexibility of deep kernels. Empirically, we show on multiple benchmark datasets of varying sizes and dimensionality, that our method is robust to overfitting, has good predictive performance, and provides reliable uncertainty estimations.',\n",
       "   'fieldsOfStudy': ['Computer Science', 'Mathematics']},\n",
       "  {'paperId': '355a88ee6967af23512f85f947270cf9d81ea098',\n",
       "   'title': 'IMPROVED GENERALIZATION IN SUPERVISED MODELS',\n",
       "   'abstract': 'We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model’s generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks. Code and pretrained models: https://europe.naverlabs.com/t-rex',\n",
       "   'fieldsOfStudy': None},\n",
       "  {'paperId': '7ba5da4f9b98c6a1976a05140d9d29a7ae8913c2',\n",
       "   'title': 'A Systematic Performance Analysis of Deep Perceptual Loss Networks Breaks Transfer Learning Conventions',\n",
       "   'abstract': 'Deep perceptual loss is a type of loss function in computer vision that aims to mimic human perception by using the deep features extracted from neural networks. In recent years the method has been applied to great effect on a host of interesting computer vision tasks, especially for tasks with image or image-like outputs. Many applications of the method use pretrained networks, often convolutional networks, for loss calculation. Despite the increased interest and broader use, more effort is needed toward exploring which networks to use for calculating deep perceptual loss and from which layers to extract the features. This work aims to rectify this by systematically evaluating a host of commonly used and readily available, pretrained networks for a number of different feature extraction points on four existing use cases of deep perceptual loss. The four use cases are implementations of previous works where the selected networks and extraction points are evaluated instead of the networks and extraction points used in the original work. The experimental tasks are dimensionality reduction, image segmentation, super-resolution, and perceptual similarity. The performance on these four tasks, attributes of the networks, and extraction points are then used as a basis for an in-depth analysis. This analysis uncovers essential information regarding which architectures provide superior performance for deep perceptual loss and how to choose an appropriate extraction point for a particular task and dataset. Furthermore, the work discusses the implications of the results for deep perceptual loss and the broader field of transfer learning. The results break commonly held assumptions in transfer learning, which imply that deep perceptual loss deviates from most transfer learning settings or that these assumptions need a thorough re-evaluation.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '62f3ecee1135503bb2cab776e915281521ef2f3a',\n",
       "   'title': 'Probabilistic Attention based on Gaussian Processes for Deep Multiple Instance Learning',\n",
       "   'abstract': \"Multiple Instance Learning (MIL) is a weakly supervised learning paradigm that is becoming increasingly popular because it requires less labeling effort than fully supervised methods. This is especially interesting for areas where the creation of large annotated datasets remains challenging, as in medicine. Although recent deep learning MIL approaches have obtained state-of-the-art results, they are fully deterministic and do not provide uncertainty estimations for the predictions. In this work, we introduce the Attention Gaussian Process (AGP) model, a novel probabilistic attention mechanism based on Gaussian Processes for deep MIL. AGP provides accurate bag-level predictions as well as instance-level explainability, and can be trained end-to-end. Moreover, its probabilistic nature guarantees robustness to overfitting on small datasets and uncertainty estimations for the predictions. The latter is especially important in medical applications, where decisions have a direct impact on the patient's health. The proposed model is validated experimentally as follows. First, its behavior is illustrated in two synthetic MIL experiments based on the well-known MNIST and CIFAR-10 datasets, respectively. Then, it is evaluated in three different real-world cancer detection experiments. AGP outperforms state-of-the-art MIL approaches, including deterministic deep learning ones. It shows a strong performance even on a small dataset with less than 100 labels and generalizes better than competing methods on an external test set. Moreover, we experimentally show that predictive uncertainty correlates with the risk of wrong predictions, and therefore it is a good indicator of reliability in practice. Our code is publicly available.\",\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '71d2356f3f3771eb94ed801db809d0b208ffe809',\n",
       "   'title': 'Using Intermediate Forward Iterates for Intermediate Generator Optimization',\n",
       "   'abstract': 'Score-based models have recently been introduced as a richer framework to model distributions in high dimensions and are generally more suitable for generative tasks. In score-based models, a generative task is formulated using a parametric model (such as a neural network) to directly learn the gradient of such high dimensional distributions, instead of the density functions themselves, as is done traditionally. From the mathematical point of view, such gradient information can be utilized in reverse by stochastic sampling to generate diverse samples. However, from a computational perspective, existing score-based models can be efficiently trained only if the forward or the corruption process can be computed in closed form. By using the relationship between the process and layers in a feed-forward network, we derive a backpropagation-based procedure which we call Intermediate Generator Optimization to utilize intermediate iterates of the process with negligible computational overhead. The main advantage of IGO is that it can be incorporated into any standard autoencoder pipeline for the generative task. We analyze the sample complexity properties of IGO to solve downstream tasks like Generative PCA. We show applications of the IGO on two dense predictive tasks viz., image extrapolation, and point cloud denoising. Our experiments indicate that obtaining an ensemble of generators for various time points is possible using first-order methods.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '8763ac18bd2af1602d75152e35fac78a7198d68f',\n",
       "   'title': 'Bag of Tricks with Quantized Convolutional Neural Networks for image classification',\n",
       "   'abstract': 'Deep neural networks have been proven effective in a wide range of tasks. However, their high computational and memory costs make them impractical to deploy on resource-constrained devices. To address this issue, quantization schemes have been proposed to reduce the memory footprint and improve inference speed. While numerous quantization methods have been proposed, they lack systematic analysis for their effectiveness. To bridge this gap, we collect and improve existing quantization methods and propose a gold guideline for post-training quantization. We evaluate the effectiveness of our proposed method with two popular models, ResNet50 and MobileNetV2, on the ImageNet dataset. By following our guidelines, no accuracy degradation occurs even after directly quantizing the model to 8-bits without additional training. A quantization-aware training based on the guidelines can further improve the accuracy in lower-bits quantization. Moreover, we have integrated a multi-stage fine-tuning strategy that works harmoniously with existing pruning techniques to reduce costs even further. Remarkably, our results reveal that a quantized MobileNetV2 with 30\\\\% sparsity actually surpasses the performance of the equivalent full-precision model, underscoring the effectiveness and resilience of our proposed scheme.',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '0dd2dafa9389f83160a63be3fde23b5d121a4786',\n",
       "   'title': 'DDP: Diffusion Model for Dense Visual Prediction',\n",
       "   'abstract': 'We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline. Our approach follows a\"noise-to-map\"generative paradigm for prediction by progressively removing noise from a random Gaussian distribution, guided by the image. The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline. Without task-specific design and architecture customization, DDP is easy to generalize to most dense prediction tasks, e.g., semantic segmentation and depth estimation. In addition, DDP shows attractive properties such as dynamic inference and uncertainty awareness, in contrast to previous single-step discriminative methods. We show top results on three representative tasks with six diverse benchmarks, without tricks, DDP achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts. For example, semantic segmentation (83.9 mIoU on Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation (0.05 REL on KITTI). We hope that our approach will serve as a solid baseline and facilitate future research',\n",
       "   'fieldsOfStudy': ['Computer Science']},\n",
       "  {'paperId': '09ac8fc062e49db54be25ee5529f0d5a6148b39d',\n",
       "   'title': 'Your Diffusion Model is Secretly a Zero-Shot Classifier',\n",
       "   'abstract': 'The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, we find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Even though these models are trained with weak augmentations and no regularization, they approach the performance of SOTA discriminative classifiers. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/',\n",
       "   'fieldsOfStudy': ['Computer Science']}]}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_user = recommended_papers[0]\n",
    "sample_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
      "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
      "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
      "['Computer Science']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user[\"positive_papers\"][0][\"paperId\"])\n",
    "print(sample_user[\"positive_papers\"][0][\"title\"])\n",
    "print(sample_user[\"positive_papers\"][0][\"abstract\"])\n",
    "print(sample_user[\"positive_papers\"][0][\"fieldsOfStudy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
      "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
      "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
      "['Computer Science', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user[\"recommendedPapers\"][0][\"paperId\"])\n",
    "print(sample_user[\"recommendedPapers\"][0][\"title\"])\n",
    "print(sample_user[\"recommendedPapers\"][0][\"abstract\"])\n",
    "print(sample_user[\"recommendedPapers\"][0][\"fieldsOfStudy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
    "\n",
    "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
    " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
    "\n",
    "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
    "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_set = set(\n",
    "    field\n",
    "    for user in recommended_papers\n",
    "    for paper in user[\"positive_papers\"]\n",
    "    for field in paper[\"fieldsOfStudy\"] or []\n",
    ")\n",
    "fields = list(fields_set)\n",
    "\n",
    "M = len(fields)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    recommended_papers, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def generate_user_vectors(user_positive_papers):\n",
    "    user_indices = list(range(len(user_positive_papers)))\n",
    "    user_fields = {\"user_index\": user_indices}\n",
    "\n",
    "    for field in fields:\n",
    "        field_counts = []\n",
    "        for positive_papers in user_positive_papers:\n",
    "            count = 0\n",
    "            for paper in positive_papers:\n",
    "                if (\n",
    "                    paper[\"fieldsOfStudy\"] is not None\n",
    "                    and field in paper[\"fieldsOfStudy\"]\n",
    "                ):\n",
    "                    count += 1\n",
    "            field_counts.append(count)\n",
    "\n",
    "        user_fields[field] = field_counts\n",
    "\n",
    "    return pd.DataFrame(user_fields).set_index(\"user_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "class CollaborativeFiltering:\n",
    "    def __init__(self, data: dict, n=10):\n",
    "        self.data = data\n",
    "        self.knn = NearestNeighbors(n_neighbors=n, metric=\"cosine\")\n",
    "        self.n = n\n",
    "\n",
    "    def fit(self):\n",
    "        user_vectors = generate_user_vectors(\n",
    "            [user[\"positive_papers\"] or [] for user in self.data]\n",
    "        )\n",
    "        self.knn.fit(user_vectors)\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_positive_papers: List[Dict[str, Any]]):\n",
    "        user_vectors = generate_user_vectors([user_positive_papers])\n",
    "        distances, indices = self.knn.kneighbors(user_vectors)\n",
    "\n",
    "        # get recommended papers from the k nearest neighbors\n",
    "        result = [\n",
    "            paper[\"paperId\"]\n",
    "            for similar_user_id in indices[0]\n",
    "            for paper in self.data[similar_user_id][\"recommendedPapers\"]\n",
    "        ]\n",
    "        # get the most common recommended papers (10)\n",
    "        result = [paper_id for paper_id, _ in Counter(result).most_common(self.n)]\n",
    "\n",
    "        return result[: self.n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborative_recommends = CollaborativeFiltering(train_data).fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Content Based (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
    "\n",
    "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
    "\n",
    "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class ContentBasedRecommendation:\n",
    "    def __init__(self, all_recommended_papers: List[Dict[str, Any]]):\n",
    "        self.data = all_recommended_papers\n",
    "        self.tf_idf = TfidfVectorizer()\n",
    "        self.recommended_vecors = None\n",
    "        self.titles = [paper[\"title\"] for paper in self.data]\n",
    "\n",
    "    def fit(self):\n",
    "        self.recommended_vecors = self.tf_idf.fit_transform(self.titles)\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_positive_papers: List[Dict[str, Any]]):\n",
    "        titles = [paper[\"title\"] for paper in user_positive_papers]\n",
    "        titles_vector = self.tf_idf.transform(titles).mean(axis=0)\n",
    "\n",
    "        # calculate the similarities between the user's positive papers and all the recommended papers\n",
    "        similarities: np.ndarray = np.asarray(\n",
    "            titles_vector @ self.recommended_vecors.T\n",
    "        ).flatten()\n",
    "\n",
    "        # get the top 10 most similar papers\n",
    "        result = [self.data[i][\"paperId\"] for i in similarities.argsort()[-10:][::-1]]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_recommendation = ContentBasedRecommendation(\n",
    "    [\n",
    "        paper\n",
    "        for user in recommended_papers\n",
    "        for paper in (user[\"recommendedPapers\"] or [])\n",
    "    ]\n",
    ").fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG score for collaborative filtering: 0.5012825149769274\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: Compare two methods of recommendation with nDCG metric.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def ndcg_score(prediction, true):\n",
    "    \"\"\"\n",
    "    Computes the nDCG score given two lists of lists: prediction and true.\n",
    "\n",
    "    Parameters:\n",
    "    prediction (List[List[int]]): A list of lists of recommended articles for each user.\n",
    "    true (List[List[int]]): A list of lists of actual articles read by each user.\n",
    "\n",
    "    Returns:\n",
    "    float: The nDCG score.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    for i, paper in enumerate(prediction):\n",
    "        if paper in true:\n",
    "            score += 1 / np.log2(i + 2)\n",
    "\n",
    "    idcg = 0.0\n",
    "    for i, paper in enumerate(true):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "    return score / idcg\n",
    "\n",
    "\n",
    "# Collaborative filtering\n",
    "ndcg_scores = []\n",
    "\n",
    "for user in test_data:\n",
    "    prediction = collaborative_recommends.predict(user[\"positive_papers\"] or [])\n",
    "    true = [paper[\"paperId\"] for paper in user[\"recommendedPapers\"]]\n",
    "    ndcg_scores.append(ndcg_score(prediction, true))\n",
    "\n",
    "print(f\"nDCG score for collaborative filtering: {np.mean(ndcg_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG score for content-based recommendation: 0.1290581089990145\n"
     ]
    }
   ],
   "source": [
    "ndcg_scores = []\n",
    "\n",
    "for user in test_data:\n",
    "    prediction = content_based_recommendation.predict(user[\"positive_papers\"] or [])\n",
    "    true = [paper[\"paperId\"] for paper in user[\"recommendedPapers\"]]\n",
    "    ndcg_scores.append(ndcg_score(prediction, true))\n",
    "\n",
    "print(f\"nDCG score for content-based recommendation: {np.mean(ndcg_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش\n",
    " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.0.2.2:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "2023-06-29 02:21:34.382 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 84, in <module>\n",
      "    main()\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 78, in main\n",
      "    search_handling(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 25, in search_handling\n",
      "    result = bs.search(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/backend_system.py\", line 255, in search\n",
      "    for doc_id in index[term]:\n",
      "KeyError: 'mamma'\n",
      "2023-06-29 02:21:37.047 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 84, in <module>\n",
      "    main()\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 78, in main\n",
      "    search_handling(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 25, in search_handling\n",
      "    result = bs.search(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/backend_system.py\", line 255, in search\n",
      "    for doc_id in index[term]:\n",
      "KeyError: 'mammad'\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "2023-06-29 02:23:54.049 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 85, in <module>\n",
      "    main()\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 79, in main\n",
      "    search_handling(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 26, in search_handling\n",
      "    result = bs.search(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/backend_system.py\", line 255, in search\n",
      "    for doc_id in index[term]:\n",
      "KeyError: 'bio'\n",
      "2023-06-29 02:23:59.724 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 85, in <module>\n",
      "    main()\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 79, in main\n",
      "    search_handling(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 26, in search_handling\n",
      "    result = bs.search(\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/backend_system.py\", line 255, in search\n",
      "    for doc_id in index[term]:\n",
      "KeyError: 'bioinformat'\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "2023-06-29 02:30:51.900 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 2, in <module>\n",
      "    import backend_system as bs\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/backend_system.py\", line 744, in <module>\n",
      "    [paper[\"title\"] for paper in papers] + [paper[\"abstract\"] for paper in papers]\n",
      "NameError: name 'papers' is not defined\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "2023-06-29 02:31:21.525 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/frontend_system.py\", line 2, in <module>\n",
      "    import backend_system as bs\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/backend_system.py\", line 744, in <module>\n",
      "    [paper[\"title\"] for paper in papers_dataset]\n",
      "  File \"/Users/amirhossein/Documents/University/MIR/Project/backend_system.py\", line 744, in <listcomp>\n",
      "    [paper[\"title\"] for paper in papers_dataset]\n",
      "KeyError: 'title'\n",
      "['Jiri Matas', 'Matej Kristan', 'Philip H. S. Torr', 'Alan Luke{\\\\vz}i{\\\\vc}', 'Michael Felsberg', 'Martin Danelljan', 'Fahad Shahbaz Khan', 'Ale{\\\\vs} Leonardis', 'Gustavo Javier Fernandez', 'Jo{\\\\~a}o F. Henriques']\n",
      "vision ['vi', 'is', 'si', 'io', 'on']\n",
      "In this paper, a novel discriminative dictionary learning method is proposed for sparse-representation-based classification (SRC) to label highly dimensional hyperspectral imagery (HSI). In SRC, a dictionary is conventionally constructed using all of the training pixels, which is not only inefficient due to the large size of typical HSI images but also ineffective in capturing class-discriminative information crucial for classification. We address the dictionary design problem with the…\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run frontend_system.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
